<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>深度学习个人笔记(MIT-CS231n) | MyBLog</title><meta name="author" content="ChangQibo"><meta name="copyright" content="ChangQibo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="MathJax.Hub.Config({             tex2jax: {             skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;],             inlineMath: [[&#39;$&#39;,&#39;$&#39;]]             }         });">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习个人笔记(MIT-CS231n)">
<meta property="og:url" content="https://smallsmallqi.github.io/posts/14569/index.html">
<meta property="og:site_name" content="MyBLog">
<meta property="og:description" content="MathJax.Hub.Config({             tex2jax: {             skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;],             inlineMath: [[&#39;$&#39;,&#39;$&#39;]]             }         });">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://smallsmallqi.github.io/img/myavatar.jpg">
<meta property="article:published_time" content="2026-01-29T08:00:00.000Z">
<meta property="article:modified_time" content="2026-01-30T06:55:22.823Z">
<meta property="article:author" content="ChangQibo">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://smallsmallqi.github.io/img/myavatar.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "深度学习个人笔记(MIT-CS231n)",
  "url": "https://smallsmallqi.github.io/posts/14569/",
  "image": "https://smallsmallqi.github.io/img/myavatar.jpg",
  "datePublished": "2026-01-29T08:00:00.000Z",
  "dateModified": "2026-01-30T06:55:22.823Z",
  "author": [
    {
      "@type": "Person",
      "name": "ChangQibo",
      "url": "https://SmallSmallQi.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://smallsmallqi.github.io/posts/14569/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习个人笔记(MIT-CS231n)',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/myavatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">1</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post type-tags" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/myavatar.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/myavatar.jpg" alt="Logo"><span class="site-name">MyBLog</span></a><a class="nav-page-title" href="/"><span class="site-name">深度学习个人笔记(MIT-CS231n)</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">深度学习个人笔记(MIT-CS231n)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2026-01-29T08:00:00.000Z" title="发表于 2026-01-29 16:00:00">2026-01-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-01-30T06:55:22.823Z" title="更新于 2026-01-30 14:55:22">2026-01-30</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

<p>这篇笔记是我学习 MIT-CS231n 课程后写出的深度学习个人笔记，主要内容包括深度学习基础、视觉感知与理解，以及生成式与交互式视觉智能等方向。</p>
<span id="more"></span>

<h1 id="第一课-·-导论"><a href="#第一课-·-导论" class="headerlink" title="第一课 · 导论"></a>第一课 · 导论</h1><h2 id="课程简介"><a href="#课程简介" class="headerlink" title="课程简介"></a>课程简介</h2><p>本课程以深度学习为工具，讲解计算机视觉（CV）与机器学习（ML）中的核心概念与方法，侧重实战与直观理解。主要内容包括深度学习基础、视觉感知与理解、以及生成式与交互式视觉智能等方向。</p>
<div align="center">
<img src="/img/image-0.png" alt="课程概览" width="80%" />
</div>

<hr>
<h2 id="1-深度学习基础"><a href="#1-深度学习基础" class="headerlink" title="1. 深度学习基础"></a>1. 深度学习基础</h2><p>计算机视觉的一个核心任务是图像分类。常见的分类方法包括：</p>
<ol>
<li><p>线性分类  </p>
<ul>
<li>基本思想：通过线性决策边界对样本进行划分，适用于特征线性可分或经特征工程后可线性分离的场景。</li>
</ul>
<div align="center"><img src="/img/image-1.png" alt="线性分类" width="40%" /></div>
</li>
<li><p>正则化与优化（Regularization &amp; Optimization）  </p>
<ul>
<li>包括 L1&#x2F;L2 正则化、dropout、早停等，用以缓解过拟合；以及常见的优化算法（SGD、Adam 等）。</li>
</ul>
<div align="center"><img src="/img/image-2.png" alt="正则化与优化" width="60%" /></div>
</li>
<li><p>神经网络（Neural Networks）  </p>
<ul>
<li>多层感知机到深度网络，通过非线性激活函数与层叠结构学习复杂映射。</li>
</ul>
<div align="center"><img src="/img/image-3.png" alt="神经网络" width="60%" /></div></li>
</ol>
<hr>
<h2 id="2-感知与理解视觉世界"><a href="#2-感知与理解视觉世界" class="headerlink" title="2. 感知与理解视觉世界"></a>2. 感知与理解视觉世界</h2><h3 id="2-1-图像理解的主要任务"><a href="#2-1-图像理解的主要任务" class="headerlink" title="2.1 图像理解的主要任务"></a>2.1 图像理解的主要任务</h3><p>常见任务及简要说明：</p>
<ol>
<li>直接分类（classification） — 将整张图像分到某个类别。  </li>
<li>语义分割（semantic segmentation） — 对每个像素进行语义标注。  </li>
<li>目标检测（object detection） — 标出图中每个目标的边界框并分类。  </li>
<li>实例分割（instance segmentation） — 在语义分割基础上区分不同实例。  </li>
<li>其他：视频分类、多模态视频理解、可视化与可解释性等。</li>
</ol>
<hr>
<h3 id="2-2-超越多层感知机的代表模型"><a href="#2-2-超越多层感知机的代表模型" class="headerlink" title="2.2 超越多层感知机的代表模型"></a>2.2 超越多层感知机的代表模型</h3><ol>
<li><p>CNN（Convolutional Neural Network，卷积神经网络）  </p>
<ul>
<li>擅长提取局部空间特征、尺度不变性和参数共享。</li>
</ul>
<div align="center"><img src="/img/image-4.png" alt="CNN" width="70%" /></div>
</li>
<li><p>RNN（Recurrent Neural Network，循环神经网络）  </p>
<ul>
<li>处理序列数据（如视频帧、文本），能够建模时序依赖。</li>
</ul>
<div align="center"><img src="/img/image-5.png" alt="RNN" width="60%" /></div>
</li>
<li><p>Attention 机制 &amp; Transformers  </p>
<ul>
<li>通过自注意力机制捕获长程依赖，已经成为图像与多模态任务的重要架构基础。</li>
</ul>
<div align="center"><img src="/img/image-6.png" alt="Attention & Transformers" width="60%" /></div>
</li>
<li><p>大规模分布式训练（Large-Scale Distributed Training）  </p>
<ul>
<li>当模型与数据规模很大时，需要借助多GPU&#x2F;多机训练，常用策略有：  <ul>
<li>数据并行（Data Parallel）：把数据切分给多个 worker，每个 worker 保持完整模型副本。  </li>
<li>模型并行（Model Parallel）：将模型切分到不同设备上，适用于单卡内存不足的超大模型。</li>
</ul>
</li>
</ul>
<div align="center"><img src="/img/image-7.png" alt="大规模分布式训练" width="60%" /></div></li>
</ol>
<hr>
<h2 id="3-生成式与交互式视觉智能"><a href="#3-生成式与交互式视觉智能" class="headerlink" title="3. 生成式与交互式视觉智能"></a>3. 生成式与交互式视觉智能</h2><p>当前热门方向，代表性主题包括：</p>
<ul>
<li>自监督学习（Self-supervised Learning）  <ul>
<li>利用未标注数据设计预训练任务以学习表征。</li>
</ul>
</li>
<li>生成式模型（Generative Modeling）  <ul>
<li>如 GAN、VAE、Diffusion Models，用于图像生成与补全。</li>
</ul>
</li>
<li>视觉-语言模型（Vision-Language Models）  <ul>
<li>融合图像与文本信息，支持检索、描述生成、跨模态理解等。</li>
</ul>
</li>
<li>3D 视觉（3D Vision）  <ul>
<li>深入点云、体素、神经渲染等三维表示与重建。</li>
</ul>
</li>
<li>具身智能（Embodied Intelligence）  <ul>
<li>感知与动作闭环，为机器人与交互系统提供视觉驱动能力。</li>
</ul>
</li>
</ul>
<h1 id="第二课-·-线性分类"><a href="#第二课-·-线性分类" class="headerlink" title="第二课 · 线性分类"></a>第二课 · 线性分类</h1><h2 id="Image-Classification-with-Linear-Classifiers"><a href="#Image-Classification-with-Linear-Classifiers" class="headerlink" title="Image Classification with Linear Classifiers"></a>Image Classification with Linear Classifiers</h2><h3 id="本节课程核心总结"><a href="#本节课程核心总结" class="headerlink" title="本节课程核心总结"></a>本节课程核心总结</h3><p>在机器学习中，为了实现图像分类（即输入一个像素张量，要求输出分类标签），可以使用两种最基础的图像分类方法：</p>
<ol>
<li>非参数方法：K-Nearest Neighbors <strong>KNN最近邻</strong></li>
<li>参数方法：Linear classifiers <strong>线性分类器</strong>（包含Softmax 多类逻辑回归）与常用损失（SVM hinge，交叉熵）</li>
</ol>
<h3 id="笔记内容"><a href="#笔记内容" class="headerlink" title="笔记内容"></a>笔记内容</h3><ol>
<li><p>KNN分类器</p>
<ol>
<li>定义：KNN是一种基于实例的学习方法，通过计算样本之间的距离来进行分类。</li>
<li>训练：让模型直接记住所有的训练数据与标签</li>
<li>预测：直接从训练集中找到与测试图最相似的标签作为结果</li>
<li>量化图像相似度的方法：曼哈顿距离与欧氏距离</li>
</ol>
<div align="center"><img src="/img/image-66.png" alt="KNN" width="80%" /></div>
5. 超参数调优：
 将数据分为训练集、验证集和测试集。
 在训练集上训练不同的模型（使用不同的超参数）。
 在验证集上评估这些模型，选择表现最好的超参数。
 最终，在测试集上仅运行一次，以报告模型的最终性能。
</li>
<li><p>线性分类器</p>
<ol>
<li><p>定义：不再存储训练数据，而是定义一个模型 <strong>f(x, W) &#x3D; Wx + b</strong>，其中 x 是输入图像（被展平为一个长向量），W 是权重矩阵，b 是偏置向量。通过学习得到最优的参数 W 和 b，使得这个函数能够输出正确的分类分数。<div align="center"><img src="/img/image-67.png" alt="图示" width="80%" /></div></p>
</li>
<li><p>训练线性分类器：<br>定义损失函数：量化预测分数与真实标签之间的差距。<br>优化：寻找能使损失函数最小化的参数</p>
</li>
<li><p>损失函数有两种：</p>
<ol>
<li><p>SVM多类支持向量机损失<br>  对于第 $i$ 条数据，其损失 $L_i$ 的计算公式为：<br>  $$L_i &#x3D; \sum_{j \ne y_i} \max(0, s_j - s_{y_i} + \Delta)$$</p>
<ul>
<li>$s_{y_i}$：正确类别的得分。</li>
<li>$s_j$：除正确类别外，其他错误类别的得分。</li>
<li>$\Delta$：设定的安全距离（通常取 1）。</li>
<li>$\max(0, \cdot)$：表示如果括号里的值小于 0，就取 0。<br>  $$L_{\text{SVM}}(f(x;W), y) &#x3D; \max(0, 1 - y \cdot f(x;W))$$</li>
</ul>
</li>
<li><p>Softmax分类器（多项逻辑回归）<br>  SVM 只在乎“得分的差距”，而 Softmax 更贪心，它想把得分变成<strong>概率</strong>。</p>
<ul>
<li><p>它把原始得分通过指数函数映射成正数，再做归一化，让所有类别的预测概率加起来等于 1。</p>
</li>
<li><p>它的目标是：让正确类别的概率尽可能接近 <strong>100% (1.0)</strong>。如果你只给正确答案 10% 的概率，模型就会非常痛苦（损失很大）。</p>
</li>
<li><p>第一步：计算 Softmax 函数（得分转概率）<br> 正确类别的预测概率 $P$ 为：<br> $$P(Y &#x3D; y_i \mid X &#x3D; x_i) &#x3D; \frac{e^{s_{y_i}}}{\sum_j e^{s_j}}$$</p>
</li>
<li><p>第二步：计算交叉熵损失 (Cross-Entropy Loss)<br> $$L_i &#x3D; -\log(P) &#x3D; -\log\left( \frac{e^{s_{y_i}}}{\sum_j e^{s_j}} \right)$$</p>
</li>
<li><p>$e$ 是自然对数的底数（约 2.718）。</p>
</li>
<li><p>$-\log$ 的特性是：当概率 $P$ 趋近于 1 时，损失趋近于 0；当概率 $P$ 趋近于 0 时，损失趋近于无穷大。</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p>数据集的总损失 (Total Loss)<br>   我们不仅关心一张图片的损失，更关心整个学校仓库（数据集）里所有图片的表现。</p>
</li>
</ol>
<p>   $$L &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} L_i + \lambda R(W)$$</p>
<p>   这里包含两部分：</p>
<ol>
<li><strong>数据损失 (Data Loss)</strong>：$\frac{1}{N} \sum L_i$，即所有样本损失的平均值。</li>
<li><strong>正则化损失 (Regularization Loss)</strong>：$\lambda R(W)$。<ul>
<li><strong>比喻</strong>：这是为了防止模型“死记硬背”每一张图的细节。我们希望模型学到的是大规律，而不是极其复杂的权重 $W$。常见的正则项是 $L2$ 正则：$R(W) &#x3D; \sum \sum W_{k,l}^2$。</li>
</ul>
</li>
</ol>
</li>
</ol>
<table>
<thead>
<tr>
<th align="left">特性</th>
<th align="left"><strong>Multiclass SVM</strong></th>
<th align="left"><strong>Softmax (Cross-Entropy)</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>关注点</strong></td>
<td align="left">正确分比错误分高出一个 Margin 即可（够好就行）。</td>
<td align="left">正确概率越高越好，永远不嫌多（精益求精）。</td>
</tr>
<tr>
<td align="left"><strong>解释</strong></td>
<td align="left">几何视角的“安全边界”。</td>
<td align="left">概率视角的“极大似然估计”。</td>
</tr>
<tr>
<td align="left"><strong>初始化检查</strong></td>
<td align="left">初始 Loss 通常约等于 类数 - 1。</td>
<td align="left">初始 Loss 通常约等于 $\log(\text{类数})$。</td>
</tr>
</tbody></table>
<h1 id="第三课-·-正则化与优化"><a href="#第三课-·-正则化与优化" class="headerlink" title="第三课 · 正则化与优化"></a>第三课 · 正则化与优化</h1><h3 id="本节课程核心总结-1"><a href="#本节课程核心总结-1" class="headerlink" title="本节课程核心总结"></a>本节课程核心总结</h3><p>在机器学习中，为了提升模型的泛化能力与训练效率，常用以下两大技术手段：</p>
<ol>
<li>正则化（Regularization）：通过在损失函数中添加正则项，防止模型对训练数据过度拟合，从而避免拟合到数据中的噪声，从而提高在未见数据上的泛化能力。<ul>
<li>没有正则化的大模型容易把训练集的噪声也“记住”，像学生把训练题答案死记住而无法解新题。正则化就是鼓励学生学习通用解法而不是死记答案。正则化像是给函数加一层平滑的约束，不允许模型在训练点周围做急剧震荡，从而避免在新点的巨大误差。</li>
</ul>
</li>
<li>优化（Optimization）：采用高效的优化算法（如梯度下降及其变种）来加速模型参数的收敛过程，提高训练效率。<ul>
<li>目标：在参数空间中寻找参数 W 使得 <strong>经验损失 + 正则化</strong> 最小：<br>$$<br>\min_W; L(W)&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N \ell\big(f(x_i;W),y_i\big)+\lambda R(W).<br>$$</li>
</ul>
</li>
</ol>
<h3 id="笔记内容-1"><a href="#笔记内容-1" class="headerlink" title="笔记内容"></a>笔记内容</h3><h2 id="一、正则化（Regularization）"><a href="#一、正则化（Regularization）" class="headerlink" title="一、正则化（Regularization）"></a>一、正则化（Regularization）</h2><ol>
<li><p>总体损失形式<br>对训练集上的数据损失（data loss）加上正则化项（regularizer）：<br>$$<br>L(W) &#x3D; \frac{1}{N}\sum_{i&#x3D;1}^N L_{\text{data}}(f(x_i;W), y_i) + \lambda R(W),<br>$$<br>其中 $$\lambda$$ 是正则化强度（超参数）。</p>
</li>
<li><p>常见正则化方法</p>
<ul>
<li>L2（权重衰减 &#x2F; Ridge）：<br> $$<br>R(W) &#x3D; \sum_k \sum_l W_{k,l}^2<br>$$<br>缩小权重，倾向于将所有特征都保留，但降低影响力</li>
<li>L1（Lasso）：<br>$$<br>R(W) &#x3D; \sum_k \sum_l |W_{k,l}|<br>$$<br>产生稀疏解，即让部分权重精确为零</li>
<li>Elastic Net（L1 + L2）：<br>$$<br>R(W) &#x3D; \sum_k \sum_l \beta W_{k,l}^2 + |W_{k,l}|<br>$$<br>  平衡L1和L2的特点，既能产生稀疏解，又能处理特征相关性</li>
</ul>
</li>
<li><p>为什么需要正则化</p>
<ul>
<li>减少过拟合（减少模型对训练噪声的拟合）。  </li>
<li>让模型更简单，以便于在测试集上更好的工作</li>
<li>通过增加“额外曲率”的方式促进正则化。</li>
</ul>
</li>
</ol>
<h2 id="二、优化（Optimization）"><a href="#二、优化（Optimization）" class="headerlink" title="二、优化（Optimization）"></a>二、优化（Optimization）</h2><ol>
<li>最优化策略：<ol>
<li>Random search，随机尝试很多不同的权重，然后看其中哪个最好。</li>
<li>Follow the slope，在多维空间中，梯度是沿每个维度的（偏导数）向量。任意方向的坡度是该方向与梯度的点积。最陡下降的方向是负梯度。跟随梯度找到损失最小的方法</li>
</ol>
</li>
<li>梯度下降(Gradient Descent)</li>
<li>程序重复地计算损失函数的梯度，然后对参数进行更新，直到结果不再变化</li>
</ol>
  <div align="center"><img src="/img/image-13.png" alt="图示" width="80%" /></div>
3. 随机梯度下降(Stochastic Gradient Descent) (SGD)
  1. 挑选数据集中的一批数据来进行训练
  <div align="center"><img src="/img/image-14.png" alt="图示" width="80%" /></div>
   2. 会产生一些问题，例如实际进展很慢，会产生抖动；出现梯度为0的情况，梯度下降被卡住，梯度来自小批量数据，可能会有噪声
4. SGD + Momentum 动量
  1. 在下降过程中添加动量，让其大步前进
  <div align="center"><img src="/img/image-15.png" alt="图示" width="80%" /></div>

<ol start="5">
<li>AdaGrad&#x2F;RMSProp <ol>
<li>如果有某个方向梯度一直很大（震荡），那就把这个方向的学习率调小点（除以梯度的平方和积累）；如果某个方向梯度一直很小（平缓），就把学习率调大点</li>
</ol>
</li>
</ol>
  <div align="center"><img src="/img/image-16.png" alt="图示" width="80%" /></div>
6. Adam(即Momentum + RMSProp)
  1. 既有惯性（一阶矩），又有自适应学习率（二阶矩）
  <div align="center"><img src="/img/image-17.png" alt="图示" width="80%" /></div>


<h2 id="本次课程重点公式"><a href="#本次课程重点公式" class="headerlink" title="本次课程重点公式"></a>本次课程重点公式</h2><h4 id="1-完整的损失函数-Total-Loss-Function"><a href="#1-完整的损失函数-Total-Loss-Function" class="headerlink" title="1. 完整的损失函数 (Total Loss Function)"></a>1. 完整的损失函数 (Total Loss Function)</h4><p>这是我们优化的目标。</p>
<p>$$L(W) &#x3D; \underbrace{\frac{1}{N} \sum_{i&#x3D;1}^{N} L_i(f(x_i, W), y_i)}<em>{\text{Data Loss (拟合数据)}} + \underbrace{\lambda R(W)}</em>{\text{Regularization Loss (保持简单)}}$$</p>
<ul>
<li>$N$: 训练样本数量</li>
<li>$\lambda$: 正则化强度（超参数，你需要调）</li>
<li>$R(W)$: 正则化项，通常是 $L2$：$R(W) &#x3D; \sum_k \sum_l W_{k,l}^2$</li>
</ul>
<h4 id="2-普通梯度下降更新-Vanilla-SGD-Update"><a href="#2-普通梯度下降更新-Vanilla-SGD-Update" class="headerlink" title="2. 普通梯度下降更新 (Vanilla SGD Update)"></a>2. 普通梯度下降更新 (Vanilla SGD Update)</h4><p>最基础的更新公式。</p>
<p>$$W_{t+1} &#x3D; W_t - \eta \cdot \nabla L(W_t)$$</p>
<ul>
<li>$W_t$: 当前时刻的权重</li>
<li>$\eta$ (eta): <strong>学习率 (Learning Rate)</strong>，最重要的超参数，决定步子迈多大。</li>
<li>$\nabla L$: 损失函数关于权重的梯度。</li>
</ul>
<h4 id="3-动量更新-SGD-Momentum"><a href="#3-动量更新-SGD-Momentum" class="headerlink" title="3. 动量更新 (SGD + Momentum)"></a>3. 动量更新 (SGD + Momentum)</h4><p>加上了物理惯性，这是对 SGD 最重要的改进之一。</p>
<p>$$\begin{aligned} v_{t+1} &amp;&#x3D; \rho v_t + \nabla L(W_t) \ W_{t+1} &amp;&#x3D; W_t - \alpha v_{t+1} \end{aligned}$$</p>
<ul>
<li>$v$: 速度向量 (Velocity)，初始为0。</li>
<li>$\rho$ (rho): <strong>摩擦系数&#x2F;动量衰减</strong>，通常取 0.9 或 0.99。</li>
<li>$\alpha$: 学习率。</li>
<li><strong>理解</strong>：现在的更新方向不仅仅取决于当前的梯度（脚下的路），还取决于之前的速度 $v_t$（惯性）。</li>
</ul>
<h4 id="4-RMSProp-自适应学习率"><a href="#4-RMSProp-自适应学习率" class="headerlink" title="4. RMSProp (自适应学习率)"></a>4. RMSProp (自适应学习率)</h4><p>让每个参数有自己的学习率步伐。</p>
<p>$$\begin{aligned} cache_{t+1} &amp;&#x3D; \text{decay_rate} \cdot cache_t + (1 - \text{decay_rate}) \cdot (\nabla L)^2 \ W_{t+1} &amp;&#x3D; W_t - \frac{\eta}{\sqrt{cache_{t+1}} + \epsilon} \cdot \nabla L \end{aligned}$$</p>
<ul>
<li>$cache$: 梯度的平方的滑动平均值（衡量该方向是否陡峭）。</li>
<li>$(\nabla L)^2$: 梯度逐元素平方。</li>
<li><strong>理解</strong>：除以 $\sqrt{cache}$ 意味着——如果某个方向梯度一直很大（陡峭），分母变大，更新步长变小（防止震荡）；反之步长变大。</li>
</ul>
<h4 id="5-Adam-万能公式"><a href="#5-Adam-万能公式" class="headerlink" title="5. Adam (万能公式)"></a>5. Adam (万能公式)</h4><p>结合了 Momentum 和 RMSProp。</p>
<p>$$\begin{aligned} m_{t+1} &amp;&#x3D; \beta_1 m_t + (1-\beta_1) \nabla L \quad \text{(一阶矩: 类似动量)} \ v_{t+1} &amp;&#x3D; \beta_2 v_t + (1-\beta_2) (\nabla L)^2 \quad \text{(二阶矩: 类似RMSProp)} \ W_{t+1} &amp;&#x3D; W_t - \frac{\eta}{\sqrt{\hat{v}<em>{t+1}} + \epsilon} \cdot \hat{m}</em>{t+1} \end{aligned}$$</p>
<ul>
<li>$\hat{m}, \hat{v}$: 是修正了初始偏差后的 $m$ 和 $v$（Bias correction）。</li>
<li><strong>推荐配置</strong>: $\beta_1 &#x3D; 0.9$, $\beta_2 &#x3D; 0.999$, $\epsilon &#x3D; 1e-8$, 学习率 $\eta \approx 1e-3$。</li>
</ul>
<h1 id="第四课-·-神经网络与反向传播"><a href="#第四课-·-神经网络与反向传播" class="headerlink" title="第四课 · 神经网络与反向传播"></a>第四课 · 神经网络与反向传播</h1><h2 id="1-目标与整体流程"><a href="#1-目标与整体流程" class="headerlink" title="1) 目标与整体流程"></a>1) 目标与整体流程</h2><ul>
<li>目标：给定训练集 $$(x_i,y_i)$$，寻找参数集合（所有权重和偏置）使得训练损失最小，并在测试集有良好泛化。</li>
<li>总损失（Loss）形式常写为：<br>$$<br>L(\Theta)&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N \ell\big(f(x_i;\Theta),y_i\big) ;+; \frac{\lambda}{2}\sum_{k}|W_k|_F^2,<br>$$<br>其中 $$\Theta$$ 表示所有参数，第一项是数据损失（例如 softmax + cross-entropy），第二项是 L2 正则化（weight decay）。</li>
</ul>
<h2 id="2-神经网络的基本结构"><a href="#2-神经网络的基本结构" class="headerlink" title="2) 神经网络的基本结构"></a>2) 神经网络的基本结构</h2><ul>
<li>基本单元：交替堆叠的 仿射（linear &#x2F; fully-connected）层 和 非线性激活（activation）。</li>
<li>2 层（1 个隐藏层）前向（minibatch 形式）：<br>$$<br>Z^{(1)} &#x3D; X W_1 + \mathbf{1} b_1^\top,\quad<br>H &#x3D; f(Z^{(1)}),\quad<br>S &#x3D; H W_2 + \mathbf{1} b_2^\top,<br>$$<br>其中 $$X\in\mathbb{R}^{N\times D}$$, $$W_1\in\mathbb{R}^{D\times H}$$, $$W_2\in\mathbb{R}^{H\times C}$$, 输出得分 $$S\in\mathbb{R}^{N\times C}$$。</li>
</ul>
<div align="center"><img src="/img/image-19.png" alt="图示" width="80%" /></div>


<h2 id="3-前向计算与常用损失"><a href="#3-前向计算与常用损失" class="headerlink" title="3) 前向计算与常用损失"></a>3) 前向计算与常用损失</h2><ul>
<li><p>Softmax + Cross-Entropy（按行对每个样本）：</p>
<ul>
<li>数值稳定的 softmax：<br>$$<br>\tilde S_{i} &#x3D; S_{i} - \max_j S_{ij},\qquad<br>P_{ij} &#x3D; \frac{e^{\tilde S_{ij}}}{\sum_k e^{\tilde S_{ik}}}.<br>$$</li>
<li>损失（平均）：<br>$$<br>L &#x3D; -\frac{1}{N}\sum_{i&#x3D;1}^N \log P_{i,y_i}.<br>$$</li>
<li>对 scores 的梯度（用于反向传播起点）：<br>$$<br>\frac{\partial L}{\partial S_{ij}} &#x3D; \frac{1}{N}\bigl(P_{ij} - \mathbf{1}_{j&#x3D;y_i}\bigr).<br>$$</li>
</ul>
</li>
<li><p>SVM&#x2F;Hinge Loss（样例）：<br>$$<br>L_i &#x3D; \sum_{j\neq y_i}\max\big(0,; S_{ij}-S_{i,y_i} + \Delta\big).<br>$$</p>
</li>
</ul>
<div align="center"><img src="/img/image-18.png" alt="图示" width="80%" /></div>

<h2 id="4-反向传播（链式法则）的概念"><a href="#4-反向传播（链式法则）的概念" class="headerlink" title="4) 反向传播（链式法则）的概念"></a>4) 反向传播（链式法则）的概念</h2><ul>
<li>标量链式法则（示意）：<div align="center"><img src="/img/image-20.png" alt="图示" width="80%" /></div><br>$$<br>\frac{dL}{dx} &#x3D; \frac{dL}{dy}\cdot\frac{dy}{dx}.<br>$$</li>
<li>向量&#x2F;矩阵情形（用雅可比的矩阵-向量乘法表示）：若 $$y&#x3D;f(x)\in\mathbb{R}^m$$ 且 $$L$$ 标量，<br>$$<br>\nabla_x L &#x3D; J_{f}(x)^\top \nabla_y L,<br>$$<br>其中 $$J_f(x)$$ 是 $$y$$ 关于 $$x$$ 的雅可比矩阵。实践中不显式构造雅可比，而用“上游梯度 × 局部梯度”隐式计算。</li>
</ul>
<h2 id="5-常见层与它们的反向公式（必须熟练）"><a href="#5-常见层与它们的反向公式（必须熟练）" class="headerlink" title="5) 常见层与它们的反向公式（必须熟练）"></a>5) 常见层与它们的反向公式（必须熟练）</h2><p>（说明：均假设 minibatch 与平均损失约定，注意是否含 1&#x2F;N 因子）</p>
<ul>
<li><p>Affine（线性）层： $$Y &#x3D; XW + \mathbf{1}b^\top$$</p>
<ul>
<li>形状： $$X(N\times D),; W(D\times M),; Y(N\times M)$$</li>
<li>反向：<br>$$<br>dX &#x3D; dY, W^\top,\qquad<br>dW &#x3D; X^\top dY,\qquad<br>db &#x3D; \sum_{i&#x3D;1}^N dY_{i,\cdot}.<br>$$</li>
</ul>
</li>
<li><p>ReLU（逐元素）： $$Z &#x3D; \mathrm{ReLU}(X)&#x3D;\max(0,X)$$</p>
<ul>
<li>导数（逐元素）：<br>$$<br>\frac{\partial Z_{ij}}{\partial X_{ij}} &#x3D; \mathbf{1}<em>{X</em>{ij}&gt;0}.<br>$$</li>
<li>反向（给上游梯度 $$dZ$$）：<br>$$<br>dX &#x3D; dZ \odot \mathbf{1}_{X&gt;0}.<br>$$</li>
</ul>
</li>
<li><p>Sigmoid：<br>$$<br>\sigma(x)&#x3D;\frac{1}{1+e^{-x}},\qquad \sigma’(x)&#x3D;\sigma(x)\big(1-\sigma(x)\big).<br>$$</p>
</li>
<li><p>Softmax + Cross-Entropy（再强调其作为 loss 的 dS）：<br>$$<br>dS &#x3D; \frac{1}{N}\bigl(P - Y_{\text{onehot}}\bigr).<br>$$</p>
</li>
<li><p>L2 正则化（若损失包括 $$\tfrac{\lambda}{2}|W|_F^2$$）：<br>$$<br>\frac{\partial}{\partial W}\Big(\tfrac{\lambda}{2}|W|_F^2\Big)&#x3D;\lambda W.<br>$$</p>
</li>
<li><p>二层网络反向（汇总步骤）：</p>
<ol>
<li>已得 $$dS$$（上面 softmax 给出）；</li>
<li>$$dW_2 &#x3D; H^\top dS + \lambda W_2$$；</li>
<li>$$db_2 &#x3D; \sum_{i} dS_{i,\cdot}$$；</li>
<li>$$dH &#x3D; dS, W_2^\top$$；</li>
<li>$$dZ^{(1)} &#x3D; dH \odot f’(Z^{(1)})$$（例如 ReLU mask）；</li>
<li>$$dW_1 &#x3D; X^\top dZ^{(1)} + \lambda W_1$$；</li>
<li>$$db_1 &#x3D; \sum_i dZ^{(1)}_{i,\cdot}$$.</li>
</ol>
</li>
</ul>
<h2 id="6-数值梯度检查（debug-必备）"><a href="#6-数值梯度检查（debug-必备）" class="headerlink" title="6) 数值梯度检查（debug 必备）"></a>6) 数值梯度检查（debug 必备）</h2><ul>
<li>中心差分估计（对参数向量 $$\theta$$ 第 $$i$$ 个分量）：<br>$$<br>g_i^{\text{num}} \approx \frac{f(\theta+\varepsilon e_i)-f(\theta-\varepsilon e_i)}{2\varepsilon},\quad \varepsilon\approx 10^{-5}.<br>$$</li>
<li>相对误差检查：<br>$$<br>\text{rel_err} &#x3D; \frac{|g^{\text{num}} - g^{\text{ana}}|}{\max(1,;|g^{\text{num}}|,;|g^{\text{ana}}|)}.<br>$$<br>若 rel_err 很小（如 $$10^{-7}$$～$$10^{-5}$$），实现通常没问题。</li>
</ul>
<h2 id="7-常用优化器（关键更新公式）"><a href="#7-常用优化器（关键更新公式）" class="headerlink" title="7) 常用优化器（关键更新公式）"></a>7) 常用优化器（关键更新公式）</h2><ul>
<li>SGD（批量或 mini-batch）：<br>$$<br>\theta \leftarrow \theta - \eta \nabla_\theta L.<br>$$</li>
<li>Momentum（常用形式）：<br>$$<br>v_t &#x3D; \mu v_{t-1} - \eta g_t,\qquad \theta_t &#x3D; \theta_{t-1} + v_t,<br>$$<br>其中 $$g_t&#x3D;\nabla_\theta L$$ 在 timestep $$t$$ 估计，$$\mu$$ 通常 0.9。</li>
<li>Adam（常用 form）：<br>$$<br>m_t &#x3D; \beta_1 m_{t-1} + (1-\beta_1) g_t,\quad<br>v_t &#x3D; \beta_2 v_{t-1} + (1-\beta_2) g_t^2,<br>$$<br>偏差修正：<br>$$<br>\hat m_t &#x3D; \frac{m_t}{1-\beta_1^t},\quad \hat v_t &#x3D; \frac{v_t}{1-\beta_2^t},<br>$$<br>参数更新：<br>$$<br>\theta_t &#x3D; \theta_{t-1} - \eta\frac{\hat m_t}{\sqrt{\hat v_t}+\epsilon},<br>$$<br>典型超参数：$$\beta_1&#x3D;0.9,\ \beta_2&#x3D;0.999,\ \epsilon&#x3D;10^{-8},\ \eta\approx10^{-3}$$。</li>
</ul>
<h1 id="第五课-·-CNN图像分类"><a href="#第五课-·-CNN图像分类" class="headerlink" title="第五课 · CNN图像分类"></a>第五课 · CNN图像分类</h1><h3 id="1-总体目标-损失（快速回顾）"><a href="#1-总体目标-损失（快速回顾）" class="headerlink" title="1) 总体目标 &#x2F; 损失（快速回顾）"></a>1) 总体目标 &#x2F; 损失（快速回顾）</h3><ul>
<li>目标：给定数据集 $$(x_i,y_i)$$，训练端到端的网络 $$f(x; \Theta)$$ 最小化平均损失（以 softmax + cross-entropy 为例）：<br>$$<br>L(\Theta)&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^{N}\ell\big(f(x_i;\Theta),y_i\big) + \frac{\lambda}{2}\sum_{k}|W_k|_F^2.<br>$$</li>
</ul>
<h3 id="2-卷积层：定义与前向计算（张量索引形式）"><a href="#2-卷积层：定义与前向计算（张量索引形式）" class="headerlink" title="2) 卷积层：定义与前向计算（张量索引形式）"></a>2) 卷积层：定义与前向计算（张量索引形式）</h3><ul>
<li><p>输入与权重的常见张量形状：</p>
<ul>
<li>输入（单张图像）： $$X \in \mathbb{R}^{C_{\text{in}}\times H \times W}$$</li>
<li>卷积核（滤波器）： $$W \in \mathbb{R}^{C_{\text{out}}\times C_{\text{in}}\times K_h \times K_w}$$</li>
<li>Bias： $$b \in \mathbb{R}^{C_{\text{out}}}$$</li>
</ul>
</li>
<li><p>单样本、单位置的点积表示（stride 单一 S，padding P）：<br>$$<br>Y_{o,i,j}&#x3D;\sum_{c&#x3D;1}^{C_{\text{in}}}\sum_{u&#x3D;1}^{K_h}\sum_{v&#x3D;1}^{K_w}<br>X_{c,; i\cdot S + u - P_h,; j\cdot S + v - P_w}; W_{o,c,u,v} + b_o,<br>$$<br>这里索引要对应边界处理（padding）与 stride 的定义。注意：很多框架实现的是 cross-correlation（不翻转核），但数学上常仍称其为 “convolution”。</p>
</li>
<li><p>输出尺寸（高度 &#x2F; 宽度）通用公式（向下取整）：<br>$$<br>H_{\text{out}}&#x3D;\left\lfloor\frac{H_{\text{in}} - K_h + 2P_h}{S}\right\rfloor + 1,\qquad<br>W_{\text{out}}&#x3D;\left\lfloor\frac{W_{\text{in}} - K_w + 2P_w}{S}\right\rfloor + 1.<br>$$<br>常见“same” padding 的设置：若 $$K$$ 为奇数，取 $$P&#x3D;(K-1)&#x2F;2$$ 可使输出与输入空间尺寸相同（当 $$S&#x3D;1$$ 时）。</p>
</li>
<li><p>权重个数（参数量）：<br>$$<br>\text{Params} &#x3D; C_{\text{out}}\times C_{\text{in}}\times K_h\times K_w ;+; C_{\text{out}} \ (\text{bias 可选}).<br>$$</p>
</li>
<li><p>每个样本的乘加（MACs）复杂度（近似）：<br>$$<br>\text{MACs} \approx H_{\text{out}} ;W_{\text{out}}; C_{\text{out}} ; (C_{\text{in}} ;K_h ;K_w).<br>$$<br>（若考虑 batch size $$N$$，则再乘以 $$N$$。）</p>
</li>
<li><p>示例（课件中的数值）：</p>
<ul>
<li>输入 $$3\times32\times32$$，10 个 $$5\times5$$ 滤波器（stride&#x3D;1, pad&#x3D;2），输出 $$10\times32\times32$$；</li>
<li>参数数：每个 filter 参数 $$&#x3D;3\times5\times5&#x3D;75$$（＋1 bias），10 个 filter → 760 个参数（含 bias）；</li>
<li>输出数 $$&#x3D;10\times32\times32&#x3D;10240$$ 个输出点，每个输出做 75 次乘加 → 总乘加约 $$75\times10240&#x3D;768000$$。</li>
</ul>
</li>
</ul>
<p>工程提示：</p>
<ul>
<li>若不想改变尺寸常用 $$K&#x3D;3, P&#x3D;1, S&#x3D;1$$（3×3 same conv）。</li>
<li>1×1 卷积：$$K_h&#x3D;K_w&#x3D;1$$，用于跨通道线性组合（参数量少，常用于降&#x2F;升维）。</li>
</ul>
<h3 id="3-池化层（Pooling）"><a href="#3-池化层（Pooling）" class="headerlink" title="3) 池化层（Pooling）"></a>3) 池化层（Pooling）</h3><ul>
<li>Pooling 是无参数操作，常见 max pooling 或 average pooling。</li>
<li>输出尺寸（无 padding 的常见形式）：<br>$$<br>H_{\text{out}}&#x3D;\left\lfloor\frac{H_{\text{in}} - F}{S}\right\rfloor + 1,\qquad<br>W_{\text{out}}&#x3D;\left\lfloor\frac{W_{\text{in}} - F}{S}\right\rfloor + 1,<br>$$<br>其中 $$F$$ 为池化核大小，$$S$$ 为步幅（常见 $$F&#x3D;2,S&#x3D;2$$ → 下采样 2 倍）。</li>
<li>作用：下采样（减小特征图尺寸）、引入对小平移的不变性（invariance）。</li>
<li>无可学习参数（参数量 &#x3D; 0）。</li>
</ul>
<h3 id="4-感受野（Receptive-Field）"><a href="#4-感受野（Receptive-Field）" class="headerlink" title="4) 感受野（Receptive Field）"></a>4) 感受野（Receptive Field）</h3><ul>
<li>概念：网络中某个输出单元在输入图像上“看到”的像素范围（窗口大小）。</li>
<li>简单情形（相同 kernel K、stride&#x3D;1）：经过 $$L$$ 个连续 $$K\times K$$ 卷积层的感受野为<br>$$<br>\text{RF} &#x3D; 1 + L\cdot (K-1).<br>$$<br>（这是课件中给出的结论，当 stride &#x3D; 1 且无下采样时成立）</li>
</ul>
<div align="center"><img src="/img/image-21.png" alt="图示" width="80%" /></div>

<h3 id="5-卷积的两大优点（与-FC-比较）"><a href="#5-卷积的两大优点（与-FC-比较）" class="headerlink" title="5) 卷积的两大优点（与 FC 比较）"></a>5) 卷积的两大优点（与 FC 比较）</h3><ul>
<li>局部连接（local connectivity）：参数只连接局部感受野 → 参数大幅减少。</li>
<li>权重共享（parameter sharing）：同一 filter 在空间上复用 → 学到的特征具有全局复用性（可检测同一模式不同位置）。</li>
<li>结果：保留空间结构，学习到从边缘→纹理→物体部件的层次特征。</li>
</ul>
<h3 id="6-下采样（Downsampling）：stride-conv-vs-pooling"><a href="#6-下采样（Downsampling）：stride-conv-vs-pooling" class="headerlink" title="6) 下采样（Downsampling）：stride conv vs pooling"></a>6) 下采样（Downsampling）：stride conv vs pooling</h3><ul>
<li>Strided convolution：通过 stride $$S&gt;1$$ 使 $$H_{\text{out}} &lt; H_{\text{in}}$$（见输出尺寸公式）。</li>
<li>池化（pooling）：常用 max pooling（K&#x3D;2,S&#x3D;2）实现 2× 下采样且无参数。</li>
<li>在现代架构中常使用 strided conv 替代 pooling（或两者组合），以便学习到更灵活的下采样方式。</li>
</ul>
<div align="center"><img src="image-22.png" alt="alt text" style="width:60%;" /></div>


<hr>
<h2 id="深度学习内容回顾"><a href="#深度学习内容回顾" class="headerlink" title="深度学习内容回顾"></a>深度学习内容回顾</h2><ol>
<li>将你的问题编码为 y &#x3D; f(x)，其中 x 和 y 是数字网格。获取一组 (x, y) 对的数据集 <img src="image-23.png" alt="alt text" style="width:60%;vertical-align:middle;" /></li>
<li>定义一个损失函数 L(ypred, ygt)，用一个数值来衡量预测的正确性 <img src="image-24.png" alt="alt text" style="width:40%;vertical-align:middle;" /></li>
<li>定义一个计算图，使用可学习的权重 w 从 x 预测 y <img src="image-25.png" alt="alt text" style="width:60%;vertical-align:middle;" /></li>
<li>使用反向传播计算梯度 dL&#x2F;dw</li>
<li>使用优化算法找到最小化损失的 w</li>
</ol>
<h1 id="第六课-·-CNN网络训练与架构"><a href="#第六课-·-CNN网络训练与架构" class="headerlink" title="第六课 · CNN网络训练与架构"></a>第六课 · CNN网络训练与架构</h1><h2 id="CNN网络训练与架构"><a href="#CNN网络训练与架构" class="headerlink" title="CNN网络训练与架构"></a>CNN网络训练与架构</h2><h2 id="1-如何构建卷积神经网络"><a href="#1-如何构建卷积神经网络" class="headerlink" title="1. 如何构建卷积神经网络"></a>1. 如何构建卷积神经网络</h2><div align="center"><img src="/img/image-27.png" alt="图示" width="80%" /></div>

<h3 id="一、CNN的重要组成部分"><a href="#一、CNN的重要组成部分" class="headerlink" title="一、CNN的重要组成部分"></a>一、CNN的重要组成部分</h3><h4 id="1-卷积层："><a href="#1-卷积层：" class="headerlink" title="1. 卷积层："></a>1. 卷积层：<div align="center"><img src="/img/image-29.png" alt="图示" width="80%" /></div></h4><h4 id="2-池化层："><a href="#2-池化层：" class="headerlink" title="2. 池化层："></a>2. 池化层：<div align="center"><img src="/img/image-30.png" alt="图示" width="80%" /></div></h4><h4 id="3-全连接层："><a href="#3-全连接层：" class="headerlink" title="3. 全连接层："></a>3. 全连接层：<div align="center"><img src="/img/image-31.png" alt="图示" width="80%" /></div></h4><h4 id="4-归一化层："><a href="#4-归一化层：" class="headerlink" title="4. 归一化层："></a>4. 归一化层：</h4><div align="center"><img src="/img/image-32.png" alt="图示" width="80%" /></div>
<div align="center"><img src="/img/image-34.png" alt="图示" width="80%" /></div>

<h4 id="5-Dropout正则化："><a href="#5-Dropout正则化：" class="headerlink" title="5. Dropout正则化："></a>5. Dropout正则化：</h4><div align="center"><img src="/img/image-35.png" alt="图示" width="80%" /></div>
训练时使用蒙版
测试时使用完整网络输出

<div align="center"><img src="/img/image-36.png" alt="图示" width="80%" /></div>

<h3 id="二、激活函数："><a href="#二、激活函数：" class="headerlink" title="二、激活函数："></a>二、激活函数：</h3><ol>
<li>sigmoid将数据压缩在[0,1]范围内<br>大的正值或负值会“消灭”梯度。多层 sigmoid 实际上会导致梯度越来越小</li>
</ol>
<div align="center"><img src="/img/image-37.png" alt="alt text" style="width:60%;" /></div>
<div align="center"><img src="/img/image-38.png" alt="alt text" style="width:60%;" /></div>
2. 修正线性单元：x<0时为0

<div align="center"><img src="/img/image-39.png" alt="alt text" style="width:60%;" /></div>

<ol>
<li>GELU（高斯误差线性单元）<br>比 ReLU 计算成本更高－大负值仍可能使梯度接近 0</li>
</ol>
<div align="center"><img src="/img/image-40.png" alt="alt text" style="width:60%;" /></div>
这些激活函数通常放在线性算子（前馈/线性层、卷积层等）之后。

<h3 id="三、CNN架构"><a href="#三、CNN架构" class="headerlink" title="三、CNN架构"></a>三、CNN架构</h3><p>因为三个 3x3 卷积（步长为 1）层的堆叠具有与一个 7x7 卷积层相同的有效感受野，更深，参数更少，所以尽可能选择更小的滤波器</p>
<div align="center"><img src="/img/image-41.png" alt="图示" width="80%" /></div>

<ol>
<li>ResNet 要解决的核心问题（为什么提出 ResNet？）</li>
</ol>
<ul>
<li>目标：训练更深的卷积网络以提升表示能力与最终精度。经验与理论显示，更深网络（在容量上）可以拟合更复杂函数。</li>
<li>观察到的问题（Degradation）：当简单把层数加深时（在没有其它结构变化的“plain”网络上），训练误差反而变差（即训练误差上升），说明更深网络变得更难优化，而不是单纯的过拟合问题。<ul>
<li>直观：深层网络难以训练，梯度传播困难、优化路径复杂。</li>
</ul>
</li>
<li>因此需要一个结构，使得“加层”不会使网络更难以至少保持性能（即能让更深网络至少不比浅网更差），并且更容易优化到好的解。</li>
</ul>
<ol start="2">
<li>核心思想：残差学习（Residual Learning）</li>
</ol>
<ul>
<li>基本重写目标函数的方式：不要直接去拟合期望的映射 $$H(x)$$，而是让网络去拟合残差函数 $$F(x) :&#x3D; H(x) - x$$。于是期望的映射由下式给出：<br>$$<br>H(x) &#x3D; F(x) + x.<br>$$</li>
<li>实现方式：在若干连续的层（一个 residual block）中，将普通的卷积串（带 BN、ReLU 等）视作 $$F(x)$$，再把输入 $$x$$ 通过一个“shortcut”（跳跃连接，通常是恒等映射或 1×1 卷积）加回去。</li>
<li>这样，如果最优的 $$H(x)$$ 恰好是恒等映射（或接近恒等），网络只需把 $$F(x)$$ 学成零，容易实现；而普通网络必须学出复杂的恒等映射参数，反而更难。</li>
</ul>
<p>以数学形式表示一个残差块（简单情形）：<br>$$<br>y &#x3D; F(x;W) + x,<br>$$<br>其中 $$F(x;W)$$ 是 residual branch（例如两个 3×3 conv + BN + ReLU 串联），$$x$$ 是 shortcut branch（恒等或线性变换）。</p>
<div align="center"><img src="/img/image-42.png" alt="图示" width="80%" /></div>

<h3 id="四、初始化神经网络层的权重"><a href="#四、初始化神经网络层的权重" class="headerlink" title="四、初始化神经网络层的权重"></a>四、初始化神经网络层的权重</h3><ol>
<li>初始化神经网络层的权重为什么重要？<ul>
<li><p>详细说明：</p>
<ul>
<li>初始化设置直接影响前向激活与反向梯度的尺度传播（若不合适会导致“梯度消失&#x2F;爆炸”或激活逐层衰减&#x2F;发散）。  </li>
<li>因此需要一种合理的初始化策略来在网络深度方向尽量保持激活&#x2F;梯度尺度稳定（或至少不呈指数级衰减&#x2F;增长）。</li>
</ul>
</li>
<li><p>若初始权重值太小，前向传播时激活会逐层衰减，深层激活将接近 0（dead network）。</p>
<ul>
<li>这页通常用示意图或激活直方图说明：越靠近深层的激活越窄、逼近零。  </li>
<li>导致的问题不仅是前向激活消失，反向时由于许多 ReLU 的梯度为 0（死亡 ReLU），梯度无法穿透网络，训练停滞。</li>
</ul>
</li>
</ul>
</li>
</ol>
  <div align="center"><img src="/img/image-43.png" alt="图示" width="80%" /></div>

<ul>
<li>若初始权重值太大，前向激活会逐层放大，可能产生数值溢出或梯度爆炸，训练不稳定。<ul>
<li>对非线性（sigmoid&#x2F;tanh）函数：大输入会进入饱和区，使导数接近 0 → 反而导致梯度消失。  </li>
<li>对 ReLU：大输入不会饱和（正区间线性），但激活增大会导致梯度量级变大，训练震荡甚至数值溢出。  </li>
<li>因此不同激活对初始化 scale 的“容忍度”不同（ReLU 更能容忍正区域但总体仍需控制方差）。</li>
</ul>
</li>
</ul>
  <div align="center"><img src="/img/image-44.png" alt="图示" width="80%" /></div>
2. 如何初始化神经网络层的权重
- 需要为不同激活函数/层尺寸选择合适的初始化策略
- 详细说明：
  - 目标：在随机初始化时，使得每一层的激活方差在“前向传播”方向上尽量保持不变（或不呈指数增长/下降）。同理希望反向传播的梯度方差也不过度衰减/增大。  
  - He/Kaiming注意到 ReLU 会把一半输出变为零，因此在保持方差时要有一个因子 2 的修正（相对于不含 ReLU 的情况）。
  - 下页给出具体公式（Kaiming / MSRA 初始化）。

<ul>
<li><p>给出 He&#x2F;Kaiming 初始化公式（标准差 &#x3D; $$\sqrt{2&#x2F;\text{fan_in}}$$），并说明这是为 ReLU 修正得到的“刚好合适”的尺度。</p>
</li>
<li><p>初始化要做的是“使得激活与梯度在层与层之间不呈指数级衰减或爆炸”；对于 ReLU 家族，He&#x2F;Kaiming 初始化（std &#x3D; $$\sqrt{2&#x2F;\text{fan_in}}$$ 或均匀版 $$a&#x3D;\sqrt{6&#x2F;\text{fan_in}}$$）是标准且有效的选择；在实践中结合 BatchNorm、合理的学习率与监控激活直方图就能稳定训练深网络。</p>
</li>
</ul>
<h2 id="2-如何训练卷积神经网络"><a href="#2-如何训练卷积神经网络" class="headerlink" title="2. 如何训练卷积神经网络"></a>2. 如何训练卷积神经网络</h2><div align="center"><img src="/img/image-28.png" alt="图示" width="80%" /></div>

<h3 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1. 数据预处理"></a>1. 数据预处理</h3><ul>
<li>图像归一化简述：对每个通道进行中心化和缩放——减去每个通道的均值并除以每个通道的标准差（每个通道的统计信息 &#x3D; 3 个数字）——需要预先计算每个像素通道的均值和标准差（根据你的数据集）</li>
</ul>
<h3 id="2-数据增强"><a href="#2-数据增强" class="headerlink" title="2. 数据增强"></a>2. 数据增强</h3><ul>
<li>训练时加入一些随机随机性</li>
<li>测试时平均随机性</li>
<li>例子：<div align="center"><img src="/img/image-45.png" alt="图示" width="80%" /></div><div align="center"><img src="/img/image-46.png" alt="图示" width="80%" /></div></li>
</ul>
<h3 id="3-迁移学习"><a href="#3-迁移学习" class="headerlink" title="3. 迁移学习"></a>3. 迁移学习</h3><ul>
<li>CNN在小数据集从头训练会出现严重过拟合或训练不收敛的情况，为了解决在没有大量数据的情况下如何训练卷积神经网络的情况，提出除了迁移学习，先在大数据集（如 ImageNet）上预训练一个强模型，再把学到的表示迁移到你的小数据集上（作为特征提取器，或微调整个网络）<div align="center"><img src="/img/image-47.png" alt="图示" width="80%" /></div></li>
<li>如果数据量很少：<ul>
<li>数据集相似的情况下，只训练最后的线性层</li>
<li>数据集不相似的情况下，换模型或收集更多数据／用其它策略</li>
</ul>
</li>
<li>如果数据量很大：<ul>
<li>数据集相似的情况下，微调全部层</li>
<li>数据集不相似的情况下，微调或从头训练</li>
</ul>
<div align="center"><img src="/img/image-48.png" alt="图示" width="80%" /></div></li>
</ul>
<h3 id="4-选择超参数"><a href="#4-选择超参数" class="headerlink" title="4. 选择超参数"></a>4. 选择超参数</h3><ul>
<li>选择超参数的步骤<ul>
<li>步骤 1：检查初始损失 </li>
<li>步骤 2：在小样本上过拟合 </li>
<li>步骤 3：找到使损失降低的学习率</li>
<li>第4步：超参数粗略网格，训练约1-5个周期 </li>
<li>第5步：细化网格，延长训练时间 </li>
<li>第6步：查看损失和准确率曲线</li>
<li>第7步：转到步骤5</li>
<li>随机搜索优于 grid 搜索</li>
</ul>
</li>
<li>模型训练曲线的几种情况<ul>
<li>准确率仍在上升，需要更长时间训练<div align="center"><img src="/img/image-49.png" alt="图示" width="80%" /></div></li>
<li>训练集和验证集差距很大意味着过拟合！增加正则化或获取更多数据<div align="center"><img src="/img/image-50.png" alt="图示" width="80%" /></div></li>
<li>训练集和验证集之间没有差距意味着欠拟合：训练时间更长，可能可以使用更大的模型<div align="center"><img src="/img/image-51.png" alt="图示" width="80%" /></div></li>
</ul>
</li>
</ul>
<h1 id="第七课-·-RNN"><a href="#第七课-·-RNN" class="headerlink" title="第七课 · RNN"></a>第七课 · RNN</h1><div align="center"><img src="/img/image-52.png" alt="图示" width="80%" /></div>

<h1 id="Lecture-7：循环神经网络（RNN）"><a href="#Lecture-7：循环神经网络（RNN）" class="headerlink" title="Lecture 7：循环神经网络（RNN）"></a>Lecture 7：循环神经网络（RNN）</h1><h2 id="三、从普通（前馈）神经网络到-RNN：基本概念与公式"><a href="#三、从普通（前馈）神经网络到-RNN：基本概念与公式" class="headerlink" title="三、从普通（前馈）神经网络到 RNN：基本概念与公式"></a>三、从普通（前馈）神经网络到 RNN：基本概念与公式</h2><p>前馈网络：输入固定大小、每个输入独立处理。RNN：处理任意长度序列，通过“隐藏状态”把时间联系起来。<br>RNN的核心思想：RNN 有一个“内部状态”，会在处理序列时不断更新<div align="center"><img src="/img/image-53.png" alt="图示" width="80%" /></div></p>
<p>Vanilla（“普通”）RNN 的常用前向公式：</p>
<ul>
<li>隐藏态更新（每个时间步 t）：<br>$$<br>h_t &#x3D; \phi(W_{hx} x_t + W_{hh} h_{t-1} + b_h)<br>$$<br>其中<ul>
<li>$x_t$：当前输入（向量）；</li>
<li>$h_{t}$：当前隐藏状态（“记忆”）；</li>
<li>$W_{hx}$：输入到隐藏的权重矩阵；</li>
<li>$W_{hh}$：隐藏到隐藏的权重矩阵（跨时间共享）；</li>
<li>$\phi$：非线性（常见为 $\tanh$ 或 ReLU）；</li>
</ul>
</li>
</ul>
<div align="center"><img src="/img/image-61.png" alt="图示" width="80%" /></div>
- 输出（如果有输出）：
$$
y_t = g(W_{yh} h_t + b_y)
$$
其中 $g$ 通常是 softmax（分类）或线性回归头等。

<p>要点：</p>
<ul>
<li>同一套参数 $W_{hx}, W_{hh}, W_{yh}$ 在每个时间步重复使用（权重共享）。</li>
<li>RNN 的隐藏状态把“历史信息”压缩到一个向量里：这是模型学习如何总结过去。</li>
</ul>
<p>形象比喻：想象一个流水线工人每天接到一个物品（$x_t$），用同一套工具（同一套权重）根据手头的“记事本”（$h_{t-1}$）更新便签（$h_t$），然后决定是否把结果放到成品（$y_t$）。</p>
<div align="center"><img src="/img/image-62.png" alt="图示" width="80%" /></div>


<h2 id="四、展开（Unroll）与计算图；任务类型"><a href="#四、展开（Unroll）与计算图；任务类型" class="headerlink" title="四、展开（Unroll）与计算图；任务类型"></a>四、展开（Unroll）与计算图；任务类型</h2><ul>
<li>“展开”就是把时间维度展开成一个深网络：$h_0 \to h_1 \to \cdots \to h_T$，每一层都用同一套参数。</li>
<li>计算图展开后，我们可以用普通的反向传播来求梯度（这就是 BPTT）。</li>
</ul>
<p>常见序列任务类型：</p>
<ul>
<li>Many→Many：序列→序列（例如逐词翻译、语音识别）；</li>
</ul>
<div align="center"><img src="/img/image-54.png" alt="图示" width="80%" /></div>
- Many→One：序列→一个输出（例如情感分类）；<div align="center"><img src="/img/image-55.png" alt="图示" width="80%" /></div>
- One→Many：单一输入生成序列（例如图像→标题）；<div align="center"><img src="/img/image-56.png" alt="图示" width="80%" /></div>

<p>举例：图像描述（one→many），把图像向量 $v$ 注入到 RNN（作为初始隐藏态或作为额外输入）；然后 RNN 逐步生成每个单词。</p>
<h2 id="五、训练：反向传播通过时间（BPTT）与截断-BPTT"><a href="#五、训练：反向传播通过时间（BPTT）与截断-BPTT" class="headerlink" title="五、训练：反向传播通过时间（BPTT）与截断 BPTT"></a>五、训练：反向传播通过时间（BPTT）与截断 BPTT</h2><p>标准 BPTT：</p>
<ul>
<li>对整个序列做一次前向传播，计算损失（例如对每个时间步求交叉熵求和）；</li>
<li>将计算图在时间维度上展开，做一次反向传播，从最后的时间步逆推到最前；</li>
<li>这个过程在数学上与在一个非常深的前馈网络上做反向传播是等价的。</li>
</ul>
<div align="center"><img src="/img/image-57.png" alt="图示" width="80%" /></div>
梯度在时间上是如何传递的（链式法则）：
$$
\frac{\partial L}{\partial h_{t-k}} = \left(\prod_{i=1}^{k} \frac{\partial h_{t-i+1}}{\partial h_{t-i}} \right) \frac{\partial L}{\partial h_t}
$$
每一步的雅可比矩阵（大致）与 $W_{hh}$ 有关，因此多步相乘会导致数值问题（详见下节）。

<p>截断 BPTT（Truncated BPTT）：</p>
<ul>
<li>若序列很长，直接对全部时间步 BPTT 昂贵且梯度容易退化；</li>
<li>常用方法：把序列切分为长度为 $K$ 的小片段。前向时隐藏状态从一段传到下一段，但反向时只回溯 $K$ 步（每段内部反向传播到段头）。</li>
<li>优点：节省内存&#x2F;计算；可在线流式训练（carry 隐藏态但只反向有限步）。</li>
<li>注意：截断会丢失很远时间步的精确梯度（是近似训练）。</li>
</ul>
<div align="center"><img src="/img/image-58.png" alt="图示" width="80%" /></div>
直观比喻：BPTT 是把时间轴上所有影响都追溯一遍；截断就是只回头看最近几天的影响而忽略很久以前的细节（折中）。



<h2 id="六、示例：字符级语言模型（Char-RNN）"><a href="#六、示例：字符级语言模型（Char-RNN）" class="headerlink" title="六、示例：字符级语言模型（Char-RNN）"></a>六、示例：字符级语言模型（Char-RNN）</h2><p>核心思想：</p>
<ul>
<li>每个字符是来自词表（vocabulary）的一维 one-hot 向量；</li>
<li>通常先将 one-hot 矢量映射到 embedding 空间（低维稠密向量）；</li>
<li>RNN 根据当前嵌入和前一隐藏态更新隐藏态，输出一个对下一个字符的概率分布（softmax）。</li>
</ul>
<div align="center"><img src="/img/image-59.png" alt="图示" width="80%" /></div>
一条重要事实（embedding 与 one-hot）：
- 若 embedding 矩阵为 $E \in \mathbb{R}^{d\times V}$，字符 $j$ 的 one-hot 向量为 $x$，则
$$
E x = E_{:,j}
$$
即乘以 one-hot 向量只是选取第 $j$ 列（嵌入向量）。

<p>训练与采样：</p>
<ul>
<li>训练时常用“teacher forcing”：在时间步 $t$ 输入真实前一个字符（而非模型生成），损失通常为每步的交叉熵并求和；</li>
<li>生成（采样）时：从 softmax 概率中采样或取 argmax，得到下一个字符并将其作为下一步输入；</li>
<li>softmax：<br>$$<br>\text{softmax}(z)_i &#x3D; \frac{e^{z_i}}{\sum_j e^{z_j}}.<br>$$</li>
</ul>
<p>Karpathy 的 min-char-rnn（经典小实现）展示了：只靠一个很小的 RNN、并且“train more”，可以学到很多语言模式（例如缩进、括号匹配、字符串结束等），并能产生可读的文本片段。</p>
<p>直观：训练就是教会模型“下一步最可能是什么字符”；采样时模型像写字的人一样，一步步自己决定下一个字符并继续写下去。</p>
<h2 id="七、RNN-的优点与缺点（幻灯片中也有总结）"><a href="#七、RNN-的优点与缺点（幻灯片中也有总结）" class="headerlink" title="七、RNN 的优点与缺点（幻灯片中也有总结）"></a>七、RNN 的优点与缺点（幻灯片中也有总结）</h2><p>优点：</p>
<ul>
<li>可处理任意长度序列（理论上无限上下文）；</li>
<li>参数量不依赖于序列长度（权重共享）；</li>
<li>能在时间维度上复用相同的处理方式（模型有「时间上的对称性」）。</li>
</ul>
<p>缺点：</p>
<ul>
<li>逐步（sequential）计算较慢，不易并行化（相比 Transformer 并行能力弱）；</li>
<li>在实践中难以捕捉非常长距离的依赖（vanishing &#x2F; exploding gradients）；</li>
<li>实际训练往往需要技巧（截断 BPTT、梯度裁剪、门控单元等）。</li>
</ul>
<h2 id="八、梯度消失与爆炸：直观-数学解释"><a href="#八、梯度消失与爆炸：直观-数学解释" class="headerlink" title="八、梯度消失与爆炸：直观 + 数学解释"></a>八、梯度消失与爆炸：直观 + 数学解释</h2><p>直观比喻：</p>
<ul>
<li>梯度连续相乘就像每天都乘以一个系数：如果系数小于 1，长期乘下来趋近于 0（信息被“压扁”——消失）；如果系数大于 1，会呈指数增长（爆炸）。</li>
<li>想象反向传播时梯度从现在往过去传：每跨一步都乘以某个矩阵（或标量），多步乘下来就可能变得非常小或非常大。</li>
</ul>
<p>数学上（简化线性情况帮助理解）：</p>
<ul>
<li>线性 RNN：$h_t &#x3D; W h_{t-1}$. 则<br>$$<br>\frac{\partial h_t}{\partial h_{t-n}} &#x3D; W^n.<br>$$</li>
<li>若 $W$ 的谱半径（最大特征值模） $\rho(W) &lt; 1$，则 $W^n \to 0$（梯度消失）；若 $\rho(W) &gt; 1$，$W^n$ 会指数增长（梯度爆炸）。</li>
<li>对于带非线性的真实 RNN，雅可比矩阵为<br>$$<br>\frac{\partial h_t}{\partial h_{t-1}} \approx \text{diag}(\phi’(a_t)) , W_{hh},<br>$$<br>逐步相乘仍会带来数值问题。</li>
</ul>
<p>实践对策：</p>
<ul>
<li>梯度裁剪（gradient clipping）：当梯度范数过大时按比例缩放，常用规则（全局范数裁剪）：<br>$$<br>\text{if } |g|_2 &gt; \tau \quad \text{then} \quad g \leftarrow g \cdot \frac{\tau}{|g|_2}.<br>$$<br>这一策略能有效避免爆炸，但不能从根本上解决消失问题。</li>
<li>改变架构（LSTM&#x2F;GRU 等），或使用更现代的 SSM&#x2F;Transformer 架构。</li>
</ul>
<h2 id="九、LSTM（Long-Short-Term-Memory）：结构、公式与直观解释"><a href="#九、LSTM（Long-Short-Term-Memory）：结构、公式与直观解释" class="headerlink" title="九、LSTM（Long Short-Term Memory）：结构、公式与直观解释"></a>九、LSTM（Long Short-Term Memory）：结构、公式与直观解释</h2><p>LSTM 是为了解决“长期依赖难学”的难题而设计的。关键思路是引入一个显式的“细胞状态” $c_t$，并用门控机制精确控制信息的写入、遗忘与读取，从而使梯度能够沿着 $c_t$ 更平稳地流动。</p>
<p>标准 LSTM 公式：</p>
<div align="center"><img src="/img/image-60.png" alt="图示" width="80%" /></div>
其中 $\odot$ 表示逐元素乘，$\sigma$ 是 sigmoid 激活（输出在 $(0,1)$）。

<p>直观比喻（非常重要）：</p>
<ul>
<li>将 $c_t$ 想象为一条输送带（conveyor belt）——信息沿着带传递；</li>
<li>门（$i_t, f_t, o_t$）像控制“水闸&#x2F;阀门”：决定向带上写多少、把带上已有的东西忘掉多少、并如何将带上的信息展示出来；</li>
<li>如果在很多时间步上 $f_t \approx 1$ 且 $i_t \approx 0$，那一维的 $c$ 值就几乎不变，从而能无阻碍地跨越很长时间（梯度也能直接回传），避免消失。</li>
</ul>
<p>为什么 LSTM 帮助梯度流：</p>
<ul>
<li>注意到 $c_t$ 对 $c_{t-1}$ 的导数是 $f_t$（逐元素），而不是一个大矩阵乘法。因此只要门能学到 $f_t \approx 1$，梯度就不会被矩阵连续相乘压缩掉；LSTM 为保存信息提供了「可学习的捷径」。</li>
</ul>
<h1 id="第八课-·-Attention机制与Transformers"><a href="#第八课-·-Attention机制与Transformers" class="headerlink" title="第八课 · Attention机制与Transformers"></a>第八课 · Attention机制与Transformers</h1><h3 id="第七章-Attention-Transformer"><a href="#第七章-Attention-Transformer" class="headerlink" title="第七章 Attention &amp; Transformer"></a>第七章 Attention &amp; Transformer</h3><h2 id="核心"><a href="#核心" class="headerlink" title="核心"></a>核心</h2><h3 id="Transformer架构"><a href="#Transformer架构" class="headerlink" title="Transformer架构"></a>Transformer架构</h3><div align="center"><img src="/img/image-65.png" alt="图示" width="80%" /></div>
目录（快速导航）
- 一、为什么需要 Attention（从 RNN 的 Seq2Seq 出发）
- 二、Bahdanau（加性）Attention：逐步推导与直观比喻
- 三、把 Attention 抽象成 Query / Key / Value（QKV）算子
- 四、Scaled Dot-Product Attention（矩阵形式，维度说明）
- 五、Self-Attention、Cross-Attention 与位置编码（为什么要加位置信息）
- 六、多头 Attention（Multi-Head Attention）：做什么、为什么、怎么计算
- 七、Transformer Block（完整结构：LayerNorm、残差、MLP）
- 八、计算/内存复杂度、FlashAttention 及实务技巧
- 九、Transformer 在语言/视觉中的典型用法（LLM、ViT）
- 十、常见改进与变体（Pre-norm, RMSNorm, SwiGLU, MoE…）
- 十一、常见问题答疑（FAQ / 小练习 / 推荐阅读）


<p>一、为什么需要 Attention（动机，从 Seq2Seq 出发）</p>
<ul>
<li>场景举例（直观）：<ul>
<li>任务：英文句子 → 意大利文句子（机器翻译）。常见 Seq2Seq 思路：用一个 RNN（或 LSTM&#x2F;GRU）把整句编码成一个固定向量 c，再由另一个 RNN 解码生成目标句子（译文）。</li>
</ul>
</li>
<li>问题（瓶颈）：<ul>
<li>如果输入很长（比如 T &#x3D; 1000），把所有信息压成单个固定向量 c 会造成信息丢失或难学（长期依赖被“挤出”）。</li>
<li>直观比喻：把一篇长篇文章浓缩成一句话摘要去翻译每个词——很多细节会丢失。</li>
</ul>
</li>
<li>解决思路（关键句）：<ul>
<li>在每个解码步 t，都“回头看”编码器产生的所有隐藏态 h_1..h_T，根据当前解码器状态自动决定“关注（attend）”哪个输入位置，从这些位置加权得到上下文向量 c_t。这样每一步都有一个动态的上下文，而不是单一的 c。</li>
</ul>
</li>
</ul>
<div align="center"><img src="/img/image-63.png" alt="图示" width="80%" /></div>

<p>二、Bahdanau（加性）Attention：逐步推导与直观比喻</p>
<ul>
<li>核心步骤（针对 Seq2Seq 的 attention）：<ol>
<li>对于解码器在时刻 t，已有前一时刻的解码器状态 $s_{t-1}$（query），以及编码器在每个输入位置 i 的隐藏向量 $h_i$（data vectors）。</li>
<li>计算对每个 i 的“对齐分数（alignment score）”：<br>$$e_{t,i} &#x3D; f_\text{att}(s_{t-1}, h_i).$$<br>一个常用的实现（加性 attention）是：<br>$$e_{t,i} &#x3D; v_a^\top \tanh(W_a s_{t-1} + U_a h_i)$$<br>其中 $W_a,U_a,v_a$ 是可以学习的参数向量&#x2F;矩阵。</li>
<li>把分数做 softmax 归一化得到注意力权重：<br>$$a_{t,i} &#x3D; \frac{\exp(e_{t,i})}{\sum_j \exp(e_{t,j})},\quad 0\le a_{t,i}\le1,\ \sum_i a_{t,i}&#x3D;1.$$</li>
<li>计算上下文向量（加权和）：<br>$$c_t &#x3D; \sum_i a_{t,i} h_i.$$</li>
<li>把 $c_t$ 和解码器前状态 $s_{t-1}$（或前一步输出）一起输入解码网络得到新的解码状态 $s_t$ 并生成输出 $y_t$。</li>
</ol>
</li>
<li>直观比喻（书架与检索）：<ul>
<li>把编码的每个 $h_i$ 想成书架上一排条目（每个条目是某句话或短语）；$s_{t-1}$ 是你当前的问题（query）。你计算每本书的“相关度”（alignment score），把注意力概率当成图书借阅次数的分布，然后把最相关书的内容按权重合并成一个简短摘要 $c_t$ 给解码器看——这样解码器“知道”该关注输入里的哪些部分来生成下一个词。</li>
</ul>
</li>
<li>优点：<ul>
<li>每一步都能动态选取输入的不同部分（没有把全部信息压成单向量）。</li>
<li>注意力权重易于可视化（可解释性）。</li>
<li>全过程是可微的，可以 end-to-end 学（没有额外的监督来告诉模型哪几个位置重要）。<div align="center"><img src="/img/image-64.png" alt="图示" width="80%" /></div></li>
</ul>
</li>
</ul>
<p>三、把 Attention 抽象成 Query &#x2F; Key &#x2F; Value（QKV）算子</p>
<ul>
<li>更通用的描述：Attention 可以看成一个算子，输入是一组“数据向量”（X）和一组“查询向量”（Q），输出是一组向量（Y）。实现分三个角色：<ul>
<li>Queries（查询）：用于检索信息（Q）。</li>
<li>Keys（键）：每个数据向量对应一个 key（K），表示该条目的“标签&#x2F;索引”。</li>
<li>Values（值）：每个数据向量对应一个 value（V），是最终要混合的信息。</li>
</ul>
</li>
<li>算子形式（一个 query 对所有 keys 计算相似度，得到权重，再对 values 做加权和）：<ul>
<li>对单个 query q 与每个数据向量 x_i：<ol>
<li>相似度： $e_i &#x3D; \text{sim}(q, x_i)$</li>
<li>归一化： $a_i &#x3D; \text{sof</li>
<li>tmax}(e)$</li>
<li>输出： $y &#x3D; \sum_i a_i x_i$</li>
</ol>
</li>
</ul>
</li>
<li>把 key 与 value 分开是因为我们希望用某种方式匹配（key）但把不同的信息（value）混合进输出。</li>
</ul>
<hr>
<p>四、Scaled Dot-Product Attention（矩阵形式与维度）<br>这是 Transformer（Vaswani et al., 2017）里广泛使用的形式。</p>
<ul>
<li>设：<ul>
<li>Queries: $Q \in \mathbb{R}^{N_Q\times d_k}$</li>
<li>Keys: $K \in \mathbb{R}^{N_K\times d_k}$</li>
<li>Values: $V \in \mathbb{R}^{N_K\times d_v}$</li>
</ul>
</li>
<li>计算：<ol>
<li>相似度矩阵（点积）：<br>$$E &#x3D; \frac{Q K^\top}{\sqrt{d_k}}\quad\text{（形状 }N_Q\times N_K\text{）}.$$<br>注意：除以 $\sqrt{d_k}$ 是“scale”步骤，能防止点积数值过大导致 softmax 梯度消失&#x2F;饱和（下面会解释）。</li>
<li>注意力权重：<br>$$A &#x3D; \text{softmax}(E)\quad(\text{对每行做 softmax，行和为 1}).$$</li>
<li>输出：<br>$$Y &#x3D; A V\quad(\text{形状 }N_Q\times d_v).$$</li>
</ol>
</li>
<li>为什么要除以 $\sqrt{d_k}$？<ul>
<li>若 $q,k$ 的每个分量是均值为 0、方差为 $\sigma^2$ 的独立随机变量，则点积 $q\cdot k$ 的方差会随着维度 $d_k$ 增长而增长，规模约为 $d_k\sigma^4$；除以 $\sqrt{d_k}$ 可以把数量级恒定化，使 softmax 不容易进入极端分布，从而梯度更稳定。</li>
</ul>
</li>
</ul>
<hr>
<p>五、Self-Attention、Cross-Attention 与位置编码</p>
<ul>
<li>Self-Attention（自注意力）：<ul>
<li>在 Self-Attention 中，Queries &#x2F; Keys &#x2F; Values 都来自同一组输入向量 $X$（例如一段句子的词向量或图像的 patch 向量）：<br>$$Q &#x3D; X W_Q,\quad K &#x3D; X W_K,\quad V &#x3D; X W_V.$$<br>然后做 $Y &#x3D; \text{softmax}(QK^\top&#x2F;\sqrt{d_k})V$。输出 $Y$ 的第 i 行就是输入第 i 个向量“看”整个集合后得到的新表示。</li>
<li>性质：Self-Attention 是“置换等变（permutation equivariant）”的 —— 如果你对输入序列做同样的置换（打乱顺序），输出会被相同的置换影响。换句话说，单纯 self-attention 并不知道原始序列的顺序信息。</li>
</ul>
</li>
<li>为什么需要位置编码（Positional Encoding）？<ul>
<li>因为 self-attention 本身没有顺序意识（它把输入当成一个集合），但序列问题（语言、时间、图像 patch 的 2D 位置）需要顺序信息。解决方法：在输入向量上叠加位置向量（positional encoding）。</li>
<li>两类常用位置编码：<ol>
<li>绝对位置学习（learned positional embeddings）：把每个位置学习一个向量。</li>
<li>正弦&#x2F;余弦编码（Vaswani 提出）：<br>$$\text{PE}<em>{pos,2i} &#x3D; \sin\left(\frac{pos}{10000^{2i&#x2F;d}}\right),\quad<br>  \text{PE}</em>{pos,2i+1} &#x3D; \cos\left(\frac{pos}{10000^{2i&#x2F;d}}\right).$$<br>优点：对位置之间的相对关系有一定的解析性质，并可推广到未见的更长序列位置（理论上）。</li>
</ol>
</li>
</ul>
</li>
<li>Masked Self-Attention（掩码自注意力）：<ul>
<li>在自回归语言模型（predict next token）中，为了不能“看见未来”需要屏蔽掉未来位置的注意力（把对应 E 的条目设置为 $-\infty$，使 softmax 后为 0）。这是实现 autoregressive 生成（例如 GPT）的关键。</li>
<li>直观：不要让模型“作弊”看未来单词。</li>
</ul>
</li>
</ul>
<hr>
<p>六、多头 Attention（Multi-Head Attention）</p>
<ul>
<li>概念：把 attention 分成 H 个并行的“头”（heads），每个头在较低维（head dim）上学习不同的注意力模式，最后把各头输出拼接并做线性投影，以融合不同头的信号。</li>
<li>公式（单层多头）：<ul>
<li>给定输入 $X\in\mathbb{R}^{N\times D}$，将投影为 H 个头，每个头的维度通常是 $d_h &#x3D; D&#x2F;H$（因此 $H d_h &#x3D; D$）：<br>$$Q_i &#x3D; X W_Q^{(i)},\quad K_i &#x3D; X W_K^{(i)},\quad V_i &#x3D; X W_V^{(i)}\quad (i&#x3D;1..H)$$<br>每个 $W_?$ 的形状会保证 $Q_i\in\mathbb{R}^{N\times d_h}$。</li>
<li>每个头计算注意力得：<br>$$\text{head}_i &#x3D; \text{Attention}(Q_i, K_i, V_i) &#x3D; \text{softmax}\left(\frac{Q_i K_i^\top}{\sqrt{d_h}}\right) V_i.$$</li>
<li>拼接所有头并投影回原维度：<br>$$\text{MultiHead}(X) &#x3D; \text{Concat}(\text{head}_1,\dots,\text{head}_H),W_O\quad(\in\mathbb{R}^{N\times D}).$$</li>
</ul>
</li>
<li>为什么要用多头？<ul>
<li>每个头可以学到不同类型的关系（例如词之间的短距离依赖、长距离依赖、语法关系、指代关系等）。如果只用单头，信息必须全部压在一个表示里，不够灵活。</li>
</ul>
</li>
<li>实现与效率（工程要点）：<ul>
<li>在实现上通常把三个投影合并为一次矩阵乘（QKV 统一投影），然后 reshape&#x2F;transpose 得到 H 个头再并行计算，最后 concat 回来并做一次输出投影，这是高效实现方式（可以利用矩阵乘法的 GEMM 加速）。</li>
</ul>
</li>
</ul>
<hr>
<p>七、Transformer Block（完整结构）</p>
<ul>
<li>一个标准 Transformer block（以 Pre-LN 形式为例）包含两大子层：<ol>
<li>Multi-Head Self-Attention + Residual</li>
<li>Position-wise Feed-Forward Network (MLP) + Residual<br>并在每个子层之前或之后加 Layer Normalization（Pre-LN 更稳定，很多现代实现采用 Pre-LN）。</li>
</ol>
</li>
<li>经典前向流程（Pre-LN 形式）：<ol>
<li>$X_1 &#x3D; X + \text{MultiHead}(\text{LN}(X))$</li>
<li>$Y &#x3D; X_1 + \text{FFN}(\text{LN}(X_1))$</li>
</ol>
</li>
<li>FFN（逐位置的 MLP，作用是对每个 token 独立计算）：<ul>
<li>标准形式：两个线性层与非线性：<br>$$\text{FFN}(x) &#x3D; W_2 (\mathrm{GELU}(W_1 x + b_1)) + b_2.$$<br>常见中间维度是 $4D$（即 $W_1$ 的输出宽度通常为 $4D$）。</li>
<li>SwiGLU 变体（常用于更好的性能）：<br>$$\text{SwiGLU}(x) &#x3D; (x W_a) \odot \mathrm{SiLU}(x W_b),\quad \text{FFN}(x) &#x3D; \text{SwiGLU}(x) W_c.$$<br>简单理解为：用门控机制把两个投影相乘，然后再投影回去。</li>
</ul>
</li>
<li>训练&#x2F;架构小结：<ul>
<li>Transformer 的大部分计算开销在 Self-Attention（4 个 matmul）和 FFN（2 个 matmul），一层 block 大约 6 次主要矩阵乘法。</li>
<li>多层堆叠构成完整模型（Encoder&#x2F;Decoder 或 Decoder-only），原始 Transformer 12 层、D&#x3D;1024 等。</li>
</ul>
</li>
</ul>
<hr>
<p>八、计算&#x2F;内存复杂度、FlashAttention 与实践注意点</p>
<ul>
<li>复杂度：<ul>
<li>Self-Attention 的计算复杂度主要在 QK^T：如果序列长度为 $N$、模型维度为 $D$，则多头 attention 的时间复杂度约为 $O(H N^2 d_h) &#x3D; O(N^2 D)$，内存（保存相似度矩阵）也需要 $O(N^2)$。</li>
<li>这对长序列（例如 N &#x3D; 100k）来说不可行：例如幻灯片举例若 N &#x3D; 100k 且 H &#x3D; 64，则 attention 矩阵会非常大（TB 级别）。</li>
</ul>
</li>
<li>FlashAttention（Dao et al., 2022）：<ul>
<li>核心思想：用分块（tiling）+ IO-aware 的流式计算，在不存储完整 $N\times N$ 矩阵的前提下精确计算 attention，从而把内存开销从 $O(N^2)$ 降到接近 $O(N)$，并且还能提速（尤其在 GPU 上）。</li>
<li>这使得精确 attention 在更长序列上变得可行（但仍有时间&#x2F;算力上界）。</li>
</ul>
</li>
<li>工程实践要点（训练大模型时）：<ul>
<li>使用混合精度（FP16 &#x2F; BF16）来节约显存与加速。</li>
<li>使用内存优化技巧：FlashAttention、gradient checkpoint、activation recomputation。</li>
<li>优化器&#x2F;调度：AdamW、学习率 warmup（线性 warmup）再衰减（cosine 或 linear decay）。</li>
<li>Normalization 风格：Pre-LN 更易训练深层模型。</li>
<li>Dropout&#x2F;attention dropout、权重衰减等作为正则化手段。</li>
<li>对于非常大模型，常采用 Mixture-of-Experts（MoE）来把参数扩张而计算不线性增大。</li>
</ul>
</li>
</ul>
<hr>
<p>九、Transformer 在语言与视觉中的典型用法</p>
<ul>
<li>语言模型（Decoder-only，自回归）：<ul>
<li>架构：词嵌入（Embedding） → 加位置编码 → 多层 Decoder-block（每层用 Masked Self-Attention）→ 最后用投影矩阵 $W_\text{out}\in\mathbb{R}^{D\times V}$ 投影到词表 logits，然后 softmax+交叉熵预测下一个 token。</li>
<li>关键点：使用 Masked Attention 来保证只看过去。</li>
</ul>
</li>
<li>BERT 类（Encoder-only，双向）：<ul>
<li>用 full self-attention（不掩码）实现上下文的双向编码，训练任务常用 Masked Language Modeling（MLM）与下游微调。</li>
</ul>
</li>
<li>Vision Transformer（ViT）：<ul>
<li>把图片分成固定大小 patch（例如 16×16），每块 flatten 后线性投影到维度 D，得到 N 个 patch token（N &#x3D; (H&#x2F;16)*(W&#x2F;16)）。</li>
<li>把这些 patch token 作为 Transformer 的输入（再加 positional encoding）；不使用 mask（patch 间可以自由互相看）。</li>
<li>最终把所有输出 token 平均池化或用一个专门的 [CLS] token 做分类。</li>
<li>等价实现：patch embedding 可以看作一个 conv 层：kernel &#x3D; patch size, stride &#x3D; patch size。</li>
<li>注意：ViT 对数据量很敏感，通常需要大量数据或预训练（因为少了 CNN 的局部先验）。</li>
</ul>
</li>
<li>并举规模（幻灯片例子）：<ul>
<li>原始 Vaswani Transformer：12 层，D&#x3D;1024，H&#x3D;16，N&#x3D;512，大约 213M 参数。</li>
<li>GPT-2： <del>1.5B；GPT-3：</del>175B（这些只是量级参考，现代更大的模型更多采用 MoE、RMSNorm、SwiGLU 等改进）。</li>
</ul>
</li>
</ul>
<hr>
<p>十、常见改进与变体（简要）</p>
<ul>
<li>Pre-LN vs Post-LN：<ul>
<li>原始 Transformer 用 Post-LN（LayerNorm 在子层后），后来发现 Pre-LN（把 LayerNorm 放在子层前）训练更稳定，尤其是深层模型。</li>
</ul>
</li>
<li>RMSNorm（Root Mean Square Norm）：<ul>
<li>不去中心化（不减去均值），只按 RMS 缩放，定义：<br>$$\mathrm{RMS}(x) &#x3D; \sqrt{\frac{1}{D}\sum_{i&#x3D;1}^D x_i^2 + \epsilon},\quad<br>  y_i &#x3D; \gamma_i \frac{x_i}{\mathrm{RMS}(x)}.$$</li>
<li>在一些大模型中替代 LayerNorm 可以带来稳定性与实现简化。</li>
</ul>
</li>
<li>SwiGLU（GLU 变体）：<ul>
<li>一种 FFN 的激活与门控变体，常写成：<br>$$\mathrm{SwiGLU}(X) &#x3D; (XW_1) \odot \mathrm{SiLU}(XW_2),\quad<br>  \text{FFN}(X) &#x3D; \mathrm{SwiGLU}(X) W_3.$$</li>
<li>经验上比普通 GELU 更好（性能 &#x2F; 参数效率）。</li>
</ul>
</li>
<li>Mixture of Experts（MoE）：<ul>
<li>在 FFN 层引入多个“专家”子网络（每个 expert 一套权重），用一个路由器（gating network）把每个 token 分配给若干 top-k 专家来处理。这样参数爆炸（例如上万亿）但计算只在被路由到的专家上发生（稀疏激活）。</li>
<li>能极大扩张模型参数规模（提高表示能力），同时把计算控制在合理范围。</li>
</ul>
</li>
<li>FlashAttention（见上节）：在长序列上实际可行的重要技巧。</li>
</ul>
<hr>
<p>十一、常见问题（FAQ）与练习建议</p>
<p>Q1：Key &#x2F; Query &#x2F; Value 的直观意义？</p>
<ul>
<li>比喻：图书馆：key 是书的目录索引（主题标签），value 是书的具体内容（正文），query 是你提出的问题。你用 query 去匹配 key 找到相关书，然后把书的内容（value）加权汇总给你。</li>
</ul>
<p>Q2：加性 attention（Bahdanau）和点积 attention（Luong &#x2F; Transformer）有什么区别？</p>
<ul>
<li>加性（additive）使用一个小的 MLP 来评估相似度（通常在低维时更稳定）；点积（dot-product）更高效，能用矩阵乘法并行加速。Transformer 使用 scaled dot-product（在点积上除以 sqrt(d_k)）。</li>
</ul>
<p>Q3：为什么要用 multi-head 而不是更宽的单头？</p>
<ul>
<li>多头在每个低维子空间里以不同角度学习注意力，能并行捕捉多种模式。与直接把全部维度放在单头相比，multi-head 通常表现更好。</li>
</ul>
<p>Q4：Transformer 为什么能并行化？</p>
<ul>
<li>Self-attention 的计算可以同时对所有 token 进行（只需要构造 Q,K,V 与一次矩阵乘），不像 RNN 必须顺序计算隐藏态，因此 GPU 上非常高效（但 N 太大时会有 O(N^2) 的问题）。</li>
</ul>
<p>练习建议（给零基础学生的上手路径）</p>
<ol>
<li>用 NumPy 实现一个简单的 scaled dot-product attention（单头），理解每一步矩阵维度变换。</li>
<li>再实现 multi-head（手写 reshape &#x2F; transpose）。</li>
<li>用 PyTorch 的 nn.MultiheadAttention 或者 nn.Transformer 做一个小翻译 demo（En→It）或语言建模 toy dataset。</li>
<li>可视化 attention 矩阵（heatmap），看模型在不同 decode step 都关注哪些 encoder positions。</li>
<li>实现 ViT 的 patch embedding（把图片切块并线性映射），跑一个小数据集的分类实验比较 CNN vs ViT 性能（在数据量小的情况下你会看到 ViT 更依赖大数据）。</li>
</ol>
<p>参考论文（强烈推荐读原文）</p>
<ul>
<li>Bahdanau, Cho, Bengio, “Neural Machine Translation by Jointly Learning to Align and Translate”, ICLR 2015.</li>
<li>Vaswani et al., “Attention is All You Need”, NeurIPS 2017.（Transformer 原始论文）</li>
<li>Dosovitskiy et al., “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)”, ICLR 2021.</li>
<li>Dao et al., “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”, NeurIPS 2022.</li>
<li>Shazeer, “GLU Variants Improve Transformers”，2020（SwiGLU 与 GLU 讨论）。</li>
<li>Shazeer et al., “Mixture-of-Experts” 系列（大模型稀疏专家路由）。</li>
</ul>
<p>附：两个简短的代码片段（帮助理解；伪代码风格）</p>
<ol>
<li>单头 scaled dot-product attention（伪 Python &#x2F; NumPy）</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Q: (Nq, dk), K: (Nk, dk), V: (Nk, dv)</span></span><br><span class="line">E = Q @ K.T               <span class="comment"># (Nq, Nk)</span></span><br><span class="line">E = E / sqrt(dk)</span><br><span class="line">A = softmax(E, axis=<span class="number">1</span>)    <span class="comment"># normalize each row (each query&#x27;s distribution)</span></span><br><span class="line">Y = A @ V                 <span class="comment"># (Nq, dv)</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>多头 attention（思路）</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X: (N, D)</span></span><br><span class="line"><span class="comment"># W_qkv: (D, 3*D)  -&gt; one big matmul to get QKV</span></span><br><span class="line">qkv = X @ W_qkv          <span class="comment"># (N, 3*D)</span></span><br><span class="line"><span class="comment"># split into Q, K, V each (N, D), then reshape to (H, N, d_h)</span></span><br><span class="line"><span class="comment"># compute attention per head, get heads_out (H, N, d_h)</span></span><br><span class="line"><span class="comment"># transpose/reshape heads_out -&gt; (N, D)</span></span><br><span class="line"><span class="comment"># final output: O = heads_concat @ W_o  # (N, D)</span></span><br></pre></td></tr></table></figure>

<p>结语（要点回顾）</p>
<ul>
<li>Attention 是一个操作集合向量的新原语（primitive）；它把“查询（query）”与“键（key）”做相似度计算，并把“值（value）”按权重混合，柔性地在不同位置间传递信息。</li>
<li>Transformer 是把 attention 放到网络“到处”使用的架构（self-attention + layer-norm + MLP + residual）。其优点是高度并行、灵活且在许多任务上都非常强（语言、视觉、语音等）。</li>
<li>代价是注意力的 $O(N^2)$ 复杂度，但有很多优化（FlashAttention、长序列稀疏化、MoE 等）可以缓解。</li>
</ul>
<p>如果你愿意，我可以接着做下面任一项（你选一项我就给出详细实现&#x2F;演示）：</p>
<ol>
<li>用 NumPy 从零实现一个完整的小型 Transformer block（含前向与反向推导的直观说明）。  </li>
<li>给出 PyTorch 可运行的 TransformerCell &#x2F; TransformerBlock（含多头 attention、pre-norm、SwiGLU）并解释各行代码。  </li>
<li>做一个具体的可视化 demo：给一句英文与其翻译，用简单注意力模型画出 attention heatmap，并解释看到的模式。</li>
</ol>
<p>告诉我你想先看哪项，或告诉我你现在最困惑的点（例如：Q&#x2F;K&#x2F;V 到底怎么看、为什么要除以 sqrt(d)、multi-head 真有必要吗……），我把该点拆成课堂讲义 + 小练习给你。</p>
<h1 id="第九课-·-检测分割可视化与理解"><a href="#第九课-·-检测分割可视化与理解" class="headerlink" title="第九课 · 检测分割可视化与理解"></a>第九课 · 检测分割可视化与理解</h1><p>下面我把第九讲（Lecture 9: Detection, Segmentation, Visualization, and Understanding）的内容按主题整理成讲义式的讲解。目标是：既专业、有条理；又穿插通俗易懂的比喻，方便零基础的你理解关键概念与直觉。每当有简单公式或关键步骤，我都会用 LaTeX 写出（按你要求默认数学公式为 LaTeX）。</p>
<p>目录（快速导航）</p>
<ul>
<li>一、Transformer 简要回顾与视觉 Transformer（ViT）</li>
<li>二、常见的 Transformer 微调技巧（Pre‑Norm、RMSNorm、SwiGLU、MoE）</li>
<li>三、计算机视觉的主要任务概览</li>
<li>四、语义分割（Semantic segmentation）<ul>
<li>问题定义、传统滑窗、Fully Convolutional 思路</li>
<li>上采样（unpooling、transposed conv、learnable upsampling）</li>
<li>U‑Net 与 skip connections</li>
</ul>
</li>
<li>五、目标检测（Object detection）<ul>
<li>单目标回归 vs 多目标问题</li>
<li>基于候选区（region proposal）的路线：R‑CNN → Fast R‑CNN → Faster R‑CNN（包含 RPN、anchors）</li>
<li>单阶段检测器（YOLO &#x2F; SSD &#x2F; RetinaNet）与优缺点</li>
<li>基于 Transformer 的检测器：DETR（简要直观）</li>
<li>重要模块：RoI Pool &#x2F; RoI Align、非极大值抑制（NMS）</li>
</ul>
</li>
<li>六、实例分割（Instance segmentation）：Mask R‑CNN 思路</li>
<li>七、可视化与理解（Visualization &amp; Understanding）<ul>
<li>卷积核可视化、第一层滤波器</li>
<li>基于梯度的显著图（Saliency）</li>
<li>CAM 与 Grad‑CAM 的原理与公式</li>
<li>guided backprop、在 ViT 上的可视化要点</li>
</ul>
</li>
<li>八、工程实践与开源框架、课程小结与要点回顾</li>
</ul>
<hr>
<p>一、Transformer 回顾与视觉 Transformer（ViT）</p>
<ul>
<li>核心回忆（极简）：<ul>
<li>Self‑attention 使得每个输出位置可以直接“看”到输入序列中所有位置，适合长距离依赖（但计算是 O(N^2)）。</li>
<li>原始 Transformer 分为 Encoder &#x2F; Decoder，但很多视觉任务只用 Encoder（分类、检测等）。</li>
</ul>
</li>
<li>Vision Transformer（ViT）的直观做法：<ul>
<li>把一张图像按固定大小 patch（例如 16×16×3）切成 N 个小块，把每个小块“展平成一个词向量”并线性投影到 D 维，视作序列的 token（类似 NLP 的 word embedding）。也加上位置编码（positional embeddings）以保留位置信息。</li>
<li>加一个特殊的 classification token（CLS），Transformer 输出后把该 token 的输出用于分类，或对 patch 输出做平均池化后分类。</li>
</ul>
</li>
<li>通俗比喻：把图片切成“语言的单词”，让 Transformer 去“阅读”整句话（图像），注意力就是单词之间的“对话”机制。</li>
</ul>
<p>二、Transformer 的常见改进（为什么要改）<br>这些改进主要为了解决训练稳定性、更好的表示能力或更高的参数效益。</p>
<ol>
<li>Pre‑Norm（把 LayerNorm 放到残差内部）</li>
</ol>
<ul>
<li>原始 Transformer 是先 residual 加再做 LayerNorm；改为先做 LayerNorm 再进残差分支（即 Pre‑Norm），通常能让深层 Transformer 更稳定地训练。</li>
</ul>
<ol start="2">
<li>RMSNorm（Root Mean Square Layer Normalization）</li>
</ol>
<ul>
<li>形式（对向量 x ∈ R^N，每个分量 i）：<br>$$<br>\text{RMS}(x)&#x3D;\sqrt{\varepsilon + \frac{1}{N}\sum_{i&#x3D;1}^N x_i^2},\qquad<br>y_i &#x3D; \frac{x_i}{\text{RMS}(x)}\gamma_i<br>$$</li>
<li>要点：去掉了 mean subtraction，仅归一化二阶矩，并乘可学习尺度 γ。训练更稳定且在大模型中效果不错。</li>
</ul>
<ol start="3">
<li>SwiGLU（改进的 MLP 激活）</li>
</ol>
<ul>
<li>传统 MLP：$$Y&#x3D;\sigma(XW_1)W_2$$（常见的隐藏维是 4D）</li>
<li>SwiGLU 的形式（简写）：<br>$$<br>Y &#x3D; \text{Swish}(XW_1)\odot (XW_2) W_3<br>$$<br>（等价地，采用一种门控结构，保持参数量接近但提高表达能力）</li>
<li>直观：给 MLP 加了门控（像门控电闸），部分信息通过，部分信息被放大或抑制。</li>
</ul>
<ol start="4">
<li>Mixture of Experts（MoE）</li>
</ol>
<ul>
<li>在每个 Transformer 块中学 E 个不同的 MLP（专家），但对每个 token 只激活 A &lt; E 个专家，参数量大幅增加而计算开销只按被激活的专家算。</li>
<li>比喻：像一个电话客服中心，学了很多专门客服，但每个来电只被路由到少数几位专家。</li>
</ul>
<p>三、计算机视觉的主要任务（为什么分类不够）</p>
<ul>
<li>图像分类：给整张图一个 label（不关心对象个数或位置）。</li>
<li>语义分割（Semantic Segmentation）：为每个像素分配一个语义类别（不区分不同实例），结果是和输入同分辨率的像素级预测图。</li>
<li>目标检测（Object Detection）：识别图中有多少对象、它们的类别以及每个对象的边界框（bounding box）。</li>
<li>实例分割（Instance Segmentation）：在检测的基础上，为每个检测到的实例给出像素级掩码（mask）。</li>
</ul>
<p>四、语义分割：从滑窗到 FCN，再到 U‑Net</p>
<ol>
<li>问题与早期直觉</li>
</ol>
<ul>
<li>最直接的想法是 sliding window：对图像上每个像素附近的小 patch 进行分类（把中心像素归类）。缺点明显：重复计算、极慢。</li>
</ul>
<ol start="2">
<li>Fully Convolutional Network（FCN）的核心思想</li>
</ol>
<ul>
<li>用卷积网络把整张图做一次前向（共享计算），最后输出为 C × H × W 的分数图（每像素 C 类分数）。所以不再针对每个像素做独立的前向。</li>
<li>现实问题：很多分类网络通过 pooling&#x2F;stride 降采样以扩大 receptive field（感受野），但语义分割需要与输入尺寸相同的输出 → 需要在网络中做 upsampling（上采样）。</li>
</ul>
<ol start="3">
<li>上采样（Upsampling）常见方法</li>
</ol>
<ul>
<li>Unpooling（基于池化位置还原）：<ul>
<li>Nearest neighbor &#x2F; bed‑of‑nails：把小图像重复或在空格填零，简单但粗糙。</li>
<li>Max unpooling：利用 maxpool 时保存的“最大值位置”来定位还原，保留结构信息。</li>
<li>比喻：像把压缩的图“用标签还原回原来的槽位”。</li>
</ul>
</li>
<li>Transposed convolution（反卷积 &#x2F; 可学习上采样）：<ul>
<li>数学上可把卷积写成矩阵乘法，transposed convolution 是用该矩阵的转置去“放大”特征图，具有可学习的权重。</li>
<li>示例（直观）：输入的每个像素会按卷积核生成一小块输出，然后在输出重叠处相加。</li>
<li>注意事项：不当的转置卷积会出现棋盘状伪影（checkerboard artifacts）。</li>
</ul>
</li>
<li>Learnable upsampling（例如先上采样再卷积、或 transposed conv）<ul>
<li>现代实践常用上采样（bilinear）+ 卷积 或者 transposed conv，再结合 skip connections。</li>
</ul>
</li>
</ul>
<ol start="4">
<li>U‑Net（Ronneberger et al., 2015）</li>
</ol>
<ul>
<li>结构：编码（downsampling）- 解码（upsampling）对称结构，解码阶段与编码阶段的高解析度特征做拼接（skip connections）。</li>
<li>直观比喻：像一条主干路（把信息压缩提取）和很多旁路（把高分辨率细节带回来），拼接让网络既有大视野（context）又有精细定位。</li>
<li>U‑Net 在医学影像与语义分割任务中非常成功。</li>
</ul>
<p>五、目标检测：从滑窗到 RPN 和单阶段方法</p>
<ol>
<li>单目标检测（回归框 + 分类）</li>
</ol>
<ul>
<li>如果图中只有一个对象，可以把分类与边框回归放在一起训练（多任务损失）。但现实图像有可变数量的对象，输出数量不固定。</li>
</ul>
<ol start="2">
<li>直接滑窗&#x2F;多尺度分类的问题</li>
</ol>
<ul>
<li>对图像上很多位置、尺度、宽高比都做分类与回归，计算量极大。</li>
</ul>
<ol start="3">
<li>区域候选（Region Proposal）方法</li>
</ol>
<ul>
<li>先用快速的启发式算法（如 Selective Search）生成 ~2000 个候选框，再对每个候选框做精分类与回归。</li>
<li>R‑CNN（Girshick et al., 2014）：对每个候选框裁剪并 warp 成 224×224，分别送到 CNN（每个候选独立前向）→ 非常慢。</li>
<li>Fast R‑CNN（2015）：先对整张图做一次卷积，得到特征图；再对候选框在特征图上 crop+resize（RoI Pool）得到固定大小特征，最后分类与回归 → 大幅加速（共享卷积）。<ul>
<li>RoI Pool: 将候选在特征图上“对齐到网格”并在每个子格做 max‑pooling，输出固定尺寸（但存在 “snap” 导致细微不对齐）。</li>
</ul>
</li>
<li>Faster R‑CNN（2015）：在 Fast R‑CNN 的基础上引入 RPN（Region Proposal Network），让网络本身学会生成候选框（anchors），从而摆脱外部候选算法，实现端到端训练。<ul>
<li>RPN 概念：在特征图每个像素位置设 K 个预设 anchor（不同尺度与宽高比），网络为每个 anchor 预测是否是 object（得分）与 4 个回归值（dx,dy,dw,dh）。按 objectness 排序取 top‑N proposals（再 NMS）。</li>
<li>比喻：在停车场每个停车位（anchor）放一个传感器，判断有没有车并微调车位边界。</li>
</ul>
</li>
</ul>
<ol start="4">
<li>RoI Pool vs RoI Align（Mask R‑CNN 的贡献）</li>
</ol>
<ul>
<li>RoI Pool 会将候选与特征图网格“snap”到最近格点，会有对齐误差。</li>
<li>RoI Align（He et al., Mask R‑CNN）：在每个子网格内用双线性插值采样固定采样点并 pool，避免 snap 导致的对齐误差 → 对 mask &#x2F; keypoint 等像素级任务尤其重要。<ul>
<li>比喻：RoI Pool 是粗略的剪刀，RoI Align 是精确的刻刀（插值使边界更平滑）。</li>
</ul>
</li>
</ul>
<ol start="5">
<li>单阶段检测器（YOLO &#x2F; SSD &#x2F; RetinaNet）</li>
</ol>
<ul>
<li>思路：用单次网络前向直接在每个格点或 anchor 上预测类别与回归（无需二阶段的 proposal+classify）。</li>
<li>优点：速度快（适用于实时）；缺点：对小目标或定位精度可能不如两阶段方法（早期如此）。</li>
<li>RetinaNet 引入 Focal Loss 来解决类别极度不平衡问题（大量背景样本占主导）。</li>
</ul>
<ol start="6">
<li>非极大值抑制（NMS）</li>
</ol>
<ul>
<li>同一对象往往被多个框预测，NMS 依置信度保留最高的并抑制重叠度高的其他框（用 IoU 阈值）。</li>
<li>比喻：广播里只保留最响亮的声音，把重叠的回声压下去。</li>
</ul>
<ol start="7">
<li>DETR（将 Transformer 用于检测）</li>
</ol>
<ul>
<li>思路：用 Transformer 端到端直接输出一个固定数量的检测框（有些输出是“空的”表示无目标），使用二分图匹配（Hungarian）将预测和 GT 一一匹配来训练（避免 NMS 和复杂的 anchors）。</li>
<li>优点：模型简洁，端到端；缺点：训练收敛慢、对小目标较敏感（后续有很多改进）。</li>
</ul>
<p>六、实例分割（Mask R‑CNN）</p>
<ul>
<li>Mask R‑CNN 在 Faster R‑CNN 基础上，加了一个小的 mask 分支（在 RoI features 上预测 28×28 的二值 mask），同时使用 RoI Align 以保证像素级对齐。</li>
<li>同时可以扩展做姿态估计（keypoints）。</li>
<li>比喻：先检测出每辆车（检测器），然后在车的周围画上剪刀走出精确的轮廓（mask 分支）。</li>
</ul>
<p>七、可视化与理解（为什么要可视化）<br>目的：帮助我们理解模型在学什么、发现错误模式、调试和解释模型决策。</p>
<ol>
<li>第一层滤波器（filters）可视化</li>
</ol>
<ul>
<li>第一层卷积核通常类似 Gabor &#x2F; 颜色边缘检测器，直观容易解释：像视觉系统的“简单细胞”。</li>
</ul>
<ol start="2">
<li>基于梯度的显著图（Saliency maps）</li>
</ol>
<ul>
<li>对某个类的未归一化得分 S_c 关于输入像素 I 的梯度：$$\frac{\partial S_c}{\partial I}$$，取绝对值或最大通道值。高梯度说明该像素对该类的分数变化敏感 → “哪些像素重要”。</li>
<li>参考：Simonyan et al., 2014。</li>
</ul>
<ol start="3">
<li>Class Activation Mapping (CAM)</li>
</ol>
<ul>
<li>要求：网络在最后有一个 global average pooling（GAP）然后接全连接做分类。</li>
<li>定义：最后一层卷积特征 f_{h,w,k}（大小 H×W×K），FC 层权重为 w_{k,c}，则类 c 的 activation map 为：<br>$$<br>M_{c,h,w}&#x3D;\sum_{k} w_{k,c}, f_{h,w,k}<br>$$</li>
<li>直观：把每个通道的热度按权重线性组合起来得到 class‑specific 的热图。缺点：只能直接用于特殊结构（GAP + FC）。</li>
</ul>
<ol start="4">
<li>Grad‑CAM（更通用）</li>
</ol>
<ul>
<li>Grad‑CAM 可以应用于任意层（不要求 GAP 结构），核心步骤：<ol>
<li>选定某一层的激活 A ∈ R^{H×W×K}。</li>
<li>计算类分数 S_c 关于该层激活的梯度 $$\frac{\partial S_c}{\partial A_{h,w,k}}$$。</li>
<li>用梯度的空间平均得到权重 α_k：<br>$$<br>\alpha_k &#x3D; \frac{1}{H W}\sum_{h,w}\frac{\partial S_c}{\partial A_{h,w,k}}<br>$$</li>
<li>得到激活图并过 ReLU：<br>$$<br>M_{c,h,w} &#x3D; \text{ReLU}!\Big(\sum_k \alpha_k A_{h,w,k}\Big)<br>$$</li>
</ol>
</li>
<li>解释：α_k 衡量通道 k 对目标类别的重要性，然后把通道加权求和得到热图。ReLU 保证只保留对该类有正向贡献的区域。</li>
<li>Grad‑CAM 通常与 Guided Backprop 结合，得到更清晰的可视化（Grad‑CAM 给出定位，guided backprop 给出细节边缘，两者乘积可以得到高分辨率的解释图）。</li>
</ul>
<ol start="5">
<li>guided backprop（引导式反向传播）</li>
</ol>
<ul>
<li>在反向传播时，只允许正梯度通过 ReLU（即只保留对激活有正贡献的反向信息），可以得到更直观、更清晰的像素级图像，常用于与 Grad‑CAM 相乘。</li>
</ul>
<ol start="6">
<li>在 ViT 上的可视化</li>
</ol>
<ul>
<li>ViT 的 patch attention 可以可视化 attention map（patch 与 patch 之间的注意力分布），观察模型如何在不同层逐步聚焦到重要 patch。也可以用 Grad‑CAM 类似方法在 token&#x2F;patch 层做可视化。</li>
</ul>
<p>八、工程实践、开源实现与要点小结</p>
<ul>
<li>开源框架（便于实验与迁移学习）：<ul>
<li>TensorFlow Detection API（包含 Faster R‑CNN, SSD, Mask R‑CNN 等）</li>
<li>Detectron2（PyTorch，Facebook Research，包含大量主流检测&#x2F;分割模型）</li>
</ul>
</li>
<li>常见实践要点：<ul>
<li>对检测分割任务常用预训练 backbone（如 ResNet）做微调；</li>
<li>RoI Align 比 RoI Pool 在像素级任务上更好；</li>
<li>对单阶段检测，Focal Loss 可缓解正负样本极不平衡；</li>
<li>DETR 类方法训练通常需要更长时间&#x2F;更大数据（但架构更简洁）；</li>
<li>分割任务中上采样方案（bilinear+conv 或 transpose conv）要小心 checkerboard，要配合 skip connection 保留细节。</li>
</ul>
</li>
</ul>
<p>课程要点（速记）</p>
<ul>
<li>ViT：把图像分成 patch 当作 token；Transformer 在视觉上直接建长距离依赖。</li>
<li>语义分割：从滑窗到 FCN，再到 U‑Net，核心是共享计算 + 合理上采样 + skip connections。</li>
<li>目标检测路线图：滑窗 → region proposal（Selective Search）→ R‑CNN → Fast R‑CNN → Faster R‑CNN（引入 RPN）→ Mask R‑CNN（再加 mask 支路）；另一条是单阶段（YOLO&#x2F;SSD&#x2F;RetinaNet），近年又出现 Transformer-based DETR。</li>
<li>可视化工具：filters、saliency（梯度）、CAM（GAP 限制）、Grad‑CAM（通用）和 guided backprop（细节增强）。</li>
</ul>
<p>补充：几个关键公式回顾（LaTeX）</p>
<ul>
<li>RMSNorm：<br>$$<br>\text{RMS}(x)&#x3D;\sqrt{\varepsilon + \frac{1}{N}\sum_{i&#x3D;1}^N x_i^2},\qquad<br>y_i &#x3D; \frac{x_i}{\text{RMS}(x)}\gamma_i.<br>$$</li>
<li>SwiGLU（简写形式）：<br>$$<br>Y &#x3D; \text{Swish}(XW_1)\odot (XW_2) W_3.<br>$$</li>
<li>CAM：<br>$$<br>M_{c,h,w}&#x3D;\sum_k w_{k,c}, f_{h,w,k}.<br>$$</li>
<li>Grad‑CAM：<br>$$<br>\alpha_k &#x3D; \frac{1}{H W}\sum_{h,w}\frac{\partial S_c}{\partial A_{h,w,k}},\qquad<br>M_{c,h,w} &#x3D; \mathrm{ReLU}!\Big(\sum_k \alpha_k A_{h,w,k}\Big).<br>$$</li>
</ul>
<p>最后，给你三条学习建议（面向“零基础大学生”）：</p>
<ol>
<li>从图像分类开始练习，熟悉卷积、pooling、stride、padding 的直观含义；再看分割&#x2F;检测时会更容易理解 down&#x2F;up sampling 的需求。</li>
<li>在实现上先用现成的 Detectron2 &#x2F; TensorFlow API 调用预训练模型，观察输出（boxes、masks、CAM、attention），边看边做理解最有效。</li>
<li>可视化是理解模型的捷径——学会画出 feature map、CAM、Grad‑CAM，这会让你更快地“看到”模型学到了什么、哪里出错。</li>
</ol>
<p>如果你愿意，我可以：</p>
<ul>
<li>针对这次讲义中的某一部分（例如 RPN、RoI Align、Grad‑CAM）做更详细的逐步推导或代码示例（PyTorch &#x2F; TensorFlow）；</li>
<li>或把 U‑Net、Faster R‑CNN、Mask R‑CNN 的结构画成序列化步骤（伪代码风格），方便你实现&#x2F;复现。</li>
</ul>
<p>你想先从哪一块深入？</p>
<h1 id="第十课-·-视频理解"><a href="#第十课-·-视频理解" class="headerlink" title="第十课 · 视频理解"></a>第十课 · 视频理解</h1><p>下面我把你贴出的 Lecture 10（Video Understanding）的内容整理成一份讲义式、结构化的笔记。目标是：条理清楚、兼具直觉与要点，便于复习与工程实践。若需要我可以把其中某个方法做成 PyTorch&#x2F;TensorFlow 代码示例或写一篇更详细的阅读笔记（含论文实现细节）。</p>
<p>目录（快速导航）</p>
<ul>
<li>一、Video 问题概览与挑战</li>
<li>二、训练策略：为何用短 clips</li>
<li>三、视频分类的基本方案对比（Single‑frame &#x2F; Late fusion &#x2F; Early fusion &#x2F; 3D CNN &#x2F; Two‑stream）</li>
<li>四、3D CNN 与 Inflated 2D Networks（C3D &#x2F; I3D）</li>
<li>五、光流（Optical Flow）与 Two‑Stream 网络</li>
<li>六、长时序建模：RNN &#x2F; Recurrent CNN &#x2F; 注意力</li>
<li>七、空间-时间自注意力：Nonlocal Block 与 Video Transformer</li>
<li>八、自监督 &#x2F; 掩码自编码器（VideoMAE 等）</li>
<li>九、时序定位与时空检测（Temporal Action Localization &#x2F; AVA）</li>
<li>十、多模态（音画）与音源分离（视觉引导）</li>
<li>十一、高效视频理解（采样、轻量化模型）</li>
<li>十二、可视化、数据集与基准、实践要点</li>
<li>十三、关键公式速查（LaTeX）</li>
<li>十四、建议的后续动手内容与阅读</li>
</ul>
<p>一、Video 问题概览与挑战</p>
<ul>
<li>Video &#x3D; 图片 + 时间：输入张量通常表示为 T × 3 × H × W（或 3 × T × H × W）。</li>
<li>主要任务：短片动作分类（clip classification）、长视频的时序动作定位（temporal localization）、时空检测（spatio‑temporal detection，如 AVA）、实例级时空分割、音视频多模态任务等。</li>
<li>挑战：视频数据体积大、帧率高（通常 ~30fps）、计算与存储开销巨大；还需同时建模空间和时间信息（短期 motion、长期结构）。</li>
</ul>
<p>二、训练策略：为何用短 clips</p>
<ul>
<li>直接输入整段高 fps 视频不可行，常用策略：<ul>
<li>在训练时采样短 clips（例如 T&#x3D;8、16、32；H&#x3D;W&#x3D;112 或 224），通常降采样时间轴（低 fps）。</li>
<li>测试时可对视频多片段采样并平均或投票得到最终预测。</li>
</ul>
</li>
<li>原则：在时间上和空间上做折中，保证合理的时空上下文且能并行训练。</li>
</ul>
<p>三、视频分类的基本方案对比（直觉与优劣）</p>
<ol>
<li>Single‑frame baseline</li>
</ol>
<ul>
<li>把每一帧独立用 2D CNN 做分类，测试时对帧概率取平均。</li>
<li>优点：实现简单、计算便宜；往往是强基线。</li>
<li>缺点：忽略运动信息（但对很多场景外观信息已足够）。</li>
</ul>
<ol>
<li>Late Fusion</li>
</ol>
<ul>
<li>对每帧用 2D CNN 提取特征（或中高层特征），在时间维度上拼接&#x2F;池化，再用 MLP&#x2F;Linear 做分类。</li>
<li>优点：能结合多帧的高层表示；实现相对简单。</li>
<li>缺点：难以建模低层 motion（像素级&#x2F;边缘移动）。</li>
</ul>
<ol>
<li>Early Fusion</li>
</ol>
<ul>
<li>把 T 帧在 channel 维度拼接（相当于把 3×T 当作“超通道”输入），用 2D Conv 在第一层融合时间信息。</li>
<li>优点：比 single‑frame 多做一点时序融合。</li>
<li>缺点：时间融合仅在最开始层完成，可能不足以建模复杂时序结构。</li>
</ul>
<ol>
<li>3D CNN（直接在空间+时间上卷积）</li>
</ol>
<ul>
<li>用 3D 卷积核（Kt × Kh × Kw）对时空体积做逐层建模（例如 C3D、I3D、SlowFast、X3D）。</li>
<li>优点：具有时间平移不变性、能逐层累积时域感受野，能学习运动模式（可将一段短时视频视作“3D 图像”）。</li>
<li>缺点：计算&#x2F;内存开销大（3×3×3 卷积昂贵），训练需要更多数据&#x2F;计算。</li>
</ul>
<p>对比直观：</p>
<ul>
<li>Single‑frame：只看外观（fast, simple）</li>
<li>Two‑stream &#x2F; optical flow：显式把 motion 与 appearance 分离（更鲁棒地捕获动作）</li>
<li>3D CNN：从底层学时空滤波器（更通用但更费算力）</li>
<li>Transformer &#x2F; attention：直接建模长距离时空依赖（可扩展，但 token 数量与复杂度需处理）</li>
</ul>
<p>四、3D CNN 与 Inflated 2D Networks（C3D &#x2F; I3D）</p>
<ul>
<li>C3D：一种“VGG 样式”的 3D CNN（3×3×3 conv + 2×2×2 pooling），曾作为视频特征提取器广泛使用（Tran et al.）。</li>
<li>I3D（Inflated 3D）：把成熟的 2D 网络（如 Inception）“膨胀”成 3D：将每个 2D conv 替换为 Kt×Kh×Kw 的 3D conv；可用 ImageNet 的 2D 权重初始化（复制沿时间维并除以 Kt），有利于加速收敛与迁移学习。<ul>
<li>膨胀权重初始化技巧：把 2D kernel 复制 Kt 次并除以 Kt，使其在“恒定视频输入”下输出相等。</li>
</ul>
</li>
</ul>
<p>五、光流（Optical Flow）与 Two‑Stream 网络</p>
<ul>
<li>光流提供每像素的位移场 F(x,y)&#x3D;(dx,dy)，满足局部亮度守恒近似： I_{t+1}(x+dx,y+dy) ≈ I_t(x,y)。</li>
<li>Two‑Stream（Simonyan &amp; Zisserman）：一条网络处理 RGB（appearance），另一条处理堆叠的光流帧（motion）；两路融合（平均或更复杂融合）能显著提升动作识别准确率。</li>
<li>要点：光流突出运动信息，但计算光流自身也有成本；可用预计算或学得的流估计网络（RAFT 等）替代经典算法。</li>
</ul>
<p>六、长时序建模：RNN &#x2F; Recurrent CNN &#x2F; 注意力</p>
<ul>
<li>RNN &#x2F; LSTM：先用 CNN（2D&#x2F;3D）提取每片段特征，再用 LSTM 处理序列，能建模较长时间依赖（many‑to‑one 或 many‑to‑many）。</li>
<li>Recurrent Convolutional Network：把 RNN 的矩阵乘替换成卷积（按时间共享参数），保留空间结构同时获得长时序记忆（Ballas et al.）。</li>
<li>缺点：RNN 不易并行，长序列训练慢；因此近年来更倾向用注意力&#x2F;Transformer 进行并行化建模。</li>
</ul>
<p>七、空间-时间自注意力：Nonlocal Block 与 Video Transformer</p>
<ul>
<li>Nonlocal Block（Wang et al., 2018）：在 3D feature 上做全局（或稀疏）时空 attention，用 1×1×1 卷积生成 Query&#x2F;Key&#x2F;Value，把 attention 权重作用回特征图并加残差。效果类似 Transformer 的 self‑attention 在时空上的应用。</li>
<li>Self‑attention（标准）关键公式（速查）：<br>$$<br>q &#x3D; xW_q,\quad k &#x3D; xW_k,\quad v &#x3D; xW_v,\quad<br>e_{ij}&#x3D;\frac{q_j\cdot k_i}{\sqrt{d}},\quad<br>a_j&#x3D;\text{softmax}(e_{\cdot j}),\quad<br>y_j&#x3D;\sum_i a_{ij} v_i.<br>$$</li>
<li>Video Transformer（如 ViViT、MViT 等）：把帧的 patches&#x2F;token 在时空上做 attention（可做因式分解的时空注意力、分层池化以控制 token 数），在大规模预训练下性能很好。</li>
</ul>
<p>八、自监督 &#x2F; 掩码自编码器（VideoMAE 等）</p>
<ul>
<li>VideoMAE &#x2F; VideoMAE v2：把 MAE 思路扩展到视频上，通过对大比例时空 token 掩码来高效学习时空表示，适合大规模自监督预训练并能迁移到下游任务。</li>
<li>要点：掩码训练在视频上需要在空间与时间维度上设计掩码策略（连续块、随机 token 等），并可能用 dual masking（图像+音频）做多模态对齐。</li>
</ul>
<p>九、时序定位与时空检测</p>
<ul>
<li>Temporal Action Localization：在长未裁剪视频中找出不同时段对应的动作（类似 Faster R‑CNN 的思想可以移植到时间轴：proposal → classify）。</li>
<li>Spatio‑Temporal Detection &#x2F; AVA：检测视频中在空间和时间上发生的行为（需要检测人物 bbox 并对每个 bbox 在时间窗口进行分类）。这类任务更复杂、对定位精度和时序建模要求高。</li>
</ul>
<p>十、多模态（音画）与视觉引导音源分离</p>
<ul>
<li>视觉信息可以帮助进行音源分离（例如把说话人脸部的唇动与声音分离），许多工作利用帧到音频的对齐、attention 或掩码学习方式做视听分离（VisualVoice 等）。</li>
<li>视觉-音频融合策略：早期直接拼接特征、注意力瓶颈（attention bottleneck）、audio‑adaptive fusion 等。视觉提示常用于分离&#x2F;增强目标声音或做音频驱动的视频操作。</li>
</ul>
<p>十一、高效视频理解（采样、轻量化模型）</p>
<ul>
<li>采样策略：采样显著 clips 或使用智能采样器（SCSampler 等）只取关键片段以减小计算。</li>
<li>轻量模型：X3D、MoViNet、EfficientNet‑style 扩展、SCSampler、AdaMML，旨在以少量 FLOPs 达到较高性能（适合移动或实时场景）。</li>
<li>其他思路：稀疏 attention、因式分解注意力、pooling &#x2F; token reduction（MViTv2 的 pooling module）以减少 token 数。</li>
</ul>
<p>十二、可视化、数据集与基准、实践要点</p>
<ul>
<li>可视化：对视频模型可以做 class‑specific saliency（和图像类似），也可以分别可视化 appearance&#x2F;flow 的贡献、attention map（ViT）、Nonlocal 权重等，帮助理解模型关注哪个时空区域。</li>
<li>常用数据集 &#x2F;基准：<ul>
<li>UCF‑101（动作分类，小规模）</li>
<li>Sports‑1M（早期大规模）</li>
<li>Kinetics‑400 &#x2F; 600 &#x2F; 700（大规模动作分类）</li>
<li>AVA（时空动作检测）</li>
<li>Something‑Something（强调物体交互&#x2F;物理）</li>
</ul>
</li>
<li>实践要点：<ul>
<li>首先尝试 single‑frame baseline；</li>
<li>若需 motion，尝试 two‑stream（或用学得的光流）；</li>
<li>若要建模短时动态，使用 3D CNN 或 I3D；需注意 FLOPs；</li>
<li>长时依赖可加入 LSTM&#x2F;Transformer&#x2F;temporal pooling；</li>
<li>训练&#x2F;finetune 常借助 ImageNet 预训练权重（通过 I3D 的 inflate 等技巧迁移）；</li>
<li>对资源敏感时优先考虑采样策略与轻量模型（X3D&#x2F;MoViNet）。</li>
</ul>
</li>
</ul>
<p>十三、关键公式速查（LaTeX）</p>
<ul>
<li>光流的亮度守恒近似：<br>$$<br>I_{t+1}(x+dx,y+dy) \approx I_t(x,y).<br>$$</li>
<li>Self‑attention（向量化表示）：<br>$$<br>q &#x3D; xW_q,\quad k &#x3D; xW_k,\quad v &#x3D; xW_v,<br>$$<br>$$<br>e_{ij}&#x3D;\frac{q_j\cdot k_i}{\sqrt{d}},\quad<br>a_j&#x3D;\mathrm{softmax}(e_{\cdot j}),\quad<br>y_j&#x3D;\sum_i a_{ij} v_i.<br>$$</li>
<li>Nonlocal（在时空上把 THW 展平后做 attention）：<br>如果 A ∈ R^{C×T×H×W}，把空间时间维展平为 N&#x3D;THW，并令 Q,K,V ∈ R^{C’×N}（来自 1×1×1 卷积），则<br>$$<br>\text{Attention} &#x3D; \text{softmax}!\left(\frac{Q^\top K}{\sqrt{C’}}\right)V^\top,<br>$$<br>并经 1×1×1 Conv 投影回原始形状后与输入做残差相加。</li>
</ul>
<p>十四、建议的后续动手与阅读</p>
<ul>
<li>若偏工程与复现：<ul>
<li>复现 single‑frame baseline → two‑stream（使用 TV‑L1 或 RAFT 计算光流）→ I3D（inflate 权重）；</li>
<li>试试非监督预训练：VideoMAE；接着 finetune 到 Kinetics。</li>
<li>对效率敏感：试 X3D &#x2F; MoViNet 或实现 SCSampler 的采样策略。</li>
</ul>
</li>
<li>若偏研究与理解：<ul>
<li>深入阅读 Nonlocal（Wang et al. CVPR 2018）、I3D（Carreira &amp; Zisserman CVPR 2017）、Two‑stream（Simonyan &amp; Zisserman NeurIPS 2014）、VideoMAE（Tong et al. NeurIPS 2022 &#x2F; 2023 for variants）、ViViT &#x2F; MViTv2。</li>
</ul>
</li>
<li>我可以为你做的事（选一项或多项）：<ul>
<li>给出 I3D &#x2F; C3D &#x2F; Nonlocal 的 PyTorch 最小实现（含 weight inflate 的代码片段）；</li>
<li>实现 Two‑Stream 训练流程（含光流预计算脚本示例）；</li>
<li>给出 VideoMAE 的训练&#x2F;finetune pipeline 概要与配置建议；</li>
<li>针对 AVA 做时空检测 pipeline（RoI tracking + classification）的伪代码与训练要点；</li>
<li>或把本讲义翻译成英文版并配上关键论文引用 BibTeX 列表。</li>
</ul>
</li>
</ul>
<p>你想我优先把哪一块变成可运行的代码示例或更详细的读书笔记？</p>
<h1 id="第十一课-·-大规模分布式训练"><a href="#第十一课-·-大规模分布式训练" class="headerlink" title="第十一课 · 大规模分布式训练"></a>第十一课 · 大规模分布式训练</h1><p>下面我用“教授讲课”的方式，把这套第十一课的内容按逻辑、有条理地讲清楚。既保留技术要点，又尽量用通俗易懂的比喻帮助理解。适合你这种“零基础大学生”听完既明白大体概念，又能看到必要的数学表述与工程权衡。</p>
<p>目录（学习路线）</p>
<ol>
<li>本课要点概览与动机  </li>
<li>GPU 的基本结构与为什么用 GPU 训练神经网络（类比）  </li>
<li>从单卡到集群：硬件拓扑（server、rack、pod）与带宽&#x2F;内存瓶颈  </li>
<li>几种常见的并行方法（总体分类与直观比喻）  <ul>
<li>数据并行（DP）——含数学表达  </li>
<li>Fully Sharded Data Parallel &#x2F; ZeRO（FSDP）与 HSDP（混合）  </li>
<li>激活检查点（Activation Checkpointing）——内存&#x2F;计算折中与复杂度分析  </li>
<li>上下文并行（Context Parallelism，CP）  </li>
<li>流水线并行（Pipeline Parallelism，PP）与 microbatches 技巧  </li>
<li>张量并行（Tensor Parallelism，TP）及两层配合技巧</li>
</ul>
</li>
<li>如何把这些方法混合使用（ND parallelism）与优化目标：MFU（Model FLOPs Utilization）  </li>
<li>实战建议与调参 checklist（面向大模型如 Llama3-405B）  </li>
<li>小结与参考文献</li>
</ol>
<p>1）本课动机（为什么要学这些）</p>
<ul>
<li>当模型参数超过单张 GPU 能容纳的内存、或者训练速度需要成百上千张 GPU 协同工作时，我们必须把“模型”和“数据”按不同维度拆分到多张 GPU 上训练。  </li>
<li>不同的拆分策略（Batch &#x2F; Sequence &#x2F; Layer &#x2F; Hidden-dim）有不同的通信、内存、计算特性与优缺点。本课教你这些策略是什么、什么时候用、各自的代价与互补组合。</li>
</ul>
<p>2）GPU 的基本结构（直观）</p>
<ul>
<li>类比：把 GPU 想成“并行加工厂”。CPU 像单个高技能工人，GPU 是成百上千个流水线工人。Tensor Core 类似专门的高效矩阵乘法流水线。  </li>
<li>关键指标：每秒理论 FLOPs（计算能力）、HBM 内存容量与内部带宽、L2&#x2F;L1 缓存、以及 GPU 之间的互连带宽（如服务器内的 NVLink、rack 级别、pod 级别带宽差异）。  </li>
<li>例子：NVIDIA H100：很多 Tensor Cores（极高的矩阵乘法吞吐），但内存带宽增长不一定跟得上 FLOPs 增速，因此有可能“计算力过剩、内存&#x2F;通信成为瓶颈”。</li>
</ul>
<p>3）从单卡到集群（硬件拓扑）</p>
<ul>
<li>层次：GPU → server（多 GPU）→ rack（多 server）→ pod&#x2F;cluster（大量 rack）。不同层次通信带宽差异很大（例如 server 内 NVLink 非常快，rack 之间慢很多）。  </li>
<li>工程含义：尽量把频繁通信或需要低延迟的并行放在相同 server &#x2F; pod 里，减少跨慢链路通信。</li>
</ul>
<p>4）并行策略详解（每种先讲直观比喻，再讲实现要点与代价）</p>
<p>A. 数据并行（Data Parallelism, DP）</p>
<ul>
<li>比喻：想象你和 M 个同学各自做同一个菜的不同份量（不同数据批次），每个人都有一份菜谱（模型副本）。每个人分别计算损失并算梯度，最后把各自的梯度平均合并，然后每人都用合并后的梯度更新自己的菜谱（模型）。  </li>
<li>数学表达（假设每个小批量 N，M 台 GPU，共 MN 样本）：<br>LaTeX:<br>$$<br>L &#x3D; \frac{1}{MN}\sum_{i&#x3D;1}^{M}\sum_{j&#x3D;1}^{N} \ell(x_{i,j}, W)<br>$$<br>$$<br>\frac{\partial L}{\partial W}<br>&#x3D; \frac{1}{M}\sum_{i&#x3D;1}^{M} \left(\frac{1}{N}\sum_{j&#x3D;1}^{N} \frac{\partial \ell(x_{i,j},W)}{\partial W}\right)<br>$$</li>
<li>特点&#x2F;代价：<ul>
<li>简单、易实现（成熟框架支持）。</li>
<li>通信模式：每步需要跨设备同步梯度（All-Reduce），通信量与模型大小成正比。</li>
<li>缺点：模型副本占用内存，限制能训练的模型大小（单卡内存瓶颈）。</li>
</ul>
</li>
</ul>
<p>B. Fully Sharded Data Parallelism（FSDP &#x2F; ZeRO）</p>
<ul>
<li>比喻：把整个“菜谱”（模型参数）按章节分给不同同学保管（每个权重只由一个机器“拥有”），但在做某一步菜时，需要把那部分章节借给所有人使用（广播），用完后归还。归还的人同时也保管该章节对应的优化器状态（例如 Adam 的动量）与梯度。  </li>
<li>核心流程（针对一个层 i 的权重 W_i）：<ol>
<li>在前向计算层 i 之前，权重的主人把 W_i 广播到所有 GPU。  </li>
<li>所有 GPU 使用 W_i 进行前向计算后删掉本地拷贝（以节省内存），继续预取下一个权重 W_{i+1}。  </li>
<li>在反向时，权重的主人再次广播 W_i（因为反向需要它），各卡计算对 W_i 的局部梯度 dL&#x2F;dW_i 并删除本地副本。  </li>
<li>所有卡把局部梯度发送回拥有 W_i 的卡，拥有卡聚合并执行参数更新（它保有优化器状态）。</li>
</ol>
</li>
<li>优化细节：前向和反向期间交叉通信&#x2F;计算以隐藏延迟，不删除最后一个权重以避免立即重传等。  </li>
<li>优点：显著节省每卡必须存储的参数份额和优化器状态，使训练超大模型（数十亿到万亿参数）成为可能。  </li>
<li>代价：增加了更多层间通信（频繁地广播&#x2F;收集权重与梯度），需要精细的调度与工程实现（DeepSpeed FSDP、PyTorch FSDP 等）。</li>
</ul>
<p>C. Hybrid Sharded Data Parallel（HSDP）</p>
<ul>
<li>思路：把 N 张 GPU 分成 M 个组，每组 K 张（N&#x3D;M×K）。组内做 FSDP（权重在组内拆分），组间做 DP（数据并行）。  </li>
<li>类比：每个小组像一个班级内部把菜谱分开保存；不同班级之间仍旧用数据并行（同样的工作不同数据）。  </li>
<li>优点：把频繁、低延迟的通信限制在组内（通常是同一节点或 pod），组间只需较少通信（如同步梯度），适合大规模集群部署并降低跨慢链路的通信。</li>
</ul>
<p>D. 激活检查点（Activation Checkpointing）</p>
<ul>
<li>背景：Transformer 等深层模型在前向要保存大量中间激活（activations），这些激活在反向时要用，导致内存消耗巨大。  </li>
<li>核心思想：不要保存每层的所有激活，而是在反向时“重算”一部分前向的激活以节省内存。  </li>
<li>类比：你读一本很长的书，如果没法把整本书放在桌上看（内存不足），你只在若干关键页做书签（checkpoint）。做书签后再需要某页时，你回到书的前面重新翻到那一页（重算）。  </li>
<li>复杂度与权衡：<ul>
<li>全部不保存，反向时每次都从头重算：计算量会变成 O(N^2)（层数 N），但内存几乎 O(1)。</li>
<li>保存每 C 层的检查点：计算量大约 O(N^2&#x2F;C)，内存 O(C)。特别地，当 C ≈ sqrt(N) 时可以在计算和内存之间取得折中。  </li>
<li>关键公式表达（复杂度用大 O）：<br>LaTeX:<br>$$<br>\text{全重算：计算量 } O(N^2),\ \text{内存 } O(1)<br>$$<br>$$<br>\text{每 }C\text{ 层做 checkpoint：计算量 } O!\left(\frac{N^2}{C}\right),\ \text{内存 } O(C)<br>$$</li>
</ul>
</li>
<li>工程实践：与 FSDP&#x2F;HSDP 一起使用可进一步减小每卡内存占用，允许更大 batch 或更长序列长度。</li>
</ul>
<p>E. 上下文并行（Context Parallelism，CP）</p>
<ul>
<li>场景：长序列（very long context），单个序列长度 S 比单卡能够处理的更长。  </li>
<li>思路：把序列沿时间&#x2F;位置维度切分到多张 GPU，每张 GPU 处理序列的一段（类似把一本长小说分成若干章节给不同人看）。  </li>
<li>注意点：<ul>
<li>对于无权重的运算（LayerNorm、残差）很容易并行。</li>
<li>含权重的 MLP 部分每卡仍需要该部分参数的拷贝并需做梯度同步（像 DP）。</li>
<li>Attention 是难点：需要设计跨分片的 attention 计算（如 Ring Attention）或限制并行方式（Ulysses：并行分头）。</li>
</ul>
</li>
<li>工程示例：Llama3 在后期阶段把序列长度扩大到 131072，并采用 16-way CP（每卡处理 8192 长度）。</li>
</ul>
<p>F. 流水线并行（Pipeline Parallelism，PP）</p>
<ul>
<li>思路：把模型的层（Layer 维度 L）分段分配给不同 GPU，如在流水线中每台机器负责不同的装配步骤。  </li>
<li>问题：按序依赖导致“气泡”——某些阶段的 GPU 会空闲等待数据&#x2F;输入。  </li>
<li>解决：把一个 batch 切成多个 microbatches，并把它们按顺序送入流水线，这样可以把空闲时间用来处理别的 microbatch，从而提高 MFU。  </li>
<li>一个简单衡量：N-way PP 的理论最大 MFU 是 1&#x2F;N（如果不做 microbatches），用 M 个 microbatches 可以显著提升利用率（比如 4-way PP + 4 microbatches 可从 25% 提升到 ~57%）。  </li>
<li>代价：需要额外内存保存各 microbatch 的激活（或应用 checkpointing），并且 pipeline 的调度更复杂。</li>
</ul>
<p>G. 张量并行（Tensor Parallelism，TP）</p>
<ul>
<li>思路：把一个线性层的权重矩阵按行或列切分到多张 GPU，使用分块矩阵乘加（block matmul）。  </li>
<li>比喻：把一个大矩阵想像成一张大“面包”，横切或竖切成若干小片，每张 GPU 只负责把这些小片加工成输出的一部分。  </li>
<li>基本例子：X（N×D）乘 W（D×D），把 W 切成 4 块（1×4 分片），每个 GPU 计算 X W_i &#x3D; Y_i。  </li>
<li>问题与优化：<ul>
<li>有时需要把 Y 的各个分片聚合&#x2F;拼接会产生通信延迟。  </li>
<li>两层技巧：如果第一层按列分，第二层按行分，则中间产物不需要即时全部收集，GPU 可以局部计算并广播部分结果，从而减少通信（见 slides 的两层示意）。</li>
</ul>
</li>
<li>常与其他并行方式结合（TP + PP + DP）形成更大的并行策略。</li>
</ul>
<p>5）混合并行（ND Parallelism）与 MFU（Model FLOPs Utilization）</p>
<ul>
<li>在大工程里通常需要同时用 DP、TP、PP、CP 等多维并行策略，把 GPU 组织成多维网格。每个 GPU 在网格中有 4 个索引，分别代表在 batch、sequence、layer、dim 上的分片 rank。  </li>
<li>我们优化目标：最大化 MFU（模型计算占 GPU 理论峰值 FLOPs 的比例），即让硬件资源尽可能做“有用的矩阵乘法”而不是空闲等待或做大量非矩阵计算&#x2F;通信。  </li>
<li>MFU 的计算步骤（务必掌握）：<ol>
<li>估算模型理论上在一次迭代（forward+backward）中需要的矩阵乘法 FLOPs（记为 FLOP_theoretical）。通常约取 backward ≈ 2× forward（忽略非矩阵操作）。<br>LaTeX:<br>$$<br>\text{FLOP}<em>{\text{theoretical}} \approx \text{FLOP}</em>{\text{forward}} \times 3\quad(\text{forward} + \text{backward约为2×forward})<br>$$<br>（视不同做法可用 3 或 2 的近似，讲义做法通常取 backward&#x3D;2×forward，总 FLOP≈3×forward 或直接计算 forward+backward）</li>
<li>查表得到设备的理论峰值 FLOP&#x2F;sec（例如 H100 的 Tensor Core 峰值）。记为 FLOP&#x2F;s_theoretical。  </li>
<li>计算理论时间： t_theoretical &#x3D; FLOP_theoretical &#x2F; FLOP&#x2F;s_theoretical。  </li>
<li>测量实际一次迭代所需时间 t_actual（包括数据加载、preprocessing、forward、backward、optimizer step、通信）。  </li>
<li>MFU &#x3D; t_theoretical &#x2F; t_actual。<br>LaTeX:<br>$$<br>\text{MFU} &#x3D; \frac{t_{\text{theoretical}}}{t_{\text{actual}}} &#x3D; \frac{\text{FLOP}<em>{\text{theoretical}} &#x2F; \text{FLOP&#x2F;s}</em>{\text{theoretical}}}{t_{\text{actual}}}<br>$$</li>
</ol>
</li>
<li>经验值：MFU &gt; 30% 是不错的；&gt;40% 很好。新一代芯片峰值 FLOPs 增速快于内存带宽，导致更难达到高 MFU。</li>
</ul>
<p>示例（玩具计算，仅为感性理解）：</p>
<ul>
<li>假设一次迭代模型理论 FLOP&#x3D;1e15 FLOP，H100 理论峰值&#x3D;1e15 FLOP&#x2F;s（虚构数），则 t_theoretical&#x3D;1s。若实际迭代耗时 t_actual&#x3D;2s（包括通信&#x2F;I&#x2F;O&#x2F;重算），则 MFU&#x3D;0.5 (50%)。</li>
</ul>
<p>6）实战建议与调参 checklist（工程师视角）</p>
<ul>
<li>规模策略（讲义给的经验）：<ol>
<li>先用 DP（数据并行）扩展到 ~128 GPU，适用于模型 ~1B 参数。  </li>
<li>把每卡 batch size 设置到能撑满显存（以提高 MFU）。  </li>
<li>若模型 &gt; 1B 参数，考虑 FSDP（ZeRO）把参数分片。  </li>
<li>使用激活检查点来降低激活占用、可以支持更大每卡 batch 或更长序列。  </li>
<li>若 GPU 数量 &gt; 256，考虑 HSDP，把通信划分为组内高频、组间低频。  </li>
<li>若 GPU &gt; 1K、模型 &gt; 50B 或序列长度 &gt; 16K，加入 CP&#x2F;PP&#x2F;TP 等更高级策略（按具体硬件做折中）。</li>
</ol>
</li>
<li>调优目标：把 MFU 提高为首要目标（在可接受的数值精度与成本下）。  </li>
<li>工具与测量：用精确的计时（避免异步 launching 带来的测量误差），测 MFU、网络带宽利用率、GPU 内存占用和占比（参数、优化器状态、激活分别占多少）。  </li>
<li>数值格式与内存：混合精度（bfloat16、fp16+fp32 master weights）常用；优化器状态在 FSDP 下可被分片以节省内存。注意不同格式的数值范围、动态缩放（loss scaling）等稳定性问题。</li>
</ul>
<p>7）小结（课程要点回顾）</p>
<ul>
<li>训练超大模型靠的是“把问题拆维度并行”，主要维度有 Batch（DP）、Sequence（CP）、Layer（PP）、Dim（TP）。  </li>
<li>FSDP&#x2F;ZeRO 是解决模型状态占用（参数 + optimizer states）的一把利器，但引入权重广播&#x2F;梯度收集的通信开销。HSDP 通过分组把这类通信局限在快速链路内。  </li>
<li>激活检查点是解决激活内存的常用工具，代价是重算（计算时间增加）——可以用 checkpoint 的间距 C 做折中。  </li>
<li>整体目标是最大化 MFU，使硬件的计算能力被“有效”利用。  </li>
<li>在实践中，往往需要把各种并行技术混合使用，并根据硬件拓扑（节点&#x2F;机架&#x2F;数据中心网络）作工程优化。</li>
</ul>
<p>8）推荐的进一步阅读（重要论文 &#x2F; 工程项目）</p>
<ul>
<li>Rajbhandari et al., “ZeRO: Memory Optimizations Toward Training Trillion Parameter Models” (DeepSpeed &#x2F; ZeRO)  </li>
<li>Huang et al., “GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism”  </li>
<li>Chowdhery et al., “PaLM: Scaling Language Modeling with Pathways”（关于 MFU 分析）  </li>
<li>Llama Team, “The Llama 3 Herd of Models”（实践案例、集群拓扑与并行策略）  </li>
<li>Liu et al., “Ring Attention with Blockwise Transformers for Near-Infinite Context”（Ring Attention）  </li>
<li>Jacobs et al., “DeepSpeed Ulysses”（Context-parallel 系统优化）</li>
</ul>
<p>最后：给你几个“课堂小练习”（帮助巩固）</p>
<ol>
<li>把一个 12 层 Transformer 在 4 张 GPU 上做 4-way Pipeline + DP（无 TP），如果 batch 被切成 8 个 microbatches，问理论上 MFU 会如何变化？（思考 pipeline 的气泡如何被 microbatch 填充）  </li>
<li>写出 Data Parallel 梯度平均的公式并说明为什么梯度是线性的可以直接相加。  </li>
<li>假设你有一张 H100（理论 1e15 FLOP&#x2F;s），模型 forward FLOP&#x3D;1e14，反向约 2×，t_actual&#x3D;0.5s，求 MFU。<br>LaTeX:<br>$$<br>\text{FLOP}<em>{\text{theoretical}}&#x3D;3\times 10^{14},\quad t</em>{\text{theoretical}}&#x3D;\frac{3\times 10^{14}}{10^{15}}&#x3D;0.3\text{s}<br>$$<br>$$<br>\text{MFU}&#x3D;\frac{0.3}{0.5}&#x3D;0.6&#x3D;60%<br>$$</li>
</ol>
<p>如果你愿意，我可以：</p>
<ul>
<li>按“教授讲义”格式把每一部分做成更详细的 slide-by-slide 的讲稿（含口语化讲解），或者  </li>
<li>把 FSDP 的通信流程画成逐步序列图并给出伪代码，或  </li>
<li>给你一个针对“想在 8 张 H100 上训练 100B 模型”的具体工程 checklist（含每步命令&#x2F;DeepSpeed 配置示例）。</li>
</ul>
<p>你想先深入哪一块？</p>
<h1 id="第十二课-·-自监督学习"><a href="#第十二课-·-自监督学习" class="headerlink" title="第十二课 · 自监督学习"></a>第十二课 · 自监督学习</h1><p>下面我以“深度学习教授对零基础大学生讲课”的方式，把这份第十二讲（Self‑Supervised Learning，自监督学习）的课件内容用专业且有条理的语言讲清楚，并穿插通俗易懂的比喻与关键数学公式（以 LaTeX 给出）。目标是让你听完能够理解“这堂课在讲什么、每种方法的直觉、关键实现点、优缺点与工程要点”，并有练习题巩固理解。</p>
<p>目录（讲课路线）</p>
<ol>
<li>为什么要自监督学习（动机与大图景）  </li>
<li>预训练 → 下游迁移：评估指标与协议  </li>
<li>基于图像变换的预任务（rotation、jigsaw、inpainting、colorization）——直觉、例子、局限  </li>
<li>Masked Autoencoders（MAE）——结构、设计要点与训练细节  </li>
<li>对比学习（Contrastive Learning）总体直觉与 InfoNCE 损失（含公式）  </li>
<li>代表方法：SimCLR、MoCo（v1&#x2F;v2）、以及关键实现差异（batch-size、negative queue、projection head）  </li>
<li>序列层面的对比：Contrastive Predictive Coding（CPC）  </li>
<li>自蒸馏方法：DINO（与 DINO v2 的要点）  </li>
<li>其他&#x2F;扩展实例：Dense Object Net 等  </li>
<li>小结、实践建议与练习题</li>
</ol>
<p>1）为什么要自监督学习（动机）</p>
<ul>
<li>问题：深度模型（尤其视觉&#x2F;语言模型）往往需要大量标注数据才能学到好表示，而手工标注昂贵。  </li>
<li>思路：利用“数据自身”构造监督信号（不需要人工标签），让模型通过解决自动生成的‘预任务’来学到通用的特征，这些特征可以迁移到下游任务（分类、检测、分割等）。  </li>
<li>通俗比喻：把自监督想像成“自己出题给自己做”的学习：你把一张图做了某种扰动（比如把某些拼图块移位或遮挡），然后训练模型去“复原”或“辨识”，在反复做这些游戏的过程中，模型学会了图像的常识（比如物体形状、颜色一致性、局部关系）。</li>
</ul>
<p>2）预训练 → 下游迁移：如何评估自监督方法？</p>
<ul>
<li>常见评估方式：<ul>
<li>预任务性能（不常作为最终目标）；</li>
<li>表征质量（Representation quality）：<ul>
<li>Linear probing（冻结编码器，只训练一个线性分类器）用来检验编码器是否把不同类别分开；</li>
<li>Fine‑tuning（全模型或部分模型微调）以看看最优迁移效果；</li>
<li>可视化&#x2F;聚类&#x2F;鲁棒性测试等。</li>
</ul>
</li>
</ul>
</li>
<li>工程指标：训练成本、批量大小需求、显存要求、训练稳定性。</li>
</ul>
<p>3）基于图像变换的预任务（直观例子）</p>
<ul>
<li>rotation prediction：把图片旋转 0&#x2F;90&#x2F;180&#x2F;270 度，训练网络预测角度。比喻：让模型学会“上下”的常识（什么是正常朝向的猫）。  </li>
<li>jigsaw &#x2F; relative patch location：把图像分块后打乱位置，模型需要预测块之间的相对位置关系——学习局部与全局的空间关系。  </li>
<li>inpainting（图像修复）：遮挡图像的一部分，模型学习去重建被遮挡像素（常带重构损失或加 adversarial loss）。这能让模型学到纹理和语义上下文。  </li>
<li>colorization（图像上色）：给出灰度或部分信息，预测颜色；在视频上做 temporal colorization，还能学到追踪与时序一致性。  </li>
<li>局限：很多这类预任务容易让模型学到与特定任务强相关的线索（容易“作弊”），或缺乏通用性；因此出现更通用的目标（如对比学习、Masked 重建等）。</li>
</ul>
<p>4）Masked Autoencoders（MAE）：最近在视觉领域非常有效</p>
<ul>
<li>基本思想（类比 BERT 的掩码语言建模）：对图像按 patch 切分，随机遮挡大比例的 patch（例如 75%），让 encoder 只看到少量可见 patch，decoder 再重建被遮挡部分。  </li>
<li>关键结构要点：<ul>
<li>Encoder 只处理未遮挡的 patch（计算量少，可把 encoder 做得很大）。  </li>
<li>Decoder 负责重建所有 patch（包括被 mask 的），但 decoder 在训练后通常丢弃，只保留 encoder 作为表示器（asymmetrical design）。  </li>
<li>重建损失通常在像素或投影空间上计算，并且只对被 mask 的 patch 计算损失（这样模型必须从上下文恢复缺失信息）。</li>
</ul>
</li>
<li>损失（对被遮挡位置的 MSE 举例）：<br>LaTeX:<br>$$<br>\mathcal{L} ;&#x3D;; \frac{1}{|M|}\sum_{i\in M}| \hat{x}_i - x_i |^2<br>$$<br>其中 M 表示被掩盖的 patch 集合，$$\hat{x}_i$$ 为 decoder 的重建，$$x_i$$ 为原始像素&#x2F;特征。  </li>
<li>直观比喻：把一张拼图的大部分碎片拿走，只给模型少数碎片，训练它把剩下的碎片猜回来。因为遮盖比例高，模型不能靠局部纹理“抄答案”，必须学到更抽象的语义信息。  </li>
<li>设计&#x2F;调参与发现：<ul>
<li>高 mask 比例（如 75%）能提高训练效率并鼓励 encoder 学习强语义特征；  </li>
<li>encoder 的输入仅包含 visible tokens，节约计算；decoder 可小一些但要能重建。</li>
</ul>
</li>
<li>评估：通常 linear probing 与 fine‑tuning 都能看到 MAE 的优秀迁移性能。</li>
</ul>
<p>5）对比学习（Contrastive Learning）：核心直觉与 InfoNCE</p>
<ul>
<li>直觉：把“属于同一对象&#x2F;实例的不同视角”吸引到一起（positive），把“不同实例&#x2F;对象”推远（negative）。学到的表示把同一物体的不同变形映射到相近向量。  </li>
<li>一般构造：对于一个样本 x，通过数据增强或时间上下文生成一个正样本 x+（两个视图），同时采样一批负样本 {x-}。训练编码器 f 使得 f(x) 与 f(x+) 相似，而与其它样本不相似。  </li>
<li>常用相似度函数：余弦相似度（cosine）或点乘；训练里经常用带温度系数的 softmax。  </li>
<li>InfoNCE 损失（给定一个正样本与 N-1 个 negatives）常写为：<br>LaTeX:<br>$$<br>\mathcal{L}<em>{\text{InfoNCE}} &#x3D; -\log\frac{\exp\big(\mathrm{sim}(z, z^+)&#x2F;\tau\big)}<br>{\exp\big(\mathrm{sim}(z, z^+)&#x2F;\tau\big) + \sum</em>{k&#x3D;1}^{N-1}\exp\big(\mathrm{sim}(z, z_k^-)&#x2F;\tau\big)}<br>$$<br>其中 $$z&#x3D;f(x)$$、$$z^+&#x3D;f(x^+)$$、$$z_k^-&#x3D;f(x_k^-)$$、$$\mathrm{sim}(\cdot,\cdot)$$ 例如余弦相似度，$$\tau$$ 为温度超参。  </li>
<li>理论连接：InfoNCE 可视作互信息下界（van den Oord et al., 2018），负样本越多（N 增大），上界越紧。直觉上负样本越多，模型学到的判别性越强。</li>
</ul>
<p>6）代表方法详解（实现差异与工程要点）</p>
<p>A. SimCLR（Chen et al., 2020）</p>
<ul>
<li>核心：用强数据增强生成两视图作为正样本；在batch内把其它样本当 negatives（batch‑based negatives）。  </li>
<li>结构：encoder f + 非线性 projection head g（即先把表示映射到对比空间再做 InfoNCE）。  </li>
<li>关键发现：projection head 有助于提高 encoder 的迁移性能（projection head 空间学习 invariance，而 encoder 的输出保留更多信息供下游使用）；大 batch size 对性能非常重要（因为更多 negatives）。  </li>
<li>工程挑战：需要超大 batch（例如数千到万），因此早期常在大量 TPUs 上训练。</li>
</ul>
<p>B. MoCo（Momentum Contrast, He et al., 2020）</p>
<ul>
<li>目标：减轻 SimCLR 对巨大 batch 的依赖。  </li>
<li>两个核心技巧：<ol>
<li>使用一个“键队列”（FIFO queue）来存储大量的 negative keys（来自过去 mini‑batch），这样 negatives 的数量可以很大但不必一次性出现在当前 batch。  </li>
<li>使用 momentum 更新的“key encoder” $$f_k$$（teacher-like），它是 query encoder $$f_q$$ 的指数移动平均（EMA），以保证队列中的 keys 与当前 encoder 分布保持稳定。</li>
</ol>
</li>
<li>优势：解耦 mini‑batch 大小与 negative 数量（可在较小 batch 下仍有大量 negatives），内存&#x2F;计算更友好。  </li>
<li>MoCo‑v2：融合了 SimCLR 的强增强与非线性 projection head，效果更好。</li>
</ul>
<p>形式上，momentum update 写成：<br>LaTeX:<br>$$<br>\theta_k \leftarrow m \theta_k + (1-m)\theta_q<br>$$<br>其中 $$\theta_k$$ 是 key encoder 参数，$$\theta_q$$ 是 query encoder 参数，$$m$$ 接近 1（如 0.99）。</p>
<p>C. 关于 projection head、温度、batch size、negatives 的实用提示</p>
<ul>
<li>projection head：通常用 2 层 MLP，训练对比时在 projection 空间做 InfoNCE，推理&#x2F;下游时去掉 projection head 并使用 encoder 的输出。  </li>
<li>温度 $$\tau$$：控制相似度分布的“锐利度”，是敏感的超参。  </li>
<li>batch size &#x2F; negatives：更多 negatives 通常更好，但 MoCo 提供了一条降低 batch 依赖的路径。</li>
</ul>
<p>7）序列级对比：Contrastive Predictive Coding（CPC）</p>
<ul>
<li>目的：在时序数据（音频、文本、图像 patch 序列）中学习表示，通过让上下文去预测未来表示并用 InfoNCE 对比真&#x2F;假未来。  </li>
<li>基本流程：<ol>
<li>把序列的每个时间步编码成向量 $$z_t &#x3D; g_{\text{enc}}(x_t)$$。  </li>
<li>用自回归模型（如 GRU 或 Transformer）把已有序列摘要成上下文向量 $$c_t$$。  </li>
<li>用可学习的打分函数（例如 $$s(c_t, z_{t+k}) &#x3D; z_{t+k}^\top W_k c_t$$）去区分真实的未来 $$z_{t+k}$$ 与负样本。</li>
</ol>
</li>
<li>InfoNCE 在时序上被用来判别“真实未来”与“噪声样本”，因此模型学到对时间有预测能力的表征。  </li>
<li>应用：音频、语音、视频帧、以及把图像按行&#x2F;patch 序列化的场景。</li>
</ul>
<p>数学形式示意（时间步 k 的打分）：<br>LaTeX:<br>$$<br>s_k(c_t, z_{t+k}) &#x3D; z_{t+k}^\top W_k c_t<br>$$<br>再带入 InfoNCE 做分类。</p>
<p>8）自蒸馏（Self‑Distillation）与 DINO</p>
<ul>
<li>DINO（Caron et al., 2021）：提出使用“无标签自蒸馏”的方案，思想上类似于 teacher‑student，其中 teacher 由 student 的 EMA 得到（类似 MoCo 的 momentum 编码器理念），但目标不是对比负样本，而是让 student 匹配 teacher 在不同视图上的输出分布（通过温度、中心化等技巧保证训练稳定）。  </li>
<li>结果与特点：DINO 在 Vision Transformer 上能够出现“attention maps 自发地对齐到物体”这样的 emergent properties，使得模型能够在无监督下学到物体级别的特征（可用于无监督分割&#x2F;定位）。DINO v2 在数据与训练技巧上进一步改进，效果更好。  </li>
<li>核心技巧：EMA teacher、温度差异、输出归一化&#x2F;中心化，避免 collapse（所有样本输出相同分布）。</li>
</ul>
<p>9）其他例子：Dense Object Net 等</p>
<ul>
<li>Dense Object Net（Florence et al., 2018）把对比学习思想用于像素级描述（给每个像素&#x2F;点学习稳定描述），适用于机器人抓取与稠密匹配任务。说明对比学习不仅能学全图表征，也能学局部、稠密的表征。</li>
</ul>
<p>10）实践要点与工程建议（对零基础学生）</p>
<ul>
<li>评估：先做 linear probing（快、能评估 representation quality），再做 fine‑tune（获取最终应用性能）。  </li>
<li>选择方法：<ul>
<li>如果有大量计算资源且能做大 batch，SimCLR 是直观且效果好的一条路；  </li>
<li>如果想用较小 mini‑batch 但需要大量 negatives，MoCo（队列 + momentum encoder）是更工程友好的方法；  </li>
<li>如果处理视觉模型且想要高效的预训练，MAE 是近年非常实用且计算上节省的方法（encoder 只看部分 token）。</li>
</ul>
</li>
<li>训练技巧：强数据增强、projection head、温度调参、避免 collapse（在某些自蒸馏或非对比方法中）。  </li>
<li>计算资源：SimCLR 的大 batch 要求显存&#x2F;分布式支持；MoCo&#x2F;MAE 在资源上更灵活些。  </li>
<li>评估集选择：ImageNet 是常用基准，但也要用目标下游数据验证迁移性（比如 detection&#x2F;segmentation 的 Pascal&#x2F;COCO）。</li>
</ul>
<p>11）小结（要点回顾）</p>
<ul>
<li>自监督学习通过“数据自身构造监督信号”极大缓解了标注依赖，能学到通用的表示供下游任务使用。  </li>
<li>两大成功范式：重建&#x2F;掩码（MAE、inpainting）与对比（SimCLR、MoCo、CPC 等）；自蒸馏（DINO）是另一条强有力路线。  </li>
<li>技术要点包括：如何构造正负样本、如何设计 loss（InfoNCE）、如何保证训练稳定（momentum、中心化、projection head）、以及工程上的 batch&#x2F;queue&#x2F;EMA 平衡。  </li>
<li>选择具体方法时需权衡：训练成本、所需负样本数、显存限制、以及希望获得的表征类型（全局 vs 稠密）。</li>
</ul>
<p>12）课堂练习（建议你亲手做几题来巩固）</p>
<ol>
<li>写出 InfoNCE 损失的公式并解释每一项的含义（已在讲义中给出）。  </li>
<li>设想你用 SimCLR 在一个小数据集上训练，为什么 batch size 太小会影响效果？用“负样本不足”的直觉解释。  </li>
<li>用一句话比较 MAE 与 SimCLR 在“需要多少负样本”和“计算分布（encoder&#x2F;decoder）”上的区别。  </li>
<li>（实践）实现一个简单的 MoCo 伪代码（核心：query encoder、key encoder、queue、momentum update），并标注出哪些部分不需要反向传播（no_grad）。  </li>
<li>（开放）思考 DINO 为什么会出现“无监督下的物体分割”这种 emergent behavior？（提示：teacher‑student + 多视图 + 输出归一化&#x2F;中心化）</li>
</ol>
<p>13）推荐阅读（入门与跟进）</p>
<ul>
<li>Gidaris et al., “Unsupervised Representation Learning by Predicting Image Rotations” (2018)  </li>
<li>Doersch et al., “Unsupervised Visual Representation Learning by Context Prediction” (2015)  </li>
<li>Pathak et al., “Context Encoders: Feature Learning by Inpainting” (2016)  </li>
<li>He et al., “Masked Autoencoders Are Scalable Vision Learners” (2021) — MAE 论文  </li>
<li>Chen et al., “A Simple Framework for Contrastive Learning of Visual Representations” (SimCLR, 2020)  </li>
<li>He et al., “Momentum Contrast for Unsupervised Visual Representation Learning” (MoCo, 2019&#x2F;2020)  </li>
<li>van den Oord et al., “Representation Learning with Contrastive Predictive Coding” (CPC, 2018)  </li>
<li>Caron et al., “Emerging Properties in Self‑Supervised Vision Transformers” (DINO, 2021)</li>
</ul>
<p>如果你愿意，我可以接着做下面几项（你选其一）：  </p>
<ul>
<li>把本讲义内容整理成一页“快速记忆卡”（包含关键公式、优劣对比、超参建议）；  </li>
<li>给你一份简单的 PyTorch&#x2F;伪码模板：SimCLR 或 MoCo 的核心训练循环（含数据增强、projection head、InfoNCE 计算）；  </li>
<li>帮你把“课堂练习第 4 题”写成可运行的伪代码并解释每一步。</li>
</ul>
<p>你想先做哪一项？</p>
<h1 id="第十三、十四课-·-生成式模型"><a href="#第十三、十四课-·-生成式模型" class="headerlink" title="第十三、十四课 · 生成式模型"></a>第十三、十四课 · 生成式模型</h1><p>下面我以“深度学习教授对零基础大学生讲课”的方式，把这份第十三讲（Generative Models，第1部分）的课件内容用专业、有条理且通俗的语言讲清楚。保留必要的数学公式（均用 LaTeX），并穿插直观比喻和工程&#x2F;直觉要点，帮助你把概念消化透彻。</p>
<p>目录（本次课学习路线）</p>
<ol>
<li>什么是生成模型？为什么关心它们？（直觉与应用）  </li>
<li>判别模型 vs 生成模型 的对比（含概率直觉）  </li>
<li>生成模型的分类／谱系（Taxonomy）——显式密度 &#x2F; 隐式密度；自回归 &#x2F; VAE &#x2F; GAN &#x2F; Diffusion 的定位  </li>
<li>自回归模型（Autoregressive models）：原理、MLE、优缺点与示例（语言模型、PixelRNN&#x2F;PixelCNN）  </li>
<li>变分自编码器（VAE）：从自编码器到 VAE，ELBO 推导、重参数化技巧与训练流程  </li>
<li>VAE 的直观解释、优缺点与常见问题（模糊样本、KL–重构的博弈、posterior collapse）  </li>
<li>小结、练习题与下一步学习建议</li>
</ol>
<p>1）什么是生成模型？为什么要学？</p>
<ul>
<li>定义（直观）：生成模型学习数据分布 p(x)，能“生成”与训练数据同类的新样本；条件生成模型学习 p(x|y)，能做“有约束地生成”（如文本到图像）。  </li>
<li>应用举例：图像生成（DALL·E、Stable Diffusion）、语言生成（GPT 系列）、语音合成、图像修复&#x2F;超分辨率、模拟物理系统、异常检测（低概率样本）等。  </li>
<li>直观比喻：把生成模型想成“学习一块手艺&#x2F;配方”的大师。训练阶段它观察大量成品（照片、句子），学会把隐含配方（latent factors）与最终成品之间的关系建起来；生成阶段从配方（或随机噪声）出发，把新作品“烤”出来。</li>
</ul>
<p>2）判别模型 vs 生成模型（概率角度）</p>
<ul>
<li>判别模型学习 p(y|x)：把输入映射到标签（例如分类）。  </li>
<li>生成模型学习 p(x) 或 p(x|y)：学习如何产出样本本身。  </li>
<li>概率直觉：生成模型给每个可能的 x 一个“概率质量&#x2F;密度”，高概率表示更常见、更合理的样本；因此它可以用来检测异常或拒绝不合理输入。  </li>
<li>Bayes 关系（有用的提醒）：<br>LaTeX:<br>$$<br>p(y \mid x) &#x3D; \frac{p(x \mid y),p(y)}{p(x)}<br>$$<br>生成模型和判别模型在某些场景可以互相转换（但工程实践中通常单独训练）。</li>
</ul>
<p>3）生成模型的分类（spectrum，便于记忆）</p>
<ul>
<li>显式密度（explicit density）：模型能直接给出或近似给出 p(x) 的值。进一步分为：<ul>
<li>可解析&#x2F;可计算密度（tractable）：自回归模型（PixelRNN、语言模型）可以精确写出 p(x) 并用 MLE 训练。  </li>
<li>近似密度（approximate）：变分自编码器（VAE），通过下界（ELBO）近似最大化似然。</li>
</ul>
</li>
<li>隐式密度（implicit）：无法写出 p(x) 的显式表达，但能从模型快速采样（生成）；典型代表是 GAN。  </li>
<li>采样方式维度：<ul>
<li>直接采样（一次性）：GAN（给噪声直接输出图像）。  </li>
<li>迭代采样（多步过程）：扩散模型（从噪声逐步去噪）。</li>
</ul>
</li>
<li>讲义把这些方法按“能否计算 p(x)”与“采样方式”画成谱系图，方便记忆。</li>
</ul>
<p>4）自回归模型（Autoregressive models）</p>
<ul>
<li>核心思想（链式法则）：把复杂分布 p(x) 分解为条件分布的连乘形式。若 x 是序列 x&#x3D;(x1,…,xT)，则<br>LaTeX:<br>$$<br>p(x)&#x3D;p(x_1,x_2,\dots,x_T)&#x3D;\prod_{t&#x3D;1}^T p(x_t\mid x_{&lt;t})<br>$$<br>（其中 $$x_{&lt;t}$$ 表示前 t−1 个元素）  </li>
<li>优点：每一项都是一个标准条件概率，可以直接用最大似然（MLE）训练（把训练集的对数似然相加）。<br>LaTeX:<br>$$<br>\mathcal{L}(\theta)&#x3D;\sum_{i&#x3D;1}^N \log p_\theta(x^{(i)}) &#x3D; \sum_{i&#x3D;1}^N \sum_{t&#x3D;1}^{T} \log p_\theta(x^{(i)}<em>t \mid x^{(i)}</em>{&lt;t})<br>$$</li>
<li>示例：<ul>
<li>语言模型（RNN&#x2F;Transformer）就是自回归：每步预测下一个 token 的分布（LLMs）。  </li>
<li>图像自回归（PixelRNN &#x2F; PixelCNN）：按 scanline 把图像转为像素序列，逐像素预测（每像素通常分 256 类）。</li>
</ul>
</li>
<li>缺点（主要工程问题）：序列太长时（例如高分辨率图像、长文本）采样与训练成本极高——例如 1024×1024 彩图 ~ 数百万子像素，逐像素建模代价大；因此常把图像做“抽样化 &#x2F; 瓦片化（tile）”或使用更高层次的 token（VQ、dVAE、latent diffusion 等）来降低序列长度。</li>
</ul>
<p>5）变分自编码器（VAE）：从自编码器到概率生成模型</p>
<ul>
<li>先回顾普通自编码器（AE）：encoder 把 x 映射为 latent z，decoder 从 z 重建 x，用 L2 或其它重构损失训练。问题是：如何用 AE「生成」全新的样本？如果 z 没有一个已知分布，从它采样没有意义。  </li>
<li>VAE 的核心想法：把 z 的先验 p(z) 设定为已知分布（常取标准正态 N(0,I)），并学习一个 encoder q_\phi(z|x) 来近似真实后验 p_\theta(z|x)。通过变分推断，联合训练 encoder（近似后验）和 decoder（条件生成 p_\theta(x|z)），目标是最大化观测数据的对数似然的下界（ELBO）。  </li>
<li>重要的公式（ELBO 推导要点）——最终得到的训练目标为最大化下界（等价于最小化负下界）：<br>LaTeX:<br>$$<br>\log p_\theta(x) ;\ge; \mathbb{E}<em>{z\sim q</em>\phi(z|x)}\big[\log p_\theta(x\mid z)\big] ;-; D_{\mathrm{KL}}\big(q_\phi(z\mid x),|, p(z)\big).<br>$$<br>其中第一项是重构项（期望似然），第二项是后验与先验的 KL 散度（正则项，鼓励 q 與先验一致）。  </li>
<li>直观意义：<ul>
<li>重构项希望 decoder 能从 encoder 给出的 z 恢复 x（鼓励信息保留）。  </li>
<li>KL 项希望 encoder 的输出分布接近先验（鼓励可采样性）。<br>这两项会发生博弈：重构要求 z 带足够信息；KL 要求 z 看起来像来自 N(0,I)。</li>
</ul>
</li>
<li>训练时的关键技巧：重参数化（reparameterization trick），把随机采样 z 的路径从网络中分离出来，以便对 encoder 参数求导。<br>LaTeX:<br>$$<br>z &#x3D; \mu_\phi(x) + \sigma_\phi(x)\odot \epsilon,\quad \epsilon\sim\mathcal{N}(0,I).<br>$$<br>这样对 $$\mu_\phi,\sigma_\phi$$ 的梯度可直接通过反向传播计算。  </li>
<li>典型实现步骤：<ol>
<li>encoder 网络输出 $$\mu_\phi(x),\ \sigma_\phi(x)$$（或对角协方差）；  </li>
<li>采样 $$\epsilon\sim N(0,I)$$，构造 z；  </li>
<li>decoder 给出 p_\theta(x|z)（例如高斯分布，均值由网络输出）；  </li>
<li>计算重构损失（通常是负对数似然或 L2）与 KL(q||p)；  </li>
<li>最小化负 ELBO（或最大化 ELBO）。</li>
</ol>
</li>
<li>生成&#x2F;采样：训练后从先验 p(z)&#x3D;N(0,I) 采样 z，然后把 z 放 decoder，得到样本 x。</li>
</ul>
<p>6）VAE 的直观比喻与常见问题</p>
<ul>
<li>比喻：把 VAE 想成“有约束的压缩箱”。encoder 把图像压成一个 z（并且把很多样本压到近似的先验分布中），decoder 是能从任意“先验箱”里的随机写字条（z）生成一幅图。KL 约束是把所有写字条的分布强行赶成“标准箱”形式（方便采样）。  </li>
<li>主要优点：<ul>
<li>可以评估近似对数似然（通过 ELBO），可解释 KL 项；  </li>
<li>训练稳定（不像 GAN 那样 adversarial）并且采样直接（一次性）。</li>
</ul>
</li>
<li>主要缺点：<ul>
<li>输出往往“模糊”（若用像素级均方误差），因为最大化似然会“平均”多种可能的细节；  </li>
<li>KL 与重构之间的博弈可能导致 posterior collapse（encoder 输出靠近先验，z 不携带信息，decoder 成为一个强大条件生成器），尤其是在强 decoder（如 autoregressive decoder）场景。常用修正：KL 退火、β‑VAE（加权 KL）、限制 decoder 容量等。</li>
</ul>
</li>
<li>评价生成质量：对图像常用 FID、IS 等感知指标；对概率建模用对数似然&#x2F;ELBO（但它们并不总是与感知质量直接对齐）。</li>
</ul>
<p>7）补充要点与工程提示</p>
<ul>
<li>Decoder 的输出分布选择很关键：像素级数据常用离散 256 类（softmax）或连续像素用高斯；选择会影响重构损失的性质与样本质量。  </li>
<li>对于高维图像，自回归解码器能提升像素质量但容易导致训练&#x2F;采样成本高且可能产生 posterior collapse（更容易把任务交给强 decoder）。  </li>
<li>自回归模型（如 Transformer LM）在文本生成上几乎称霸，因为它直接建模下一个 token 的条件分布；采样时按序生成（无法并行）。  </li>
<li>VAE 强用于学习结构化潜空间（可解释性、插值、操纵 latent 因子），而 GAN&#x2F;扩散模型在生成质量（清晰度、细节）上表现更好（但另有各自缺点：GAN 的训练不稳定与 mode collapse；扩散模型采样慢但质量高）。</li>
</ul>
<p>8）本讲关键公式速记（LaTeX）</p>
<ul>
<li>自回归链式分解：<br>LaTeX:<br>$$<br>p(x)&#x3D;\prod_{t&#x3D;1}^T p(x_t\mid x_{&lt;t}).<br>$$</li>
<li>最大似然训练目标（对数似然）：<br>LaTeX:<br>$$<br>\mathcal{L}(\theta)&#x3D;\sum_{i&#x3D;1}^N \log p_\theta(x^{(i)}).<br>$$</li>
<li>VAE 的 Evidence Lower Bound（ELBO）：<br>LaTeX:<br>$$<br>\log p_\theta(x) \ge \mathbb{E}<em>{z\sim q</em>\phi(z\mid x)}\big[\log p_\theta(x\mid z)\big] - D_{\mathrm{KL}}\big(q_\phi(z\mid x),|, p(z)\big).<br>$$</li>
<li>重参数化技巧：<br>LaTeX:<br>$$<br>z &#x3D; \mu_\phi(x) + \sigma_\phi(x)\odot \epsilon,\qquad \epsilon\sim\mathcal{N}(0,I).<br>$$</li>
</ul>
<p>9）课堂练习与思考题（用来巩固）</p>
<ol>
<li>证明自回归分解式：为什么任意序列的联合概率都可以写成条件概率的乘积？（提示：链式法则）  </li>
<li>写出 VAE 的 ELBO 推导关键步骤（把 $$\log p_\theta(x)$$ 乘以并除以 $$q_\phi(z|x)$$ 并应用 Jensen &#x2F; KL 展开）。  </li>
<li>解释“posterior collapse” 是什么？给出两种常用缓解方法（提示：KL 退火、限制 decoder 表达力、β‑VAE）。  </li>
<li>比较自回归模型与 VAE 在“采样速度”和“样本质量&#x2F;多样性”上的典型差异。（短答）  </li>
<li>（实践）写出 VAE 的训练循环伪代码要点（encoder→sample z→decoder→计算重构损失+KL→反向）。</li>
</ol>
<p>10）小结（三句话）</p>
<ul>
<li>生成模型目标是学习数据的概率分布 p(x)，可用于采样、新样本生成与异常检测。  </li>
<li>自回归模型以链式分解直接建模条件概率，训练简单但在长序列&#x2F;高分辨率时代价高；VAE 用变分推断把生成建模成带先验的潜变量模型，通过 ELBO 学习，能做可控&#x2F;可解释的采样但可能生成模糊。  </li>
<li>生成建模是一个大范畴：后续课会继续讲 GAN 与扩散模型（在样本质量与训练稳定性上各有取舍），这些方法在实际图像&#x2F;音频生成中非常重要。</li>
</ul>
<p>如果你愿意，我接下来可以（选其一）：</p>
<ul>
<li>把 VAE 的 ELBO 推导写成逐步数学推导（更严谨版），带注释每一步的直觉；  </li>
<li>给你一个简单可运行的 PyTorch VAE 模板（含 encoder&#x2F;decoder、重参数化、训练循环）；  </li>
<li>给你一页「生成模型速查表」：方法、优缺点、典型应用、计算&#x2F;采样特点。</li>
</ul>
<p>你想先看哪一个？<br>下面我以“深度学习教授对零基础大学生讲课”的方式，把你提供的 Lecture 14（Generative Models, Part 2）讲稿内容，用专业、有条理且通俗的语言讲清楚。保留必要的数学公式（均用 LaTeX），穿插直观比喻和工程&#x2F;直觉要点，便于理解与后续实现。</p>
<p>目录（本讲学习路线）</p>
<ol>
<li>回顾与本讲目标（GAN &#x2F; Diffusion &#x2F; LDM）  </li>
<li>Generative Adversarial Networks（GAN）——原理、目标、训练技巧与局限  </li>
<li>Diffusion Models（含 Rectified Flow &#x2F; Generalized Diffusion &#x2F; SDE &#x2F; Score）——直觉、训练与采样流程  </li>
<li>Classifier‑Free Guidance（CFG）——为何重要、公式与工程代价  </li>
<li>Latent Diffusion Models（LDMs）与组合管线（VAE + GAN + Diffusion）  </li>
<li>其他工程要点（Noise schedule、distillation、DiT &#x2F; 条件注入、视频扩散）  </li>
<li>关键公式速记、课堂练习与下一步建议</li>
</ol>
<p>1）回顾与本讲目标（一句话）</p>
<ul>
<li>上节我们讲了自回归与 VAE。本讲重点介绍三大现代一线方法：GAN（显式采样、隐式密度）、Diffusion（迭代去噪、能做到极高质量但采样慢）和把扩散放到潜空间里的 Latent Diffusion（工程上最常用的组合）。</li>
</ul>
<p>2）GAN（Generative Adversarial Networks）<br>直觉</p>
<ul>
<li>想象两个选手：Generator（G）负责“伪造”样本，Discriminator（D）负责区分“真／假”。G 的目标是骗过 D，D 的目标是正确识别。两者打“零和博弈”。</li>
</ul>
<p>基本对抗目标</p>
<ul>
<li>原始 min-max 目标为（Goodfellow et al., 2014）：<br>LaTeX:<br>$$<br>\min_G \max_D ; \mathbb{E}<em>{x\sim p</em>{\text{data}}}\big[\log D(x)\big] ;+; \mathbb{E}_{z\sim p(z)}\big[\log(1 - D(G(z)))\big].<br>$$</li>
<li>在实践中，为了给 G 更强的初期梯度，常替换为“非饱和”生成器损失：<br>LaTeX:<br>$$<br>\mathcal{L}<em>G &#x3D; -\mathbb{E}</em>{z\sim p(z)}\big[\log D(G(z))\big].<br>$$</li>
</ul>
<p>重要结论（性质）</p>
<ul>
<li>对任意固定 G，最优的判别器为<br>LaTeX:<br>$$<br>D^*(x) &#x3D; \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_G(x)}.<br>$$</li>
<li>如果 D 能达到最优并且两者有无限容量，minimax 最优解在于 $$p_G &#x3D; p_{\text{data}}$$。</li>
</ul>
<p>优点与缺点（工程&#x2F;直觉）</p>
<ul>
<li>优点：能生成非常清晰、逼真的图像（尤其在 2016–2021 年间领先）。  </li>
<li>缺点：训练不稳定（容易震荡）、没有单一可监控的全局损失曲线、模式崩溃（mode collapse）、在大规模&#x2F;大模型上扩展困难。  </li>
<li>工程技巧：DCGAN 架构、Progressive GAN、StyleGAN（通过层级的调节向潜空间注入可控噪声与样式）、用谱归一化、平衡 D&#x2F;G 的更新步数等来稳定训练。</li>
</ul>
<p>3）Diffusion Models（扩散模型）——直觉与核心实现<br>核心直觉（简化）</p>
<ul>
<li>把生成问题反过来想：把真实数据逐步加入噪声（前向过程），变成纯噪声；训练网络学习如何逐步去噪（逆过程）。采样时从噪声开始，逐步去噪回到数据分布。</li>
</ul>
<p>两种相近视角</p>
<ul>
<li>Score &#x2F; SDE 视角：学习数据分布的分数函数 s(x)&#x3D;∇_x log p(x)，把去噪看作解一个 SDE（Song &amp; Ermon 系列工作）。  </li>
<li>变分&#x2F;潜变量视角（DDPM）：把前向噪声过程视为一个已知潜变量模型，学习近似后验&#x2F;逆过程，优化 ELBO（Ho et al., 2020）。</li>
</ul>
<p>Rectified Flow（讲稿中给出的“干净”实现）</p>
<ul>
<li>训练采样步骤（讲稿模块化）：<ol>
<li>采样真实图像 x ~ p_data，采样噪声 z ~ p_noise（通常标准高斯），采样噪声比例 t ~ Uniform(0,1)。  </li>
<li>构造线性插值的 noisy sample:<br>LaTeX:<br>$$<br>x_t &#x3D; (1-t),x + t,z.<br>$$</li>
<li>定义目标向量（v）为 z - x（直指把 x_t 往 z 或 x 的方向移动）：<br>LaTeX:<br>$$<br>v &#x3D; z - x.<br>$$</li>
<li>训练网络 f_\theta(x_t, t) 去预测 v，通过最小化均方误差：<br>LaTeX:<br>$$<br>\mathcal{L}(\theta) &#x3D; \mathbb{E}<em>{x,z,t}; |,v - f</em>\theta(x_t,t)|_2^2.<br>$$</li>
</ol>
</li>
<li>采样（离散迭代）：<ul>
<li>设步数 T（常 30–50）：初始化 x^{(1)} ~ p_noise；按时间栈（t&#x3D;1, 1-1&#x2F;T, 1-2&#x2F;T, …, 0）循环：<ul>
<li>计算 v_t &#x3D; f_\theta(x_t,t)，更新<br>LaTeX:<br>$$<br>x \leftarrow x - \frac{v_t}{T}<br>$$</li>
<li>最终返回 x（近似 p_data 的样本）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Generalized Diffusion（更一般的形式）</p>
<ul>
<li>可把 x_t 写成更一般的线性组合：<br>LaTeX:<br>$$<br>x_t &#x3D; a(t),x + b(t),z,<br>$$<br>以及要预测的目标：<br>LaTeX:<br>$$<br>y_{\text{gt}} &#x3D; c(t),x + d(t),z,<br>$$<br>并训练网络预测 y_gt（用 MSE）。不同的 a,b,c,d 对应 VP&#x2F;VE&#x2F;ε-pred&#x2F;x-pred&#x2F;v-pred 等不同算法&#x2F;目标的数学等价写法（见讲稿）。</li>
</ul>
<p>Score &#x2F; SDE 视角（一句话）</p>
<ul>
<li>连续极限下，扩散训练等价于学会数据分布的分数函数并用数值方法解逆 SDE 来采样。核心量是 score s(x,t)&#x3D;∇_x log p_t(x)。</li>
</ul>
<p>4）Classifier‑Free Guidance（CFG）<br>目的与直觉</p>
<ul>
<li>在条件扩散（例如 text→image）中，希望在采样时“放大”条件信号以得到更契合提示的生成结果。CFG 是一种不需要额外分类器的简洁技巧（Ho &amp; Salimans 2022）。</li>
</ul>
<p>训练方法（关键步骤）</p>
<ul>
<li>在训练阶段，随机丢弃条件 y（按某概率把 y 置空），使同一模型学会在有条件和无条件两种情形下工作。这样训练后 f_\theta 可同时输出 v_y（条件输入）与 v_∅（无条件输入）。</li>
</ul>
<p>采样时的组合公式（常用形式）</p>
<ul>
<li>以 scale（权重）为 w，CFG 常用的向量组合为：<br>LaTeX:<br>$$<br>v_{\text{cfg}} &#x3D; v_{\varnothing} + w,(v_{y} - v_{\varnothing}) ;&#x3D;; (1-w),v_{\varnothing} + w,v_{y}.<br>$$<br>等价写法（有时写成另一种代数形式）：<br>LaTeX:<br>$$<br>v_{\text{cfg}} &#x3D; (1+w),v_{y} - w,v_{\varnothing}<br>$$<br>（两式在数值上等价只要对应关系正确，常见实现以第一式为直观表述）。<br>工程要点：这需要在每一步对条件和无条件都评估模型（即采样成本大约翻倍），但实践中能显著提升条件对齐度与视觉质量；CFG 已是主流实践。</li>
</ul>
<p>5）Latent Diffusion Models（LDMs）与混合管线<br>动机</p>
<ul>
<li>扩散直接在高分辨率像素上训练成本高（内存、计算、相关性强）。LDM 的核心是把扩散放到较低维的潜空间来做，从而显著降低计算量，同时保持视觉质量。</li>
</ul>
<p>典型管线（Rombach et al., 2022）</p>
<ol>
<li>用一个 encoder&#x2F;decoder（通常是 VAE 结构）把图像 x 映射到潜表示 z_lat（下采样如 D&#x3D;8，C&#x3D;16：256×256→32×32×16）。  </li>
<li>在潜空间上训练扩散模型（或 Rectified Flow 等）去噪 latent（encoder 通常训练好后在扩散训练中冻结）。  </li>
<li>采样时在潜空间运行扩散生成 latent，再用 decoder 解码成图像。</li>
</ol>
<p>VAE + GAN 混合（工程选择）</p>
<ul>
<li>为了避免 VAE 解码器模糊，实务上常把 encoder&#x2F;decoder 训练成 VAE（但把 KL 权重设小），并在 latent 空间&#x2F;像素空间加一个判别器（GAN）来提升细节，使最终 pipeline &#x3D; VAE + GAN + Diffusion（或 diffusion 用在 latent 或解码后再精炼）。</li>
</ul>
<p>优点</p>
<ul>
<li>减少扩散计算量（更少 token &#x2F; 更小模型）；容易扩展到超高分辨率（1024、2048 等）。这是当前图像&#x2F;视频生成系统（如 Stable Diffusion）的主流做法。</li>
</ul>
<p>6）其他工程要点与进阶主题<br>Noise schedule（噪声采样策略）</p>
<ul>
<li>并非所有 t 的权重都等同重要：中间噪声级别最模糊、学习最困难，需要在训练时用非均匀 p(t)（如 logit‑normal）把重心移向中间噪声；高分辨率数据可能还需把分布向更高噪声偏移以处理像素间强相关性。</li>
</ul>
<p>Diffusion Distillation（蒸馏）</p>
<ul>
<li>扩散采样需要多步（几十步），很慢。用 distillation&#x2F;consistency methods 可以把采样步数减少到很小（多步蒸馏、一步蒸馏、consistency models 等）。有些方法还能把 CFG “烘焙”到蒸馏后的模型中，降低运行时代价但保留强条件对齐。</li>
</ul>
<p>Diffusion Transformer（DiT）与条件注入</p>
<ul>
<li>扩散模型可采用 Transformer block（DiT），关键是如何注入条件信息（时间步、文本等）。常见做法：<ul>
<li>用预测 scale&#x2F;shift（相当于时序的 AdaIN）来注入时刻 t；  </li>
<li>用 cross‑attention 将文本 embedding 与图像&#x2F;latent tokens 做注意力交互来注入文本条件。</li>
</ul>
</li>
</ul>
<p>文本到视频 &#x2F;视频扩散</p>
<ul>
<li>与图像类似，但 latent 加了时间维，要处理时空一致性与更大的内存&#x2F;计算量。工程上常用 3D 下采样（如 8×8×8）和大模型（几十亿参数）。许多近年工作设法把视频分成 latents、用时间交互模块并加入 pretrained text encoder（T5&#x2F;CLIP&#x2F;UL2 等）。</li>
</ul>
<p>7）关键公式速记（LaTeX）</p>
<ul>
<li>GAN 原始 min-max：<br>LaTeX:<br>$$<br>\min_G \max_D; \mathbb{E}<em>{x\sim p</em>{\text{data}}}[\log D(x)] + \mathbb{E}_{z\sim p(z)}[\log(1-D(G(z)))].<br>$$</li>
<li>非饱和生成器损失（实用）：<br>LaTeX:<br>$$<br>\mathcal{L}<em>G &#x3D; -\mathbb{E}</em>{z\sim p(z)}\big[\log D(G(z))\big].<br>$$</li>
<li>Rectified Flow 训练（关键步骤）：<br>LaTeX:<br>$$<br>x_t &#x3D; (1-t)x + t z,\qquad v &#x3D; z - x,<br>$$<br>$$<br>\mathcal{L}(\theta)&#x3D; \mathbb{E}<em>{x,z,t}\big|,v - f</em>\theta(x_t,t)\big|_2^2.<br>$$</li>
<li>Generalized diffusion（通用写法）：<br>LaTeX:<br>$$<br>x_t &#x3D; a(t)x + b(t)z,\qquad y_{\text{gt}} &#x3D; c(t)x + d(t)z,<br>$$<br>$$<br>\mathcal{L}(\theta)&#x3D;\mathbb{E},\big|,y_{\text{gt}} - f_\theta(x_t,t)\big|_2^2.<br>$$</li>
<li>Classifier‑Free Guidance 采样组合：<br>LaTeX:<br>$$<br>v_{\text{cfg}} &#x3D; v_{\varnothing} + w,(v_{y} - v_{\varnothing}),<br>$$<br>其中 $$w$$ 为 guidance strength（通常 &gt;0）。</li>
</ul>
<p>8）工程要点清单（速查）</p>
<ul>
<li>GAN：注意 D&#x2F;G 的学习速率平衡、谱归一、判别器容量、使用非饱和损失、尝试 StyleGAN 类型架构以获得高质量图像。  </li>
<li>Diffusion：调优 noise schedule（p(t)），使用潜空间（LDM）以节省计算，结合 distillation 减少采样步数，使用 CFG 提升条件对齐但付出采样成本翻倍。  </li>
<li>LDM：训练 encoder&#x2F;decoder（VAE）时通常使用小 KL 权重，必要时在 decoder 上加 discriminator 以恢复细节。  </li>
<li>大规模文本条件：使用强 text encoder（T5&#x2F;CLIP&#x2F;UL2）；在扩散模型里用 cross‑attention 注入文本；采样时常配 CFG。</li>
</ul>
<p>9）课堂练习与思考题</p>
<ol>
<li>推导 GAN 中固定 G 时最优判别器 D^*(x) &#x3D; p_data&#x2F;(p_data + p_G)。  </li>
<li>在 Rectified Flow 中，解释为何目标 v &#x3D; z − x，训练网络预测 v，可以实现从噪声到图像的逐步变换。  </li>
<li>说明 CFG 的直观作用并写出两种不同的实现形式（基于 score 的和基于概率放缩的）。讨论其优缺点（质量 vs 采样代价）。  </li>
<li>讨论为什么要把扩散放在 latent 空间（LDM），以及把 encoder&#x2F;decoder 作为 VAE 时为何要把 KL 权重设小。  </li>
<li>（实践）写出 Rectified Flow 的训练伪代码或 Latent Diffusion 的采样伪代码（我可以帮你写 PyTorch 版本）。</li>
</ol>
<p>10）小结（三句话）</p>
<ul>
<li>GAN 是“对抗式”一次性生成器，样本质量高但训练不稳定；Diffusion 是“迭代去噪”生成器，训练稳定、样本质量高但采样慢。  </li>
<li>把扩散放在潜空间（LDM）是当前工程上的制胜方法：在保持质量的同时显著节省计算。  </li>
<li>CFG、噪声调度与蒸馏是工程中决定质量&#x2F;速度的重要杠杆：它们常被联合使用以达到可用的生成系统（Text→Image&#x2F;Video 等）。</li>
</ul>
<p>你想接下来看哪部分的“深度”内容？（选其一）</p>
<ul>
<li>我把 GAN 的数学推导与常见稳定化策略写成可运行的训练说明（含超参建议）。  </li>
<li>我给出一个简单可运行的 PyTorch&#x2F;伪代码实现：Rectified Flow（训练 + 采样）或 Latent Diffusion（包含 VAE encoder&#x2F;decoder + diffusion on latents）。  </li>
<li>我把 CFG、蒸馏与噪声调度写成工程实战小抄（如何选 w、p(t)、蒸馏步骤数以及常用 trick）。</li>
</ul>
<p>请选择一个，我会马上为你展开。</p>
<h1 id="第十六课-·-多模态大模型"><a href="#第十六课-·-多模态大模型" class="headerlink" title="第十六课 · 多模态大模型"></a>第十六课 · 多模态大模型</h1><p>下面我以“深度学习教授对零基础大学生讲课”的风格，把你上传的 Lecture 16（“Multi‑Modal Foundation Models” &#x2F; 多模态基础模型）内容整理成一节有条理、通俗且可用于后续工程实现的讲稿。保留必要的数学公式（均用 LaTeX），穿插直观比喻与工程&#x2F;直觉要点，最后给出练习与参考阅读。</p>
<p>主题：多模态（视觉+语言）基础模型 —— 思路、代表模型、优缺点与工程要点</p>
<p>一、课程目标（本讲你要学会）</p>
<ul>
<li>理解“基础模型（Foundation Model）”的概念以及为啥对视觉任务重要。  </li>
<li>掌握 CLIP 类型的对比式视觉-文本模型的核心思想、训练目标与如何做 zero-shot 分类（含 prompt engineering）。  </li>
<li>了解 CoCa、LLaVA、Flamingo、SAM 等近期视觉+语言&#x2F;分割基础模型的设计要点与差异。  </li>
<li>理解“链式（chaining）”方法（用 LLM 生成 prompt&#x2F;逻辑，再调用视觉模型）与 Visual Programming 的思想。  </li>
<li>听懂数据与标注质量为何能胜过海量但随意的抓取；掌握工程中常见的 trick 与陷阱。</li>
</ul>
<p>二、为什么需要“基础模型”？</p>
<ul>
<li>传统做法：为每个任务&#x2F;数据域训练一个专门模型（费时、难以复用）。  </li>
<li>基础模型思路：先做大规模自监督或弱监督的预训练，得到一个通用模型（encoder 或 encoder+decoder）。下游任务可以零样本 &#x2F; 少样本 &#x2F; 微调使用。  </li>
<li>直观比喻：把基础模型看作“通用工具箱”，而不是每个任务都制造一把新工具。</li>
</ul>
<p>三、CLIP：用对比学习把图像与文本拉到同一向量空间</p>
<ol>
<li>核心直觉</li>
</ol>
<ul>
<li>把图像和对应的文本描述编码到同一个 embedding 空间，使得配对 (image, caption) 在向量空间里靠得近，不配对的远离。这样就能用文本向量直接去匹配图片（或反过来），实现 open‑vocabulary 的零样本分类与检索。</li>
</ul>
<ol>
<li>相似度与损失（InfoNCE &#x2F; 对比损失）</li>
</ol>
<ul>
<li>假设一批 N 对 (image_i, text_i)，图像编码器输出 v_i，文本编码器输出 t_i（通常都再 L2 归一化）。定义相似度：<br>LaTeX:<br>$$<br>\text{sim}(v,t) &#x3D; \frac{v^\top t}{|v||t|}.<br>$$</li>
<li>对比式（双向）InfoNCE 损失（图像对文本和文本对图像两个方向，示意单方向）：<br>LaTeX:<br>$$<br>\mathcal{L}<em>{\text{I-&gt;T}} &#x3D; -\frac{1}{N}\sum</em>{i&#x3D;1}^N \log\frac{\exp(\text{sim}(v_i,t_i)&#x2F;\tau)}{\sum_{j&#x3D;1}^N \exp(\text{sim}(v_i,t_j)&#x2F;\tau)},<br>$$<br>其中 $$\tau$$ 是 temperature（控制 softmax 峰度）。</li>
</ul>
<ol>
<li>训练数据与规模效应</li>
</ol>
<ul>
<li>CLIP 的成功很大程度来自“规模”：大模型 + 海量（400M）图片-文本对。规模让模型学到丰富概念与鲁棒性，尤其在未见数据分布下的泛化能力很强。</li>
</ul>
<ol>
<li>用 CLIP 做 zero-shot 分类（技巧）</li>
</ol>
<ul>
<li>思路：为每类别（category）写几条自然语言描述（prompt template），编码成文本向量，得到每个类别的 prototype；把图片编码后与这些 prototype 做相似度比对，选最大者。  </li>
<li>常用技巧：<ul>
<li>prompt template：比如 “A photo of a {label}”，比单字 label 好。</li>
<li>prompt ensemble（或称 prompt‑averaging）：为同一类别构造多个不同风格的描述（photo&#x2F;drawing&#x2F;painting&#x2F;close-up&#x2F;…），对这些向量取平均作为该类别向量，能显著提升 ImageNet 等上的零样本精度。</li>
<li>这相当于把一个类别扩成一组“不同视角”的文本样本，降低偏置。</li>
</ul>
</li>
</ul>
<ol>
<li>CLIP 的优点汇总</li>
</ol>
<ul>
<li>高效的点积相似度检索（检索海量图像可用向量索引）。  </li>
<li>Open‑vocabulary（可以用任意文本描述做类别）。  </li>
<li>可与其他模型轻松串联（检索、生成、指令式任务等）。</li>
</ul>
<ol>
<li>CLIP 的局限与常见改进</li>
</ol>
<ul>
<li>依赖大批次对比学习：大 batch 帮助提供更多“负样本”，提高区分能力。但仅靠批大小并不能学出真正的组合性（compositionality）。  </li>
<li>组合性&#x2F;语义顺序（例如“a mug in grass” vs “grass in a mug”）往往学不好。常见缓解：<ul>
<li>引入 hard negatives（困难负样本的精挑细选、使用交互式对比或困难对比挖掘）。但这会产生“hard positives”问题（误把同类不同配对当负样本）。  </li>
<li>引入 region&#x2F;box 级标注（局部监督），提高细粒度匹配能力。</li>
</ul>
</li>
<li>图像级文本通常语义稀疏：很多细节无法被 image-level captions 覆盖 → 需要 密集 caption、口述描述或人工 annotation（如 PixMo 的做法）。</li>
</ul>
<p>四、CoCa：对比 + 生成的联合目标</p>
<ul>
<li>CoCa 在 CLIP 的基础上加入了生成（captioning）decoder，使模型同时学习对比式嵌入与生成式的描述能力。这样 embedding 既能用于检索&#x2F;分类，又能支持更好的图文生成或评分任务。直观上把“理解”与“表达”两个能力一并训练。</li>
</ul>
<p>五、把视觉接入大语言模型（LLM）：LLaVA、Flamingo 等</p>
<ol>
<li>目标</li>
</ol>
<ul>
<li>让强大的自回归 LLM 能直接以图像 + 文本为输入，输出文本（问答、说明、推理、指令执行等）。优势是利用 LLM 的通用推理与语言技巧，扩展视觉任务的能力。</li>
</ul>
<ol start="2">
<li>LLaVA（思路简要）</li>
</ol>
<ul>
<li>用预训练的视觉 encoder（典型是 CLIP encoder）提取图像表示。把这些 visual features 线性映射到 LLM 的 embedding 空间，然后把它们作为 LLM 的一部分输入（例如作为 prefix tokens）。  </li>
<li>训练流程：初始化来自 pretrained LLM（如 LLaMA）和 visual encoder，训练一个线性桥接层，然后微调 LLM+桥接层（通常冻结 visual encoder 或按需微调）。  </li>
<li>工程要点：从 CLIP 中取哪一层特征？讲稿建议使用 penultimate 层（倒数第二层）的 patch tokens，因为它们保留更多空间信息相比最终 pooled token。不要盲目用 CLS。</li>
</ul>
<ol start="3">
<li>Flamingo（重要设计）</li>
</ol>
<ul>
<li>Flamingo 引入了 gated cross‑attention（门控的跨模态注意力）和 Perceiver Sampler（将可变大小的视觉 token 投影为定长 token）来融合视觉信息。  </li>
<li>它通过特殊的 attention 布局和训练数据格式（在文本序列中插入 <image> 标记）支持 few‑shot 的 in‑context 学习：给模型若干图文示例 + 测试图像，就能在零微调下完成新任务。</li>
</ul>
<p>六）Segmentation 基础模型：SAM（Segment Anything Model）</p>
<ol>
<li>目标</li>
</ol>
<ul>
<li>传统 Mask R‑CNN 按一套固定类别输出掩码（例如 COCO 的 80 类）。SAM 的目标是“任何用户关心的”物体或区域都能被 mask 出来：prompt-driven segmentation（用点、框、文本等提示定位要分割的目标）。</li>
</ul>
<ol start="2">
<li>架构要点（简化）</li>
</ol>
<ul>
<li>大型 image encoder（例如 ViT）提取特征；prompt encoder（处理点&#x2F;框&#x2F;文本）将 prompt 映射成 conditioning；mask decoder 把特征+prompt 解码为 mask，并伴随置信度分数。  </li>
<li>训练数据覆盖大量类型的对象与类别，使其具备强泛化的 zero‑shot 分割能力。</li>
</ul>
<ol start="3">
<li>交互式特征</li>
</ol>
<ul>
<li>因为 prompt 有歧义（用户输入“点在图上”可能指不同对象），SAM 给出多个 mask 候选并返回置信度，用户可通过进一步提示或选择来精化结果。</li>
</ul>
<p>七）链式（Chaining）与 Visual Programming（将多模型组合成更强大的系统）</p>
<ol>
<li>CuPL（Customized Prompts via Language models）</li>
</ol>
<ul>
<li>问题：某些类别模型从未见过（例如非常专业的实体），直接用基本 prompt 可能效果差。解决：用 LLM 为每个类别生成高质量、细粒度的描述（多个 prompt），然后把这些作为 CLIP 的文本 prototype。效果往往比人工写单一句子更好。</li>
</ul>
<ol start="2">
<li>VisProg（Visual Programming）</li>
</ol>
<ul>
<li>思路：把视觉任务拆成一系列小步骤，用不同模型（VQA、detector、segmenter、OCR 等）按逻辑顺序调用，并用一个编排器（由 LLM 生成或控制）构成可组合的“视觉程序”。优点是复用性强，不必为每个复合任务训练新模型。缺点是依赖各子模块的可靠性与接口匹配。</li>
</ul>
<p>八）数据的重要性：规模 vs 意图（intentional data）</p>
<ul>
<li>纯粹“大量网络抓取”确实能带来很多概念，但许多关键能力（细粒度、组合性、稠密理解）依赖“精心设计”的标注数据：密集 captions、区域标注、口述描述、指向性的标注（pointing）、人工偏好比较（用于 SFT &#x2F; RLHF）。  </li>
<li>例：PixMo &#x2F; Molmo 等工作展示：小规模但高质量、精心设计的标注数据 + 强模型架构，能超越仅凭大规模未筛选数据的模型。</li>
</ul>
<p>九）近期趋势与工程要点速记</p>
<ul>
<li>设计上选用 penultimate tokens（而非 pooled CLS）作为 LLM 的视觉输入，保留更多空间信息。  </li>
<li>若用 CLIP 做 zero‑shot 分类：使用 prompt templates、prompt ensemble（平均）、必要时用 LLM 生成 prompts（CuPL）。  </li>
<li>对比学习需注意负样本策略：大 batch 带来更多自然负样本；硬负的引入需要谨慎避免把“hard positives”当负样本。  </li>
<li>对于视觉+语言的 finetune，常用做法是：冻结大部分 LLM（减少灾难性遗忘），只微调小的桥接层或用 LoRA 等低秩调优方法。  </li>
<li>SAM 类交互式模型：提供交互&#x2F;多种提示方式（点&#x2F;框&#x2F;文本），并返回多个候选以处理歧义。  </li>
<li>链接时注意接口（embedding 尺寸、tokenization、position encoding 等），加一层小的线性投影或 adapter 通常就能桥接。</li>
</ul>
<p>十）工程&#x2F;实现示例（伪代码思路）</p>
<ol>
<li>CLIP zero‑shot 分类（伪代码思路）</li>
</ol>
<ul>
<li>输入：图片 I，类别列表 C &#x3D; [c1,…,cK]，prompt templates T &#x3D; [“A photo of a {}”, “A drawing of a {}” …]  </li>
<li>处理：<ul>
<li>image_emb &#x3D; ImageEncoder(I) → 归一化  </li>
<li>for each class ck: text_embs &#x3D; [TextEncoder(t.format(ck)) for t in T]; proto_ck &#x3D; mean(normalize(text_embs))  </li>
<li>scores &#x3D; [dot(image_emb, proto_ck) for proto_ck]; predict &#x3D; argmax(scores)</li>
</ul>
</li>
</ul>
<ol start="2">
<li>LLaVA 风格接入（思路）</li>
</ol>
<ul>
<li>取 CLIP 的 penultimate patch tokens P (shape M × d_v); 用线性映射 W: d_v → d_LLM，把映射后的 tokens 插入到 LLM 的输入序列（作为 prefix），并用微调训练桥接层与 LLM。</li>
</ul>
<p>十一）开放问题与挑战（研究&#x2F;应用角度）</p>
<ul>
<li>组合性与逻辑结构的学习：如何让模型真正理解场景中对象之间的关系（而非仅靠统计关联）？  </li>
<li>数据偏见与安全：大规模抓取数据中含有偏见与敏感内容，如何有系统地过滤与评估？  </li>
<li>交互与可控性：如何在保证泛化的同时给用户简单、可解释的控制接口（如 SAM 的 prompt + mask 候选）？  </li>
<li>可解释性与评估：如何衡量视觉‑语言模型真正“理解”图片而不是依赖提示中的统计线索？（需要精心设计的基准集，如 Winoground、ARO、CREPE 等）</li>
</ul>
<p>十二）课堂练习（建议）</p>
<ol>
<li>推导&#x2F;实现 CLIP 的 InfoNCE 损失（双向），并在小数据集上训练一个 tiny CLIP 来观察效果随 batch 大小变化。  </li>
<li>用预训练 CLIP 做零样本 ImageNet 分类：对比三种 prompt 策略（单字、”A photo of a {}”, prompt ensemble），分析结果差异并写总结。  </li>
<li>设计一个简单的 VisProg：把两张图像的 VQA 输出拼接后做比对（例如判断两个图像是否包含相同物体），将模块封装成调用接口并由一个小规则&#x2F;LLM 脚本编排。  </li>
<li>阅读并比较 Flamingo 与 LLaVA 的视觉融合方式：画出它们的 attention flow（谁 attend 谁？何时融合？）。</li>
</ol>
<p>十三）参考文献（建议阅读）</p>
<ul>
<li>Radford et al., “Learning Transferable Visual Models From Natural Language Supervision” (CLIP, 2021)  </li>
<li>Guu et al., “CoCa: Contrastive Captioners are Image‑Text Foundation Models” (2022)  </li>
<li>Alayrac et al., “Flamingo: a Visual Language Model for Few‑Shot Learning” (2022)  </li>
<li>Li et al., “LLaVA: Large Language and Vision Assistant” (2023)  </li>
<li>Kirillov et al., “Segment Anything” (SAM, 2023)  </li>
<li>Pratt et al., “What does a platypus look like? Generating customized prompts for zero-shot image classification” (CuPL, 2023)  </li>
<li>Gupta et al., “Visual Programming: Compositional visual reasoning without training” (VisProg, 2023)</li>
</ul>
<p>十四）三句话小结</p>
<ul>
<li>Foundation models（尤其视觉-语言）通过大模型+大数据的预训练，实现强大的零样本与少样本泛化。  </li>
<li>CLIP‑style 对比学习提供了极简且强大的跨模态 embedding，使零样本分类与检索变得直接可行，但组合性与细粒度理解仍有短板。  </li>
<li>把视觉表示接入 LLM（LLaVA&#x2F;Flamingo）与构建交互式模块（SAM）、并使用链式编排（CuPL&#x2F;VisProg）是当前把基础模型应用到复杂视觉任务的主流工程路径；数据质量往往比单纯的规模更关键。</li>
</ul>
<p>如果你想要接下来更深一步的材料，可以从下面任选一项，我会立刻展开：</p>
<ul>
<li>给出 CLIP 的 PyTorch 训练&#x2F;评估最小可运行示例（含 InfoNCE 实现与 prompt ensemble 的零样例）。  </li>
<li>给出 LLaVA 风格把 CLIP features 接入 LLM 的微调伪代码（含桥接层、训练&#x2F;数据格式建议与微调策略）。  </li>
<li>给出 SAM 的交互式使用示例与如何在工程上把 SAM 与下游模型（检测&#x2F;跟踪&#x2F;编辑）串联的最佳实践。</li>
</ul>
<p>你要先看哪一个？</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://SmallSmallQi.github.io">ChangQibo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://smallsmallqi.github.io/posts/14569/">https://smallsmallqi.github.io/posts/14569/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://SmallSmallQi.github.io" target="_blank">MyBLog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/myavatar.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/myavatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">ChangQibo</div><div class="author-info-description">欢迎来到常淇博的个人博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">1</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/SmallSmallQi" target="_blank" title="GitHub"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:your-email@3239947093@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E8%AF%BE-%C2%B7-%E5%AF%BC%E8%AE%BA"><span class="toc-number">1.</span> <span class="toc-text">第一课 · 导论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BE%E7%A8%8B%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">课程简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-number">1.2.</span> <span class="toc-text">1. 深度学习基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%84%9F%E7%9F%A5%E4%B8%8E%E7%90%86%E8%A7%A3%E8%A7%86%E8%A7%89%E4%B8%96%E7%95%8C"><span class="toc-number">1.3.</span> <span class="toc-text">2. 感知与理解视觉世界</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%9B%BE%E5%83%8F%E7%90%86%E8%A7%A3%E7%9A%84%E4%B8%BB%E8%A6%81%E4%BB%BB%E5%8A%A1"><span class="toc-number">1.3.1.</span> <span class="toc-text">2.1 图像理解的主要任务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E8%B6%85%E8%B6%8A%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E4%BB%A3%E8%A1%A8%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.2.</span> <span class="toc-text">2.2 超越多层感知机的代表模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%94%9F%E6%88%90%E5%BC%8F%E4%B8%8E%E4%BA%A4%E4%BA%92%E5%BC%8F%E8%A7%86%E8%A7%89%E6%99%BA%E8%83%BD"><span class="toc-number">1.4.</span> <span class="toc-text">3. 生成式与交互式视觉智能</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E8%AF%BE-%C2%B7-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB"><span class="toc-number">2.</span> <span class="toc-text">第二课 · 线性分类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Image-Classification-with-Linear-Classifiers"><span class="toc-number">2.1.</span> <span class="toc-text">Image Classification with Linear Classifiers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E8%8A%82%E8%AF%BE%E7%A8%8B%E6%A0%B8%E5%BF%83%E6%80%BB%E7%BB%93"><span class="toc-number">2.1.1.</span> <span class="toc-text">本节课程核心总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%94%E8%AE%B0%E5%86%85%E5%AE%B9"><span class="toc-number">2.1.2.</span> <span class="toc-text">笔记内容</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E8%AF%BE-%C2%B7-%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="toc-number">3.</span> <span class="toc-text">第三课 · 正则化与优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E8%8A%82%E8%AF%BE%E7%A8%8B%E6%A0%B8%E5%BF%83%E6%80%BB%E7%BB%93-1"><span class="toc-number">3.1.</span> <span class="toc-text">本节课程核心总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%94%E8%AE%B0%E5%86%85%E5%AE%B9-1"><span class="toc-number">3.2.</span> <span class="toc-text">笔记内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Regularization%EF%BC%89"><span class="toc-number">3.3.</span> <span class="toc-text">一、正则化（Regularization）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E4%BC%98%E5%8C%96%EF%BC%88Optimization%EF%BC%89"><span class="toc-number">3.4.</span> <span class="toc-text">二、优化（Optimization）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E6%AC%A1%E8%AF%BE%E7%A8%8B%E9%87%8D%E7%82%B9%E5%85%AC%E5%BC%8F"><span class="toc-number">3.5.</span> <span class="toc-text">本次课程重点公式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AE%8C%E6%95%B4%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-Total-Loss-Function"><span class="toc-number">3.5.1.</span> <span class="toc-text">1. 完整的损失函数 (Total Loss Function)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%99%AE%E9%80%9A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%9B%B4%E6%96%B0-Vanilla-SGD-Update"><span class="toc-number">3.5.2.</span> <span class="toc-text">2. 普通梯度下降更新 (Vanilla SGD Update)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%8A%A8%E9%87%8F%E6%9B%B4%E6%96%B0-SGD-Momentum"><span class="toc-number">3.5.3.</span> <span class="toc-text">3. 动量更新 (SGD + Momentum)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-RMSProp-%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">3.5.4.</span> <span class="toc-text">4. RMSProp (自适应学习率)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-Adam-%E4%B8%87%E8%83%BD%E5%85%AC%E5%BC%8F"><span class="toc-number">3.5.5.</span> <span class="toc-text">5. Adam (万能公式)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E8%AF%BE-%C2%B7-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">4.</span> <span class="toc-text">第四课 · 神经网络与反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%9B%AE%E6%A0%87%E4%B8%8E%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B"><span class="toc-number">4.1.</span> <span class="toc-text">1) 目标与整体流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84"><span class="toc-number">4.2.</span> <span class="toc-text">2) 神经网络的基本结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1"><span class="toc-number">4.3.</span> <span class="toc-text">3) 前向计算与常用损失</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99%EF%BC%89%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">4.4.</span> <span class="toc-text">4) 反向传播（链式法则）的概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%B8%B8%E8%A7%81%E5%B1%82%E4%B8%8E%E5%AE%83%E4%BB%AC%E7%9A%84%E5%8F%8D%E5%90%91%E5%85%AC%E5%BC%8F%EF%BC%88%E5%BF%85%E9%A1%BB%E7%86%9F%E7%BB%83%EF%BC%89"><span class="toc-number">4.5.</span> <span class="toc-text">5) 常见层与它们的反向公式（必须熟练）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E6%95%B0%E5%80%BC%E6%A2%AF%E5%BA%A6%E6%A3%80%E6%9F%A5%EF%BC%88debug-%E5%BF%85%E5%A4%87%EF%BC%89"><span class="toc-number">4.6.</span> <span class="toc-text">6) 数值梯度检查（debug 必备）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E5%B8%B8%E7%94%A8%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88%E5%85%B3%E9%94%AE%E6%9B%B4%E6%96%B0%E5%85%AC%E5%BC%8F%EF%BC%89"><span class="toc-number">4.7.</span> <span class="toc-text">7) 常用优化器（关键更新公式）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E8%AF%BE-%C2%B7-CNN%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB"><span class="toc-number">5.</span> <span class="toc-text">第五课 · CNN图像分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%80%BB%E4%BD%93%E7%9B%AE%E6%A0%87-%E6%8D%9F%E5%A4%B1%EF%BC%88%E5%BF%AB%E9%80%9F%E5%9B%9E%E9%A1%BE%EF%BC%89"><span class="toc-number">5.1.</span> <span class="toc-text">1) 总体目标 &#x2F; 损失（快速回顾）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%8D%B7%E7%A7%AF%E5%B1%82%EF%BC%9A%E5%AE%9A%E4%B9%89%E4%B8%8E%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%EF%BC%88%E5%BC%A0%E9%87%8F%E7%B4%A2%E5%BC%95%E5%BD%A2%E5%BC%8F%EF%BC%89"><span class="toc-number">5.2.</span> <span class="toc-text">2) 卷积层：定义与前向计算（张量索引形式）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%B1%A0%E5%8C%96%E5%B1%82%EF%BC%88Pooling%EF%BC%89"><span class="toc-number">5.3.</span> <span class="toc-text">3) 池化层（Pooling）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%84%9F%E5%8F%97%E9%87%8E%EF%BC%88Receptive-Field%EF%BC%89"><span class="toc-number">5.4.</span> <span class="toc-text">4) 感受野（Receptive Field）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%8D%B7%E7%A7%AF%E7%9A%84%E4%B8%A4%E5%A4%A7%E4%BC%98%E7%82%B9%EF%BC%88%E4%B8%8E-FC-%E6%AF%94%E8%BE%83%EF%BC%89"><span class="toc-number">5.5.</span> <span class="toc-text">5) 卷积的两大优点（与 FC 比较）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E4%B8%8B%E9%87%87%E6%A0%B7%EF%BC%88Downsampling%EF%BC%89%EF%BC%9Astride-conv-vs-pooling"><span class="toc-number">5.6.</span> <span class="toc-text">6) 下采样（Downsampling）：stride conv vs pooling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9%E5%9B%9E%E9%A1%BE"><span class="toc-number">5.7.</span> <span class="toc-text">深度学习内容回顾</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E8%AF%BE-%C2%B7-CNN%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%9E%B6%E6%9E%84"><span class="toc-number">6.</span> <span class="toc-text">第六课 · CNN网络训练与架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%9E%B6%E6%9E%84"><span class="toc-number">6.1.</span> <span class="toc-text">CNN网络训练与架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.2.</span> <span class="toc-text">1. 如何构建卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81CNN%E7%9A%84%E9%87%8D%E8%A6%81%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86"><span class="toc-number">6.2.1.</span> <span class="toc-text">一、CNN的重要组成部分</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%8D%B7%E7%A7%AF%E5%B1%82%EF%BC%9A"><span class="toc-number">6.2.1.1.</span> <span class="toc-text">1. 卷积层：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%B1%A0%E5%8C%96%E5%B1%82%EF%BC%9A"><span class="toc-number">6.2.1.2.</span> <span class="toc-text">2. 池化层：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%9A"><span class="toc-number">6.2.1.3.</span> <span class="toc-text">3. 全连接层：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82%EF%BC%9A"><span class="toc-number">6.2.1.4.</span> <span class="toc-text">4. 归一化层：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-Dropout%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%9A"><span class="toc-number">6.2.1.5.</span> <span class="toc-text">5. Dropout正则化：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9A"><span class="toc-number">6.2.2.</span> <span class="toc-text">二、激活函数：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81CNN%E6%9E%B6%E6%9E%84"><span class="toc-number">6.2.3.</span> <span class="toc-text">三、CNN架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%82%E7%9A%84%E6%9D%83%E9%87%8D"><span class="toc-number">6.2.4.</span> <span class="toc-text">四、初始化神经网络层的权重</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.3.</span> <span class="toc-text">2. 如何训练卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">6.3.1.</span> <span class="toc-text">1. 数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">6.3.2.</span> <span class="toc-text">2. 数据增强</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.3.3.</span> <span class="toc-text">3. 迁移学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E9%80%89%E6%8B%A9%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-number">6.3.4.</span> <span class="toc-text">4. 选择超参数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%83%E8%AF%BE-%C2%B7-RNN"><span class="toc-number">7.</span> <span class="toc-text">第七课 · RNN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Lecture-7%EF%BC%9A%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88RNN%EF%BC%89"><span class="toc-number">8.</span> <span class="toc-text">Lecture 7：循环神经网络（RNN）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E4%BB%8E%E6%99%AE%E9%80%9A%EF%BC%88%E5%89%8D%E9%A6%88%EF%BC%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%B0-RNN%EF%BC%9A%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%85%AC%E5%BC%8F"><span class="toc-number">8.1.</span> <span class="toc-text">三、从普通（前馈）神经网络到 RNN：基本概念与公式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%B1%95%E5%BC%80%EF%BC%88Unroll%EF%BC%89%E4%B8%8E%E8%AE%A1%E7%AE%97%E5%9B%BE%EF%BC%9B%E4%BB%BB%E5%8A%A1%E7%B1%BB%E5%9E%8B"><span class="toc-number">8.2.</span> <span class="toc-text">四、展开（Unroll）与计算图；任务类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E8%AE%AD%E7%BB%83%EF%BC%9A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E9%80%9A%E8%BF%87%E6%97%B6%E9%97%B4%EF%BC%88BPTT%EF%BC%89%E4%B8%8E%E6%88%AA%E6%96%AD-BPTT"><span class="toc-number">8.3.</span> <span class="toc-text">五、训练：反向传播通过时间（BPTT）与截断 BPTT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E7%A4%BA%E4%BE%8B%EF%BC%9A%E5%AD%97%E7%AC%A6%E7%BA%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88Char-RNN%EF%BC%89"><span class="toc-number">8.4.</span> <span class="toc-text">六、示例：字符级语言模型（Char-RNN）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81RNN-%E7%9A%84%E4%BC%98%E7%82%B9%E4%B8%8E%E7%BC%BA%E7%82%B9%EF%BC%88%E5%B9%BB%E7%81%AF%E7%89%87%E4%B8%AD%E4%B9%9F%E6%9C%89%E6%80%BB%E7%BB%93%EF%BC%89"><span class="toc-number">8.5.</span> <span class="toc-text">七、RNN 的优点与缺点（幻灯片中也有总结）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E7%88%86%E7%82%B8%EF%BC%9A%E7%9B%B4%E8%A7%82-%E6%95%B0%E5%AD%A6%E8%A7%A3%E9%87%8A"><span class="toc-number">8.6.</span> <span class="toc-text">八、梯度消失与爆炸：直观 + 数学解释</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B9%9D%E3%80%81LSTM%EF%BC%88Long-Short-Term-Memory%EF%BC%89%EF%BC%9A%E7%BB%93%E6%9E%84%E3%80%81%E5%85%AC%E5%BC%8F%E4%B8%8E%E7%9B%B4%E8%A7%82%E8%A7%A3%E9%87%8A"><span class="toc-number">8.7.</span> <span class="toc-text">九、LSTM（Long Short-Term Memory）：结构、公式与直观解释</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%85%AB%E8%AF%BE-%C2%B7-Attention%E6%9C%BA%E5%88%B6%E4%B8%8ETransformers"><span class="toc-number">9.</span> <span class="toc-text">第八课 · Attention机制与Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%83%E7%AB%A0-Attention-Transformer"><span class="toc-number">9.1.</span> <span class="toc-text">第七章 Attention &amp; Transformer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83"><span class="toc-number">9.2.</span> <span class="toc-text">核心</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer%E6%9E%B6%E6%9E%84"><span class="toc-number">9.2.1.</span> <span class="toc-text">Transformer架构</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B9%9D%E8%AF%BE-%C2%B7-%E6%A3%80%E6%B5%8B%E5%88%86%E5%89%B2%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B8%8E%E7%90%86%E8%A7%A3"><span class="toc-number">10.</span> <span class="toc-text">第九课 · 检测分割可视化与理解</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%8D%81%E8%AF%BE-%C2%B7-%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3"><span class="toc-number">11.</span> <span class="toc-text">第十课 · 视频理解</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%8D%81%E4%B8%80%E8%AF%BE-%C2%B7-%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83"><span class="toc-number">12.</span> <span class="toc-text">第十一课 · 大规模分布式训练</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%8D%81%E4%BA%8C%E8%AF%BE-%C2%B7-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">13.</span> <span class="toc-text">第十二课 · 自监督学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%8D%81%E4%B8%89%E3%80%81%E5%8D%81%E5%9B%9B%E8%AF%BE-%C2%B7-%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B"><span class="toc-number">14.</span> <span class="toc-text">第十三、十四课 · 生成式模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%8D%81%E5%85%AD%E8%AF%BE-%C2%B7-%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">15.</span> <span class="toc-text">第十六课 · 多模态大模型</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/14569/" title="深度学习个人笔记(MIT-CS231n)"><img src="/img/myavatar.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="深度学习个人笔记(MIT-CS231n)"/></a><div class="content"><a class="title" href="/posts/14569/" title="深度学习个人笔记(MIT-CS231n)">深度学习个人笔记(MIT-CS231n)</a><time datetime="2026-01-29T08:00:00.000Z" title="发表于 2026-01-29 16:00:00">2026-01-29</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/myavatar.jpg);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By ChangQibo</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>