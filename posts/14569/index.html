<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>深度学习个人笔记(MIT-CS231n) | MyBLog</title><meta name="author" content="ChangQibo"><meta name="copyright" content="ChangQibo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="这篇笔记是我学习 MIT-CS231n 课程后写出的深度学习个人笔记，主要内容包括深度学习基础、视觉感知与理解，以及生成式与交互式视觉智能等方向。  本文为个人学习笔记，整理自 MIT-CS231n 课程内容，仅供学习交流使用。作者：常淇博邮箱：（3239947093@qq.com）如有侵权或错误，请联系作者及时修正。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习个人笔记(MIT-CS231n)">
<meta property="og:url" content="https://smallsmallqi.github.io/posts/14569/index.html">
<meta property="og:site_name" content="MyBLog">
<meta property="og:description" content="这篇笔记是我学习 MIT-CS231n 课程后写出的深度学习个人笔记，主要内容包括深度学习基础、视觉感知与理解，以及生成式与交互式视觉智能等方向。  本文为个人学习笔记，整理自 MIT-CS231n 课程内容，仅供学习交流使用。作者：常淇博邮箱：（3239947093@qq.com）如有侵权或错误，请联系作者及时修正。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://smallsmallqi.github.io/img/myavatar.jpg">
<meta property="article:published_time" content="2026-01-29T08:00:00.000Z">
<meta property="article:modified_time" content="2026-02-04T02:41:05.367Z">
<meta property="article:author" content="ChangQibo">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://smallsmallqi.github.io/img/myavatar.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "深度学习个人笔记(MIT-CS231n)",
  "url": "https://smallsmallqi.github.io/posts/14569/",
  "image": "https://smallsmallqi.github.io/img/myavatar.jpg",
  "datePublished": "2026-01-29T08:00:00.000Z",
  "dateModified": "2026-02-04T02:41:05.367Z",
  "author": [
    {
      "@type": "Person",
      "name": "ChangQibo",
      "url": "https://SmallSmallQi.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://smallsmallqi.github.io/posts/14569/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习个人笔记(MIT-CS231n)',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/myavatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">1</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post type-tags" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/myavatar.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/myavatar.jpg" alt="Logo"><span class="site-name">MyBLog</span></a><a class="nav-page-title" href="/"><span class="site-name">深度学习个人笔记(MIT-CS231n)</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">深度学习个人笔记(MIT-CS231n)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2026-01-29T08:00:00.000Z" title="发表于 2026-01-29 16:00:00">2026-01-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-02-04T02:41:05.367Z" title="更新于 2026-02-04 10:41:05">2026-02-04</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>这篇笔记是我学习 MIT-CS231n 课程后写出的深度学习个人笔记，主要内容包括深度学习基础、视觉感知与理解，以及生成式与交互式视觉智能等方向。</p>
<blockquote>
<p>本文为个人学习笔记，整理自 MIT-CS231n 课程内容，仅供学习交流使用。<br>作者：常淇博<br>邮箱：（<a href="mailto:&#51;&#50;&#x33;&#x39;&#57;&#52;&#55;&#48;&#x39;&#51;&#x40;&#x71;&#113;&#46;&#x63;&#111;&#109;">3239947093@qq.com</a>）<br>如有侵权或错误，请联系作者及时修正。</p>
</blockquote>
<span id="more"></span>

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>


<h1 id="第一课-·-导论"><a href="#第一课-·-导论" class="headerlink" title="第一课 · 导论"></a>第一课 · 导论</h1><h2 id="课程简介"><a href="#课程简介" class="headerlink" title="课程简介"></a>课程简介</h2><p>本课程以深度学习为工具，讲解计算机视觉（CV）与机器学习（ML）中的核心概念与方法，侧重实战与直观理解。主要内容包括深度学习基础、视觉感知与理解、以及生成式与交互式视觉智能等方向。</p>
<div align="center">
<img src="/img/image-0.png" alt="课程概览" width="80%" />
</div>

<hr>
<h2 id="1-深度学习基础"><a href="#1-深度学习基础" class="headerlink" title="1. 深度学习基础"></a>1. 深度学习基础</h2><p>计算机视觉的一个核心任务是图像分类。常见的分类方法包括：</p>
<ol>
<li><p>线性分类  </p>
<ul>
<li>基本思想：通过线性决策边界对样本进行划分，适用于特征线性可分或经特征工程后可线性分离的场景。</li>
</ul>
<div align="center"><img src="/img/image-1.png" alt="线性分类" width="40%" /></div>
</li>
<li><p>正则化与优化（Regularization &amp; Optimization）  </p>
<ul>
<li>包括 L1&#x2F;L2 正则化、dropout、早停等，用以缓解过拟合；以及常见的优化算法（SGD、Adam 等）。</li>
</ul>
<div align="center"><img src="/img/image-2.png" alt="正则化与优化" width="60%" /></div>
</li>
<li><p>神经网络（Neural Networks）  </p>
<ul>
<li>多层感知机到深度网络，通过非线性激活函数与层叠结构学习复杂映射。</li>
</ul>
<div align="center"><img src="/img/image-3.png" alt="神经网络" width="60%" /></div></li>
</ol>
<hr>
<h2 id="2-感知与理解视觉世界"><a href="#2-感知与理解视觉世界" class="headerlink" title="2. 感知与理解视觉世界"></a>2. 感知与理解视觉世界</h2><h3 id="2-1-图像理解的主要任务"><a href="#2-1-图像理解的主要任务" class="headerlink" title="2.1 图像理解的主要任务"></a>2.1 图像理解的主要任务</h3><p>常见任务及简要说明：</p>
<ol>
<li>直接分类（classification） — 将整张图像分到某个类别。  </li>
<li>语义分割（semantic segmentation） — 对每个像素进行语义标注。  </li>
<li>目标检测（object detection） — 标出图中每个目标的边界框并分类。  </li>
<li>实例分割（instance segmentation） — 在语义分割基础上区分不同实例。  </li>
<li>其他：视频分类、多模态视频理解、可视化与可解释性等。</li>
</ol>
<hr>
<h3 id="2-2-超越多层感知机的代表模型"><a href="#2-2-超越多层感知机的代表模型" class="headerlink" title="2.2 超越多层感知机的代表模型"></a>2.2 超越多层感知机的代表模型</h3><ol>
<li><p>CNN（Convolutional Neural Network，卷积神经网络）  </p>
<ul>
<li>擅长提取局部空间特征、尺度不变性和参数共享。</li>
</ul>
<div align="center"><img src="/img/image-4.png" alt="CNN" width="70%" /></div>
</li>
<li><p>RNN（Recurrent Neural Network，循环神经网络）  </p>
<ul>
<li>处理序列数据（如视频帧、文本），能够建模时序依赖。</li>
</ul>
<div align="center"><img src="/img/image-5.png" alt="RNN" width="60%" /></div>
</li>
<li><p>Attention 机制 &amp; Transformers  </p>
<ul>
<li>通过自注意力机制捕获长程依赖，已经成为图像与多模态任务的重要架构基础。</li>
</ul>
<div align="center"><img src="/img/image-6.png" alt="Attention & Transformers" width="60%" /></div>
</li>
<li><p>大规模分布式训练（Large-Scale Distributed Training）  </p>
<ul>
<li>当模型与数据规模很大时，需要借助多GPU&#x2F;多机训练，常用策略有：  <ul>
<li>数据并行（Data Parallel）：把数据切分给多个 worker，每个 worker 保持完整模型副本。  </li>
<li>模型并行（Model Parallel）：将模型切分到不同设备上，适用于单卡内存不足的超大模型。</li>
</ul>
</li>
</ul>
<div align="center"><img src="/img/image-7.png" alt="大规模分布式训练" width="60%" /></div></li>
</ol>
<hr>
<h2 id="3-生成式与交互式视觉智能"><a href="#3-生成式与交互式视觉智能" class="headerlink" title="3. 生成式与交互式视觉智能"></a>3. 生成式与交互式视觉智能</h2><p>当前热门方向，代表性主题包括：</p>
<ul>
<li>自监督学习（Self-supervised Learning）  <ul>
<li>利用未标注数据设计预训练任务以学习表征。</li>
</ul>
</li>
<li>生成式模型（Generative Modeling）  <ul>
<li>如 GAN、VAE、Diffusion Models，用于图像生成与补全。</li>
</ul>
</li>
<li>视觉-语言模型（Vision-Language Models）  <ul>
<li>融合图像与文本信息，支持检索、描述生成、跨模态理解等。</li>
</ul>
</li>
<li>3D 视觉（3D Vision）  <ul>
<li>深入点云、体素、神经渲染等三维表示与重建。</li>
</ul>
</li>
<li>具身智能（Embodied Intelligence）  <ul>
<li>感知与动作闭环，为机器人与交互系统提供视觉驱动能力。</li>
</ul>
</li>
</ul>
<h1 id="第二课-·-线性分类"><a href="#第二课-·-线性分类" class="headerlink" title="第二课 · 线性分类"></a>第二课 · 线性分类</h1><h2 id="Image-Classification-with-Linear-Classifiers"><a href="#Image-Classification-with-Linear-Classifiers" class="headerlink" title="Image Classification with Linear Classifiers"></a>Image Classification with Linear Classifiers</h2><h3 id="本节课程核心总结"><a href="#本节课程核心总结" class="headerlink" title="本节课程核心总结"></a>本节课程核心总结</h3><p>在机器学习中，为了实现图像分类（即输入一个像素张量，要求输出分类标签），可以使用两种最基础的图像分类方法：</p>
<ol>
<li>非参数方法：K-Nearest Neighbors <strong>KNN最近邻</strong></li>
<li>参数方法：Linear classifiers <strong>线性分类器</strong>（包含Softmax 多类逻辑回归）与常用损失（SVM hinge，交叉熵）</li>
</ol>
<h3 id="笔记内容"><a href="#笔记内容" class="headerlink" title="笔记内容"></a>笔记内容</h3><ol>
<li><p>KNN分类器</p>
<ol>
<li>定义：KNN是一种基于实例的学习方法，通过计算样本之间的距离来进行分类。</li>
<li>训练：让模型直接记住所有的训练数据与标签</li>
<li>预测：直接从训练集中找到与测试图最相似的标签作为结果</li>
<li>量化图像相似度的方法：曼哈顿距离与欧氏距离</li>
</ol>
<div align="center"><img src="/img/image-66.png" alt="KNN" width="80%" /></div>
5. 超参数调优：
 将数据分为训练集、验证集和测试集。
 在训练集上训练不同的模型（使用不同的超参数）。
 在验证集上评估这些模型，选择表现最好的超参数。
 最终，在测试集上仅运行一次，以报告模型的最终性能。
</li>
<li><p>线性分类器</p>
<ol>
<li><p>定义：不再存储训练数据，而是定义一个模型 <strong>f(x, W) &#x3D; Wx + b</strong>，其中 x 是输入图像（被展平为一个长向量），W 是权重矩阵，b 是偏置向量。通过学习得到最优的参数 W 和 b，使得这个函数能够输出正确的分类分数。<div align="center"><img src="/img/image-67.png" alt="图示" width="80%" /></div></p>
</li>
<li><p>训练线性分类器：<br>定义损失函数：量化预测分数与真实标签之间的差距。<br>优化：寻找能使损失函数最小化的参数</p>
</li>
<li><p>损失函数有两种：</p>
<ol>
<li><p>SVM多类支持向量机损失<br>  对于第 $i$ 条数据，其损失 $L_i$ 的计算公式为：<br>  $$L_i &#x3D; \sum_{j \ne y_i} \max(0, s_j - s_{y_i} + \Delta)$$</p>
<ul>
<li>$s_{y_i}$：正确类别的得分。</li>
<li>$s_j$：除正确类别外，其他错误类别的得分。</li>
<li>$\Delta$：设定的安全距离（通常取 1）。</li>
<li>$\max(0, \cdot)$：表示如果括号里的值小于 0，就取 0。<br>  $$L_{\text{SVM}}(f(x;W), y) &#x3D; \max(0, 1 - y \cdot f(x;W))$$</li>
</ul>
</li>
<li><p>Softmax分类器（多项逻辑回归）<br>  SVM 只在乎“得分的差距”，而 Softmax 更贪心，它想把得分变成<strong>概率</strong>。</p>
<ul>
<li><p>它把原始得分通过指数函数映射成正数，再做归一化，让所有类别的预测概率加起来等于 1。</p>
</li>
<li><p>它的目标是：让正确类别的概率尽可能接近 <strong>100% (1.0)</strong>。如果你只给正确答案 10% 的概率，模型就会非常痛苦（损失很大）。</p>
</li>
<li><p>第一步：计算 Softmax 函数（得分转概率）<br> 正确类别的预测概率 $P$ 为：<br> $$P(Y &#x3D; y_i \mid X &#x3D; x_i) &#x3D; \frac{e^{s_{y_i}}}{\sum_j e^{s_j}}$$</p>
</li>
<li><p>第二步：计算交叉熵损失 (Cross-Entropy Loss)<br> $$L_i &#x3D; -\log(P) &#x3D; -\log\left( \frac{e^{s_{y_i}}}{\sum_j e^{s_j}} \right)$$</p>
</li>
<li><p>$e$ 是自然对数的底数（约 2.718）。</p>
</li>
<li><p>$-\log$ 的特性是：当概率 $P$ 趋近于 1 时，损失趋近于 0；当概率 $P$ 趋近于 0 时，损失趋近于无穷大。</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p>数据集的总损失 (Total Loss)<br>   我们不仅关心一张图片的损失，更关心整个学校仓库（数据集）里所有图片的表现。</p>
</li>
</ol>
<p>   $$L &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} L_i + \lambda R(W)$$</p>
<p>   这里包含两部分：</p>
<ol>
<li><strong>数据损失 (Data Loss)</strong>：$\frac{1}{N} \sum L_i$，即所有样本损失的平均值。</li>
<li><strong>正则化损失 (Regularization Loss)</strong>：$\lambda R(W)$。<ul>
<li><strong>比喻</strong>：这是为了防止模型“死记硬背”每一张图的细节。我们希望模型学到的是大规律，而不是极其复杂的权重 $W$。常见的正则项是 $L2$ 正则：$R(W) &#x3D; \sum \sum W_{k,l}^2$。</li>
</ul>
</li>
</ol>
</li>
</ol>
<table>
<thead>
<tr>
<th align="left">特性</th>
<th align="left"><strong>Multiclass SVM</strong></th>
<th align="left"><strong>Softmax (Cross-Entropy)</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>关注点</strong></td>
<td align="left">正确分比错误分高出一个 Margin 即可（够好就行）。</td>
<td align="left">正确概率越高越好，永远不嫌多（精益求精）。</td>
</tr>
<tr>
<td align="left"><strong>解释</strong></td>
<td align="left">几何视角的“安全边界”。</td>
<td align="left">概率视角的“极大似然估计”。</td>
</tr>
<tr>
<td align="left"><strong>初始化检查</strong></td>
<td align="left">初始 Loss 通常约等于 类数 - 1。</td>
<td align="left">初始 Loss 通常约等于 $\log(\text{类数})$。</td>
</tr>
</tbody></table>
<h1 id="第三课-·-正则化与优化"><a href="#第三课-·-正则化与优化" class="headerlink" title="第三课 · 正则化与优化"></a>第三课 · 正则化与优化</h1><h2 id="本节课程核心总结-1"><a href="#本节课程核心总结-1" class="headerlink" title="本节课程核心总结"></a>本节课程核心总结</h2><p>在机器学习中，为了提升模型的泛化能力与训练效率，常用以下两大技术手段：</p>
<ol>
<li>正则化（Regularization）：通过在损失函数中添加正则项，防止模型对训练数据过度拟合，从而避免拟合到数据中的噪声，从而提高在未见数据上的泛化能力。<ul>
<li>没有正则化的大模型容易把训练集的噪声也“记住”，像学生把训练题答案死记住而无法解新题。正则化就是鼓励学生学习通用解法而不是死记答案。正则化像是给函数加一层平滑的约束，不允许模型在训练点周围做急剧震荡，从而避免在新点的巨大误差。</li>
</ul>
</li>
<li>优化（Optimization）：采用高效的优化算法（如梯度下降及其变种）来加速模型参数的收敛过程，提高训练效率。<ul>
<li>目标：在参数空间中寻找参数 W 使得 <strong>经验损失 + 正则化</strong> 最小：<br>$$<br>\min_W; L(W)&#x3D;\frac{1}{N}\sum_{i&#x3D;1}^N \ell\big(f(x_i;W),y_i\big)+\lambda R(W).<br>$$</li>
</ul>
</li>
</ol>
<h2 id="笔记内容-1"><a href="#笔记内容-1" class="headerlink" title="笔记内容"></a>笔记内容</h2><h3 id="一、正则化（Regularization）"><a href="#一、正则化（Regularization）" class="headerlink" title="一、正则化（Regularization）"></a>一、正则化（Regularization）</h3><ol>
<li><p>总体损失形式<br>对训练集上的数据损失（data loss）加上正则化项（regularizer）：<br>$$<br>L(W) &#x3D; \frac{1}{N}\sum_{i&#x3D;1}^N L_{\text{data}}(f(x_i;W), y_i) + \lambda R(W),<br>$$<br>其中 $$\lambda$$ 是正则化强度（超参数）。</p>
</li>
<li><p>常见正则化方法</p>
<ul>
<li>L2（权重衰减 &#x2F; Ridge）：<br> $$<br>R(W) &#x3D; \sum_k \sum_l W_{k,l}^2<br>$$<br>缩小权重，倾向于将所有特征都保留，但降低影响力</li>
<li>L1（Lasso）：<br>$$<br>R(W) &#x3D; \sum_k \sum_l |W_{k,l}|<br>$$<br>产生稀疏解，即让部分权重精确为零</li>
<li>Elastic Net（L1 + L2）：<br>$$<br>R(W) &#x3D; \sum_k \sum_l \beta W_{k,l}^2 + |W_{k,l}|<br>$$<br>  平衡L1和L2的特点，既能产生稀疏解，又能处理特征相关性</li>
</ul>
</li>
<li><p>为什么需要正则化</p>
<ul>
<li>减少过拟合（减少模型对训练噪声的拟合）。  </li>
<li>让模型更简单，以便于在测试集上更好的工作</li>
<li>通过增加“额外曲率”的方式促进正则化。</li>
</ul>
</li>
</ol>
<h3 id="二、优化（Optimization）"><a href="#二、优化（Optimization）" class="headerlink" title="二、优化（Optimization）"></a>二、优化（Optimization）</h3><ol>
<li><p>最优化策略：</p>
<ol>
<li>Random search，随机尝试很多不同的权重，然后看其中哪个最好。</li>
<li>Follow the slope，在多维空间中，梯度是沿每个维度的（偏导数）向量。任意方向的坡度是该方向与梯度的点积。最陡下降的方向是负梯度。跟随梯度找到损失最小的方法</li>
</ol>
</li>
<li><p>梯度下降(Gradient Descent)</p>
<ol>
<li>程序重复地计算损失函数的梯度，然后对参数进行更新，直到结果不再变化</li>
</ol>
<div align="center"><img src="/img/image-13.png" alt="图示" 
width="80%" /></div>
![alt text](image.png)</li>
<li><p>随机梯度下降(Stochastic Gradient Descent) (SGD)</p>
<ol>
<li>挑选数据集中的一批数据来进行训练</li>
</ol>
<div align="center"><img src="/img/image-14.png" alt="图示" width="80%" /></div>
![alt text](image-1.png)  

<ol start="2">
<li>会产生一些问题，例如实际进展很慢，会产生抖动；出现梯度为0的情况，梯度下降被卡住，梯度来自小批量数据，可能会有噪声</li>
</ol>
</li>
<li><p>SGD + Momentum 动量</p>
<ol>
<li>在下降过程中添加动量，让其大步前进</li>
</ol>
<div align="center"><img src="/img/image-15.png" alt="图示" width="80%" /></div>
</li>
<li><p>AdaGrad&#x2F;RMSProp </p>
<ol>
<li>如果有某个方向梯度一直很大（震荡），那就把这个方向的学习率调小点（除以梯度的平方和积累）；如果某个方向梯度一直很小（平缓），就把学习率调大点</li>
</ol>
</li>
</ol>
  <div align="center"><img src="/img/image-16.png" alt="图示" width="80%" /></div>

<ol start="6">
<li>Adam(即Momentum + RMSProp)<ol>
<li>既有惯性（一阶矩），又有自适应学习率（二阶矩）</li>
</ol>
<div align="center"><img src="/img/image-17.png" alt="图示" width="80%" /></div></li>
</ol>
<h2 id="本次课程重点公式"><a href="#本次课程重点公式" class="headerlink" title="本次课程重点公式"></a>本次课程重点公式</h2><h4 id="1-完整的损失函数-Total-Loss-Function"><a href="#1-完整的损失函数-Total-Loss-Function" class="headerlink" title="1. 完整的损失函数 (Total Loss Function)"></a>1. 完整的损失函数 (Total Loss Function)</h4><p>这是我们优化的目标。</p>
<p>$$L(W) &#x3D; \underbrace{\frac{1}{N} \sum_{i&#x3D;1}^{N} L_i(f(x_i, W), y_i)}<em>{\text{Data Loss (拟合数据)}} + \underbrace{\lambda R(W)}</em>{\text{Regularization Loss (保持简单)}}$$</p>
<ul>
<li>$N$: 训练样本数量</li>
<li>$\lambda$: 正则化强度（超参数，你需要调）</li>
<li>$R(W)$: 正则化项，通常是 $L2$：$R(W) &#x3D; \sum_k \sum_l W_{k,l}^2$</li>
</ul>
<h4 id="2-普通梯度下降更新-Vanilla-SGD-Update"><a href="#2-普通梯度下降更新-Vanilla-SGD-Update" class="headerlink" title="2. 普通梯度下降更新 (Vanilla SGD Update)"></a>2. 普通梯度下降更新 (Vanilla SGD Update)</h4><p>最基础的更新公式。</p>
<p>$$W_{t+1} &#x3D; W_t - \eta \cdot \nabla L(W_t)$$</p>
<ul>
<li>$W_t$: 当前时刻的权重</li>
<li>$\eta$ (eta): <strong>学习率 (Learning Rate)</strong>，最重要的超参数，决定步子迈多大。</li>
<li>$\nabla L$: 损失函数关于权重的梯度。</li>
</ul>
<h4 id="3-动量更新-SGD-Momentum"><a href="#3-动量更新-SGD-Momentum" class="headerlink" title="3. 动量更新 (SGD + Momentum)"></a>3. 动量更新 (SGD + Momentum)</h4><p>加上了物理惯性，这是对 SGD 最重要的改进之一。</p>
<p>$$\begin{aligned} v_{t+1} &amp;&#x3D; \rho v_t + \nabla L(W_t) \ W_{t+1} &amp;&#x3D; W_t - \alpha v_{t+1} \end{aligned}$$</p>
<ul>
<li>$v$: 速度向量 (Velocity)，初始为0。</li>
<li>$\rho$ (rho): <strong>摩擦系数&#x2F;动量衰减</strong>，通常取 0.9 或 0.99。</li>
<li>$\alpha$: 学习率。</li>
<li><strong>理解</strong>：现在的更新方向不仅仅取决于当前的梯度（脚下的路），还取决于之前的速度 $v_t$（惯性）。</li>
</ul>
<h4 id="4-RMSProp-自适应学习率"><a href="#4-RMSProp-自适应学习率" class="headerlink" title="4. RMSProp (自适应学习率)"></a>4. RMSProp (自适应学习率)</h4><p>让每个参数有自己的学习率步伐。</p>
<p>$$\begin{aligned} cache_{t+1} &amp;&#x3D; \text{decay_rate} \cdot cache_t + (1 - \text{decay_rate}) \cdot (\nabla L)^2 \ W_{t+1} &amp;&#x3D; W_t - \frac{\eta}{\sqrt{cache_{t+1}} + \epsilon} \cdot \nabla L \end{aligned}$$</p>
<ul>
<li>$cache$: 梯度的平方的滑动平均值（衡量该方向是否陡峭）。</li>
<li>$(\nabla L)^2$: 梯度逐元素平方。</li>
<li><strong>理解</strong>：除以 $\sqrt{cache}$ 意味着——如果某个方向梯度一直很大（陡峭），分母变大，更新步长变小（防止震荡）；反之步长变大。</li>
</ul>
<h4 id="5-Adam-万能公式"><a href="#5-Adam-万能公式" class="headerlink" title="5. Adam (万能公式)"></a>5. Adam (万能公式)</h4><p>结合了 Momentum 和 RMSProp。</p>
<p>$$\begin{aligned} m_{t+1} &amp;&#x3D; \beta_1 m_t + (1-\beta_1) \nabla L \quad \text{(一阶矩: 类似动量)} \ v_{t+1} &amp;&#x3D; \beta_2 v_t + (1-\beta_2) (\nabla L)^2 \quad \text{(二阶矩: 类似RMSProp)} \ W_{t+1} &amp;&#x3D; W_t - \frac{\eta}{\sqrt{\hat{v}<em>{t+1}} + \epsilon} \cdot \hat{m}</em>{t+1} \end{aligned}$$</p>
<ul>
<li>$\hat{m}, \hat{v}$: 是修正了初始偏差后的 $m$ 和 $v$（Bias correction）。</li>
<li><strong>推荐配置</strong>: $\beta_1 &#x3D; 0.9$, $\beta_2 &#x3D; 0.999$, $\epsilon &#x3D; 1e-8$, 学习率 $\eta \approx 1e-3$。</li>
</ul>
<h1 id="第四课-·-神经网络与反向传播"><a href="#第四课-·-神经网络与反向传播" class="headerlink" title="第四课 · 神经网络与反向传播"></a>第四课 · 神经网络与反向传播</h1><h2 id="本节课程核心总结-2"><a href="#本节课程核心总结-2" class="headerlink" title="本节课程核心总结"></a>本节课程核心总结</h2><ul>
<li>什么是神经网络：神经网络（Neural Networks）是由多层线性变换和非线性激活函数组成的复杂函数映射，本质上是把多个线性变换与逐点非线性激活叠加，构成强大的函数近似器。</li>
<li>什么是多层神经网络（MLP）：MLP（多层神经网络）也叫全连接神经网络，由输入层、隐藏层、输出层一共三类层组成，<strong>全连接</strong>意味着前一层的每一个神经元，都会连接到后一层的每一个神经元。每一条连接线上都有一个可学习的权重<strong>W</strong></li>
<li>W为什么需要非线性激活：如果MLP只做线性组合，无论堆多少层结果依然是线性变化，只有在之后套上非线性函数<strong>σ</strong>即sigmoid或ReLU或Tanh，才能让网络学到复杂的非线性映射。</li>
</ul>
<h2 id="笔记内容-2"><a href="#笔记内容-2" class="headerlink" title="笔记内容"></a>笔记内容</h2><ol>
<li>从线性到多层网络</li>
</ol>
<ul>
<li>线性模型的形式：<br>[<br>s &#x3D; f(x;W,b) &#x3D; Wx + b.<br>]<br>无论你堆多少层线性变换，最终仍是线性：A(Bx) &#x3D; (AB)x。也就是说，单纯堆线性层没有增加表达能力。  </li>
<li>2‑layer MLP（含非线性激活）：<br>[<br>h &#x3D; f(xW_1 + b_1),\qquad s &#x3D; hW_2 + b_2.<br>]<br>这里的 (f(\cdot)) 是逐元素非线性（activation）。  </li>
<li>直观：非线性激活把输入投影到一个更有判别力的“特征空间”，在该空间内简单的线性分割（最后一层）就能完成复杂任务。类比：把复杂曲线在某个非线性变换下“摊平”成直线可分。</li>
</ul>
<div align="center"><img src="/img/image-102.png" alt="图示" width="80%" /></div>

<ol start="2">
<li>神经网络结构</li>
</ol>
<div align="center"><img src="/img/image-18.png" alt="图示" width="80%" /></div>
<div align="center"><img src="/img/image-103.png" alt="图示" width="80%" /></div>

<ol start="3">
<li>训练流程</li>
</ol>
<ul>
<li>一个训练循环的高层伪代码逻辑：<ol>
<li>forward(x): 计算 s &#x3D; model(x)。  </li>
<li>loss &#x3D; Loss(s, y)（例如 softmax + CE）。  </li>
<li>backward(loss): 计算每个参数的梯度。  </li>
<li>参数更新（SGD&#x2F;Adam 等）。</li>
</ol>
</li>
</ul>
<ol start="4">
<li>激活函数</li>
</ol>
<ul>
<li>常用激活及其性质：<ul>
<li>ReLU：( \operatorname{ReLU}(x)&#x3D;\max(0,x) )。优点：计算简单、不饱和（正区间梯度恒为 1），收敛快，是现代默认。缺点：负区间梯度为 0，可能出现“dead ReLU”。<br>导数： ( \mathbf{1}_{{x&gt;0}} )。  </li>
<li>Sigmoid：( \sigma(x)&#x3D;1&#x2F;(1+e^{-x}) )。优点：输出在 (0,1)，历史悠久。缺点：在大|x|时饱和导致梯度消失，数值不稳。<br>导数： ( \sigma’(x)&#x3D;\sigma(x)(1-\sigma(x)) )。  </li>
<li>Tanh：类似 sigmoid，中心化在 0（输出在 (−1,1)），但仍有饱和问题。</li>
</ul>
</li>
</ul>
<ol start="3">
<li>反向传播</li>
</ol>
<ul>
<li>标量链式法则（示意）：<br>$$<br>\frac{dL}{dx} &#x3D; \frac{dL}{dy}\cdot\frac{dy}{dx}.<br>$$</li>
</ul>
<div align="center"><img src="/img/image-104.png" alt="图示" width="80%" /></div>

<ol start="4">
<li>小结<br>Fully‑connected networks &#x3D; 线性层组合 + 非线性激活。Backprop &#x3D; 在计算图上对链式法则的递归应用，通过每个节点的局部导数与传入的上游梯度相乘，得到对输入与参数的梯度。实现时用 forward&#x2F;backward API、缓存中间量、用隐式矩阵运算避免显式雅可比。</li>
</ol>
<h1 id="第五课-·-CNN图像分类"><a href="#第五课-·-CNN图像分类" class="headerlink" title="第五课 · CNN图像分类"></a>第五课 · CNN图像分类</h1><h2 id="本节课程核心总结-3"><a href="#本节课程核心总结-3" class="headerlink" title="本节课程核心总结"></a>本节课程核心总结</h2><ul>
<li>本讲介绍卷积神经网络（CNN）在图像分类中的原理与工程要点：卷积算子（局部感受野、权值共享）、填充（padding）、步幅（stride）、池化（pooling）、感受野（receptive field）</li>
<li>CNN 能以较少参数并利用图像的局部与平移结构高效学习层次化特征，是现代视觉任务的基石。</li>
</ul>
<h2 id="笔记内容-3"><a href="#笔记内容-3" class="headerlink" title="笔记内容"></a>笔记内容</h2><ol>
<li>卷积神经网络CNN的结构</li>
</ol>
<div align="center"><img src="/img/image-106.png" alt="图示" width="80%" /></div>

<ol start="2">
<li>卷积操作的细节与滤波器</li>
</ol>
<div align="center"><img src="/img/image-107.png" alt="图示" width="80%" /></div>
- 输入与权重的常见张量形状：
  - 输入（单张图像）： $$X \in \mathbb{R}^{C_{\text{in}}\times H \times W}$$
  - 卷积核（滤波器）： $$W \in \mathbb{R}^{C_{\text{out}}\times C_{\text{in}}\times K_h \times K_w}$$
  - Bias： $$b \in \mathbb{R}^{C_{\text{out}}}$$

<ul>
<li>输出尺寸（高度 &#x2F; 宽度）通用公式（向下取整）：<br>$$<br>H_{\text{out}}&#x3D;\left\lfloor\frac{H_{\text{in}} - K_h + 2P_h}{S}\right\rfloor + 1,\qquad<br>W_{\text{out}}&#x3D;\left\lfloor\frac{W_{\text{in}} - K_w + 2P_w}{S}\right\rfloor + 1.<br>$$<br>常见“same” padding 的设置：若 $$K$$ 为奇数，取 $$P&#x3D;(K-1)&#x2F;2$$ 可使输出与输入空间尺寸相同（当 $$S&#x3D;1$$ 时）。<ul>
<li><p>每个滤波器（filter）尺寸为 Cin × KH × KW，滤波器在空间上滑动并在每个空间位置做点积得到 activation map 的一个通道。  </p>
</li>
<li><p>多个滤波器产生多通道输出，带有对应偏置（bias）。  </p>
</li>
<li><p>输出尺寸（高度&#x2F;宽度）计算公式：<br>[<br>H’&#x3D;\frac{H - K + 2P}{S} + 1,\qquad W’&#x3D;\frac{W - K + 2P}{S} + 1.<br>]<br>常见设置：K&#x3D;3,P&#x3D;1,S&#x3D;1（“same” padding）；下采样可用 stride&gt;1 或 pooling。</p>
</li>
</ul>
</li>
</ul>
<h3 id="3-池化层（Pooling）"><a href="#3-池化层（Pooling）" class="headerlink" title="3) 池化层（Pooling）"></a>3) 池化层（Pooling）</h3><ul>
<li>Pooling 是无参数操作，常见 max pooling 或 average pooling。</li>
<li>输出尺寸（无 padding 的常见形式）：<br>$$<br>H_{\text{out}}&#x3D;\left\lfloor\frac{H_{\text{in}} - F}{S}\right\rfloor + 1,\qquad<br>W_{\text{out}}&#x3D;\left\lfloor\frac{W_{\text{in}} - F}{S}\right\rfloor + 1,<br>$$<br>其中 $$F$$ 为池化核大小，$$S$$ 为步幅（常见 $$F&#x3D;2,S&#x3D;2$$ → 下采样 2 倍）。</li>
</ul>
<div align="center"><img src="/img/image-108.png" alt="图示" width="80%" /></div>
### 4) 感受野（Receptive Field）
- 概念：网络中某个输出单元在输入图像上“看到”的像素范围（窗口大小）。
- 简单情形（相同 kernel K、stride=1）：经过 $$L$$ 个连续 $$K\times K$$ 卷积层的感受野为
  $$
  \text{RF} = 1 + L\cdot (K-1).
  $$

<div align="center"><img src="/img/image-21.png" alt="图示" width="80%" /></div>
单层卷积的每个输出像素对应输入的 K×K 区域（感受野）。多层叠加能逐步扩大感受野，使高层单元能“看到”输入更大范围。感受野决定高层能捕捉多大语义上下文，设计网络需兼顾分辨本地细节与宏观上下文。

<h3 id="5-卷积的两大优点（与-FC-比较）"><a href="#5-卷积的两大优点（与-FC-比较）" class="headerlink" title="5) 卷积的两大优点（与 FC 比较）"></a>5) 卷积的两大优点（与 FC 比较）</h3><ul>
<li>局部连接（local connectivity）：参数只连接局部感受野 → 参数大幅减少。</li>
<li>权重共享（parameter sharing）：同一 filter 在空间上复用 → 学到的特征具有全局复用性（可检测同一模式不同位置）。</li>
<li>结果：保留空间结构，学习到从边缘→纹理→物体部件的层次特征。</li>
</ul>
<hr>
<h2 id="深度学习内容回顾"><a href="#深度学习内容回顾" class="headerlink" title="深度学习内容回顾"></a>深度学习内容回顾</h2><ol>
<li>将你的问题编码为 y &#x3D; f(x)，其中 x 和 y 是数字网格。获取一组 (x, y) 对的数据集 <img src="/img/image-23.png" alt="alt text" style="width:60%;vertical-align:middle;" /></li>
<li>定义一个损失函数 L(ypred, ygt)，用一个数值来衡量预测的正确性 <img src="/img/image-24.png" alt="alt text" style="width:40%;vertical-align:middle;" /></li>
<li>定义一个计算图，使用可学习的权重 w 从 x 预测 y <img src="/img/image-25.png" alt="alt text" style="width:60%;vertical-align:middle;" /></li>
<li>使用反向传播计算梯度 dL&#x2F;dw</li>
<li>使用优化算法找到最小化损失的 w</li>
</ol>
<hr>
<h1 id="第六课-·-CNN网络训练与架构"><a href="#第六课-·-CNN网络训练与架构" class="headerlink" title="第六课 · CNN网络训练与架构"></a>第六课 · CNN网络训练与架构</h1><h2 id="CNN网络训练与架构"><a href="#CNN网络训练与架构" class="headerlink" title="CNN网络训练与架构"></a>CNN网络训练与架构</h2><h2 id="1-如何构建卷积神经网络"><a href="#1-如何构建卷积神经网络" class="headerlink" title="1. 如何构建卷积神经网络"></a>1. 如何构建卷积神经网络</h2><div align="center"><img src="/img/image-27.png" alt="图示" width="80%" /></div>

<h3 id="一、CNN的重要组成部分"><a href="#一、CNN的重要组成部分" class="headerlink" title="一、CNN的重要组成部分"></a>一、CNN的重要组成部分</h3><h4 id="1-卷积层："><a href="#1-卷积层：" class="headerlink" title="1. 卷积层："></a>1. 卷积层：<div align="center"><img src="/img/image-29.png" alt="图示" width="80%" /></div></h4><h4 id="2-池化层："><a href="#2-池化层：" class="headerlink" title="2. 池化层："></a>2. 池化层：<div align="center"><img src="/img/image-30.png" alt="图示" width="80%" /></div></h4><h4 id="3-全连接层："><a href="#3-全连接层：" class="headerlink" title="3. 全连接层："></a>3. 全连接层：<div align="center"><img src="/img/image-31.png" alt="图示" width="80%" /></div></h4><h4 id="4-归一化层："><a href="#4-归一化层：" class="headerlink" title="4. 归一化层："></a>4. 归一化层：</h4><div align="center"><img src="/img/image-32.png" alt="图示" width="80%" /></div>
<div align="center"><img src="/img/image-34.png" alt="图示" width="80%" /></div>

<h4 id="5-Dropout正则化："><a href="#5-Dropout正则化：" class="headerlink" title="5. Dropout正则化："></a>5. Dropout正则化：</h4><div align="center"><img src="/img/image-35.png" alt="图示" width="80%" /></div>
训练时使用蒙版
测试时使用完整网络输出

<div align="center"><img src="/img/image-36.png" alt="图示" width="80%" /></div>

<h3 id="二、激活函数："><a href="#二、激活函数：" class="headerlink" title="二、激活函数："></a>二、激活函数：</h3><ol>
<li>sigmoid将数据压缩在[0,1]范围内<br>大的正值或负值会“消灭”梯度。多层 sigmoid 实际上会导致梯度越来越小</li>
</ol>
<div align="center"><img src="/img/image-37.png" alt="alt text" style="width:60%;" /></div>
<div align="center"><img src="/img/image-38.png" alt="alt text" style="width:60%;" /></div>
2. 修正线性单元：x<0时为0

<div align="center"><img src="/img/image-39.png" alt="alt text" style="width:60%;" /></div>

<ol>
<li>GELU（高斯误差线性单元）<br>比 ReLU 计算成本更高－大负值仍可能使梯度接近 0</li>
</ol>
<div align="center"><img src="/img/image-40.png" alt="alt text" style="width:60%;" /></div>
这些激活函数通常放在线性算子（前馈/线性层、卷积层等）之后。

<h3 id="三、CNN架构"><a href="#三、CNN架构" class="headerlink" title="三、CNN架构"></a>三、CNN架构</h3><p>因为三个 3x3 卷积（步长为 1）层的堆叠具有与一个 7x7 卷积层相同的有效感受野，更深，参数更少，所以尽可能选择更小的滤波器</p>
<div align="center"><img src="/img/image-41.png" alt="图示" width="80%" /></div>

<ol>
<li>ResNet 要解决的核心问题（为什么提出 ResNet？）</li>
</ol>
<ul>
<li>目标：训练更深的卷积网络以提升表示能力与最终精度。经验与理论显示，更深网络（在容量上）可以拟合更复杂函数。</li>
<li>观察到的问题（Degradation）：当简单把层数加深时（在没有其它结构变化的“plain”网络上），训练误差反而变差（即训练误差上升），说明更深网络变得更难优化，而不是单纯的过拟合问题。<ul>
<li>直观：深层网络难以训练，梯度传播困难、优化路径复杂。</li>
</ul>
</li>
<li>因此需要一个结构，使得“加层”不会使网络更难以至少保持性能（即能让更深网络至少不比浅网更差），并且更容易优化到好的解。</li>
</ul>
<ol start="2">
<li>核心思想：残差学习（Residual Learning）</li>
</ol>
<ul>
<li>基本重写目标函数的方式：不要直接去拟合期望的映射 $$H(x)$$，而是让网络去拟合残差函数 $$F(x) :&#x3D; H(x) - x$$。于是期望的映射由下式给出：<br>$$<br>H(x) &#x3D; F(x) + x.<br>$$</li>
<li>实现方式：在若干连续的层（一个 residual block）中，将普通的卷积串（带 BN、ReLU 等）视作 $$F(x)$$，再把输入 $$x$$ 通过一个“shortcut”（跳跃连接，通常是恒等映射或 1×1 卷积）加回去。</li>
<li>这样，如果最优的 $$H(x)$$ 恰好是恒等映射（或接近恒等），网络只需把 $$F(x)$$ 学成零，容易实现；而普通网络必须学出复杂的恒等映射参数，反而更难。</li>
</ul>
<p>以数学形式表示一个残差块（简单情形）：<br>$$<br>y &#x3D; F(x;W) + x,<br>$$<br>其中 $$F(x;W)$$ 是 residual branch（例如两个 3×3 conv + BN + ReLU 串联），$$x$$ 是 shortcut branch（恒等或线性变换）。</p>
<div align="center"><img src="/img/image-42.png" alt="图示" width="80%" /></div>

<h3 id="四、初始化神经网络层的权重"><a href="#四、初始化神经网络层的权重" class="headerlink" title="四、初始化神经网络层的权重"></a>四、初始化神经网络层的权重</h3><ol>
<li>初始化神经网络层的权重为什么重要？<ul>
<li><p>详细说明：</p>
<ul>
<li>初始化设置直接影响前向激活与反向梯度的尺度传播（若不合适会导致“梯度消失&#x2F;爆炸”或激活逐层衰减&#x2F;发散）。  </li>
<li>因此需要一种合理的初始化策略来在网络深度方向尽量保持激活&#x2F;梯度尺度稳定（或至少不呈指数级衰减&#x2F;增长）。</li>
</ul>
</li>
<li><p>若初始权重值太小，前向传播时激活会逐层衰减，深层激活将接近 0（dead network）。</p>
<ul>
<li>这页通常用示意图或激活直方图说明：越靠近深层的激活越窄、逼近零。  </li>
<li>导致的问题不仅是前向激活消失，反向时由于许多 ReLU 的梯度为 0（死亡 ReLU），梯度无法穿透网络，训练停滞。</li>
</ul>
</li>
</ul>
</li>
</ol>
  <div align="center"><img src="/img/image-43.png" alt="图示" width="80%" /></div>

<ul>
<li>若初始权重值太大，前向激活会逐层放大，可能产生数值溢出或梯度爆炸，训练不稳定。<ul>
<li>对非线性（sigmoid&#x2F;tanh）函数：大输入会进入饱和区，使导数接近 0 → 反而导致梯度消失。  </li>
<li>对 ReLU：大输入不会饱和（正区间线性），但激活增大会导致梯度量级变大，训练震荡甚至数值溢出。  </li>
<li>因此不同激活对初始化 scale 的“容忍度”不同（ReLU 更能容忍正区域但总体仍需控制方差）。</li>
</ul>
</li>
</ul>
  <div align="center"><img src="/img/image-44.png" alt="图示" width="80%" /></div>
2. 如何初始化神经网络层的权重
- 需要为不同激活函数/层尺寸选择合适的初始化策略
- 详细说明：
  - 目标：在随机初始化时，使得每一层的激活方差在“前向传播”方向上尽量保持不变（或不呈指数增长/下降）。同理希望反向传播的梯度方差也不过度衰减/增大。  
  - He/Kaiming注意到 ReLU 会把一半输出变为零，因此在保持方差时要有一个因子 2 的修正（相对于不含 ReLU 的情况）。
  - 下页给出具体公式（Kaiming / MSRA 初始化）。

<ul>
<li><p>给出 He&#x2F;Kaiming 初始化公式（标准差 &#x3D; $$\sqrt{2&#x2F;\text{fan_in}}$$），并说明这是为 ReLU 修正得到的“刚好合适”的尺度。</p>
</li>
<li><p>初始化要做的是“使得激活与梯度在层与层之间不呈指数级衰减或爆炸”；对于 ReLU 家族，He&#x2F;Kaiming 初始化（std &#x3D; $$\sqrt{2&#x2F;\text{fan_in}}$$ 或均匀版 $$a&#x3D;\sqrt{6&#x2F;\text{fan_in}}$$）是标准且有效的选择；在实践中结合 BatchNorm、合理的学习率与监控激活直方图就能稳定训练深网络。</p>
</li>
</ul>
<h2 id="2-如何训练卷积神经网络"><a href="#2-如何训练卷积神经网络" class="headerlink" title="2. 如何训练卷积神经网络"></a>2. 如何训练卷积神经网络</h2><div align="center"><img src="/img/image-28.png" alt="图示" width="80%" /></div>

<h3 id="1-数据预处理"><a href="#1-数据预处理" class="headerlink" title="1. 数据预处理"></a>1. 数据预处理</h3><ul>
<li>图像归一化简述：对每个通道进行中心化和缩放——减去每个通道的均值并除以每个通道的标准差（每个通道的统计信息 &#x3D; 3 个数字）——需要预先计算每个像素通道的均值和标准差（根据你的数据集）</li>
</ul>
<h3 id="2-数据增强"><a href="#2-数据增强" class="headerlink" title="2. 数据增强"></a>2. 数据增强</h3><ul>
<li>训练时加入一些随机随机性</li>
<li>测试时平均随机性</li>
<li>例子：<div align="center"><img src="/img/image-45.png" alt="图示" width="80%" /></div><div align="center"><img src="/img/image-46.png" alt="图示" width="80%" /></div></li>
</ul>
<h3 id="3-迁移学习"><a href="#3-迁移学习" class="headerlink" title="3. 迁移学习"></a>3. 迁移学习</h3><ul>
<li>CNN在小数据集从头训练会出现严重过拟合或训练不收敛的情况，为了解决在没有大量数据的情况下如何训练卷积神经网络的情况，提出除了迁移学习，先在大数据集（如 ImageNet）上预训练一个强模型，再把学到的表示迁移到你的小数据集上（作为特征提取器，或微调整个网络）<div align="center"><img src="/img/image-47.png" alt="图示" width="80%" /></div></li>
<li>如果数据量很少：<ul>
<li>数据集相似的情况下，只训练最后的线性层</li>
<li>数据集不相似的情况下，换模型或收集更多数据／用其它策略</li>
</ul>
</li>
<li>如果数据量很大：<ul>
<li>数据集相似的情况下，微调全部层</li>
<li>数据集不相似的情况下，微调或从头训练</li>
</ul>
<div align="center"><img src="/img/image-48.png" alt="图示" width="80%" /></div></li>
</ul>
<h3 id="4-选择超参数"><a href="#4-选择超参数" class="headerlink" title="4. 选择超参数"></a>4. 选择超参数</h3><ul>
<li>选择超参数的步骤<ul>
<li>步骤 1：检查初始损失 </li>
<li>步骤 2：在小样本上过拟合 </li>
<li>步骤 3：找到使损失降低的学习率</li>
<li>第4步：超参数粗略网格，训练约1-5个周期 </li>
<li>第5步：细化网格，延长训练时间 </li>
<li>第6步：查看损失和准确率曲线</li>
<li>第7步：转到步骤5</li>
<li>随机搜索优于 grid 搜索</li>
</ul>
</li>
<li>模型训练曲线的几种情况<ul>
<li>准确率仍在上升，需要更长时间训练<div align="center"><img src="/img/image-49.png" alt="图示" width="80%" /></div></li>
<li>训练集和验证集差距很大意味着过拟合！增加正则化或获取更多数据<div align="center"><img src="/img/image-50.png" alt="图示" width="80%" /></div></li>
<li>训练集和验证集之间没有差距意味着欠拟合：训练时间更长，可能可以使用更大的模型<div align="center"><img src="/img/image-51.png" alt="图示" width="80%" /></div></li>
</ul>
</li>
</ul>
<h1 id="第七课-·-循环神经网络（RNN）"><a href="#第七课-·-循环神经网络（RNN）" class="headerlink" title="第七课 · 循环神经网络（RNN）"></a>第七课 · 循环神经网络（RNN）</h1><h2 id="一、从普通（前馈）神经网络到-RNN：基本概念与公式"><a href="#一、从普通（前馈）神经网络到-RNN：基本概念与公式" class="headerlink" title="一、从普通（前馈）神经网络到 RNN：基本概念与公式"></a>一、从普通（前馈）神经网络到 RNN：基本概念与公式</h2><p>前馈网络：输入固定大小、每个输入独立处理。RNN：处理任意长度序列，通过“隐藏状态”把时间联系起来。<br>RNN的核心思想：RNN 有一个“内部状态”，会在处理序列时不断更新<div align="center"><img src="/img/image-53.png" alt="图示" width="80%" /></div></p>
<p>Vanilla（“普通”）RNN 的常用前向公式：</p>
<ul>
<li>隐藏态更新（每个时间步 t）：<br>$$<br>h_t &#x3D; \phi(W_{hx} x_t + W_{hh} h_{t-1} + b_h)<br>$$<br>其中<ul>
<li>$x_t$：当前输入（向量）；</li>
<li>$h_{t}$：当前隐藏状态（“记忆”）；</li>
<li>$W_{hx}$：输入到隐藏的权重矩阵；</li>
<li>$W_{hh}$：隐藏到隐藏的权重矩阵（跨时间共享）；</li>
<li>$\phi$：非线性（常见为 $\tanh$ 或 ReLU）；</li>
</ul>
</li>
</ul>
<div align="center"><img src="/img/image-61.png" alt="图示" width="80%" /></div>
- 输出（如果有输出）：

<p>$$<br>y_t &#x3D; g(W_{yh} h_t + b_y)<br>$$</p>
<p>其中 $g$ 通常是 softmax（分类）或线性回归头等。</p>
<p>要点：</p>
<ul>
<li>同一套参数 $W_{hx}, W_{hh}, W_{yh}$ 在每个时间步重复使用（权重共享）。</li>
<li>RNN 的隐藏状态把“历史信息”压缩到一个向量里：这是模型学习如何总结过去。</li>
</ul>
<p>形象比喻：想象一个流水线工人每天接到一个物品（$x_t$），用同一套工具（同一套权重）根据手头的“记事本”（$h_{t-1}$）更新便签（$h_t$），然后决定是否把结果放到成品（$y_t$）。</p>
<div align="center"><img src="/img/image-62.png" alt="图示" width="80%" /></div>


<h2 id="二、展开（Unroll）与计算图；任务类型"><a href="#二、展开（Unroll）与计算图；任务类型" class="headerlink" title="二、展开（Unroll）与计算图；任务类型"></a>二、展开（Unroll）与计算图；任务类型</h2><ul>
<li>“展开”就是把时间维度展开成一个深网络：$h_0 \to h_1 \to \cdots \to h_T$，每一层都用同一套参数。</li>
<li>计算图展开后，我们可以用普通的反向传播来求梯度（这就是 BPTT）。</li>
</ul>
<p>常见序列任务类型：</p>
<ul>
<li>Many→Many：序列→序列（例如逐词翻译、语音识别）；</li>
</ul>
<div align="center"><img src="/img/image-54.png" alt="图示" width="80%" /></div>
- Many→One：序列→一个输出（例如情感分类）；<div align="center"><img src="/img/image-55.png" alt="图示" width="80%" /></div>
- One→Many：单一输入生成序列（例如图像→标题）；<div align="center"><img src="/img/image-56.png" alt="图示" width="80%" /></div>

<p>举例：图像描述（one→many），把图像向量 $v$ 注入到 RNN（作为初始隐藏态或作为额外输入）；然后 RNN 逐步生成每个单词。</p>
<h2 id="三、训练：反向传播通过时间（BPTT）与截断-BPTT"><a href="#三、训练：反向传播通过时间（BPTT）与截断-BPTT" class="headerlink" title="三、训练：反向传播通过时间（BPTT）与截断 BPTT"></a>三、训练：反向传播通过时间（BPTT）与截断 BPTT</h2><p>标准 BPTT：</p>
<ul>
<li>对整个序列做一次前向传播，计算损失（例如对每个时间步求交叉熵求和）；</li>
<li>将计算图在时间维度上展开，做一次反向传播，从最后的时间步逆推到最前；</li>
<li>这个过程在数学上与在一个非常深的前馈网络上做反向传播是等价的。</li>
</ul>
<div align="center"><img src="/img/image-57.png" alt="图示" width="80%" /></div>


<p>截断 BPTT（Truncated BPTT）：</p>
<ul>
<li>若序列很长，直接对全部时间步 BPTT 昂贵且梯度容易退化；</li>
<li>常用方法：把序列切分为长度为 $K$ 的小片段。前向时隐藏状态从一段传到下一段，但反向时只回溯 $K$ 步（每段内部反向传播到段头）。</li>
<li>优点：节省内存&#x2F;计算；可在线流式训练（carry 隐藏态但只反向有限步）。</li>
<li>注意：截断会丢失很远时间步的精确梯度（是近似训练）。</li>
</ul>
<div align="center"><img src="/img/image-58.png" alt="图示" width="80%" /></div>
直观比喻：BPTT 是把时间轴上所有影响都追溯一遍；截断就是只回头看最近几天的影响而忽略很久以前的细节（折中）。



<h2 id="六、示例：字符级语言模型（Char-RNN）"><a href="#六、示例：字符级语言模型（Char-RNN）" class="headerlink" title="六、示例：字符级语言模型（Char-RNN）"></a>六、示例：字符级语言模型（Char-RNN）</h2><p>核心思想：</p>
<ul>
<li>每个字符是来自词表（vocabulary）的一维 one-hot 向量；</li>
<li>通常先将 one-hot 矢量映射到 embedding 空间（低维稠密向量）；</li>
<li>RNN 根据当前嵌入和前一隐藏态更新隐藏态，输出一个对下一个字符的概率分布（softmax）。</li>
</ul>
<div align="center"><img src="/img/image-59.png" alt="图示" width="80%" /></div>


<p>训练与采样：</p>
<ul>
<li>训练时常用“teacher forcing”：在时间步 $t$ 输入真实前一个字符（而非模型生成），损失通常为每步的交叉熵并求和；</li>
<li>生成（采样）时：从 softmax 概率中采样或取 argmax，得到下一个字符并将其作为下一步输入；</li>
<li>softmax：<br>$$<br>\text{softmax}(z)_i &#x3D; \frac{e^{z_i}}{\sum_j e^{z_j}}.<br>$$</li>
</ul>
<p>Karpathy 的 min-char-rnn（经典小实现）展示了：只靠一个很小的 RNN、并且“train more”，可以学到很多语言模式（例如缩进、括号匹配、字符串结束等），并能产生可读的文本片段。</p>
<p>直观：训练就是教会模型“下一步最可能是什么字符”；采样时模型像写字的人一样，一步步自己决定下一个字符并继续写下去。</p>
<h2 id="七、RNN-的优点与缺点（幻灯片中也有总结）"><a href="#七、RNN-的优点与缺点（幻灯片中也有总结）" class="headerlink" title="七、RNN 的优点与缺点（幻灯片中也有总结）"></a>七、RNN 的优点与缺点（幻灯片中也有总结）</h2><p>优点：</p>
<ul>
<li>可处理任意长度序列（理论上无限上下文）；</li>
<li>参数量不依赖于序列长度（权重共享）；</li>
<li>能在时间维度上复用相同的处理方式（模型有「时间上的对称性」）。</li>
</ul>
<p>缺点：</p>
<ul>
<li>逐步（sequential）计算较慢，不易并行化（相比 Transformer 并行能力弱）；</li>
<li>在实践中难以捕捉非常长距离的依赖（vanishing &#x2F; exploding gradients）；</li>
<li>实际训练往往需要技巧（截断 BPTT、梯度裁剪、门控单元等）。</li>
</ul>
<h2 id="八、LSTM（Long-Short-Term-Memory）：结构、公式与直观解释"><a href="#八、LSTM（Long-Short-Term-Memory）：结构、公式与直观解释" class="headerlink" title="八、LSTM（Long Short-Term Memory）：结构、公式与直观解释"></a>八、LSTM（Long Short-Term Memory）：结构、公式与直观解释</h2><p>LSTM 是为了解决“长期依赖难学”的难题而设计的。关键思路是引入一个显式的“细胞状态” $c_t$，并用门控机制精确控制信息的写入、遗忘与读取，从而使梯度能够沿着 $c_t$ 更平稳地流动。</p>
<p>标准 LSTM 公式：</p>
<div align="center"><img src="/img/image-60.png" alt="图示" width="80%" /></div>
其中⊙ 表示逐元素乘，σ是 sigmoid 激活（输出在 (0,1)）。

<p>直观比喻（非常重要）：</p>
<ul>
<li>将 $c_t$ 想象为一条输送带（conveyor belt）——信息沿着带传递；</li>
<li>门（$i_t, f_t, o_t$）像控制“水闸&#x2F;阀门”：决定向带上写多少、把带上已有的东西忘掉多少、并如何将带上的信息展示出来；</li>
<li>如果在很多时间步上 $f_t \approx 1$ 且 $i_t \approx 0$，那一维的 $c$ 值就几乎不变，从而能无阻碍地跨越很长时间（梯度也能直接回传），避免消失。</li>
</ul>
<p>为什么 LSTM 帮助梯度流：</p>
<ul>
<li>注意到 $c_t$ 对 $c_{t-1}$ 的导数是 $f_t$（逐元素），而不是一个大矩阵乘法。因此只要门能学到 $f_t \approx 1$，梯度就不会被矩阵连续相乘压缩掉；LSTM 为保存信息提供了「可学习的捷径」。</li>
</ul>
<h1 id="第八课-·-Attention机制与Transformers"><a href="#第八课-·-Attention机制与Transformers" class="headerlink" title="第八课 · Attention机制与Transformers"></a>第八课 · Attention机制与Transformers</h1><h2 id="本课核心内容"><a href="#本课核心内容" class="headerlink" title="本课核心内容"></a>本课核心内容</h2><p><strong>Attention</strong>：一个作用于向量集合的新原语<br><strong>Transformers</strong>：一种在各处使用注意力机制的神经网络架构</p>
<ol>
<li>Attention（逐步对齐）</li>
</ol>
<ul>
<li>要点：每步计算对齐分数并归一化，产生时刻特定上下文 ct 用于解码。  </li>
<li>核心公式：对齐分数 $e_{t,i}&#x3D;f_{\text{att}}(s_{t-1},h_i)$，权重 $a_{t,i}&#x3D;\mathrm{softmax}<em>i(e</em>{t,i})$，上下文 $c_t&#x3D;\sum_i a_{t,i}h_i$。</li>
</ul>
<ol>
<li>Query &#x2F; Key &#x2F; Value 抽象与缩放点积注意力</li>
</ol>
<ul>
<li>要点：把注意力统一为 Q&#x2F;K&#x2F;V 形式，使用缩放避免 softmax 饱和。  </li>
<li>核心公式：$$E&#x3D;\frac{QK^\top}{\sqrt{D}},\qquad A&#x3D;\mathrm{softmax}(E),\qquad Y&#x3D;AV.$$</li>
</ul>
<ol>
<li>Self‑Attention 与位置编码、掩码</li>
</ol>
<ul>
<li>要点：自注意力在集合上运算（置换等变），需位置编码恢复序列顺序；自回归用 mask 阻断“看未来”。  </li>
<li>核心公式（位置编码加法）：$X_{\text{in}}&#x3D;E_{\text{token}}+E_{\text{pos}}$；掩码实现为在相似度矩阵替换不允许项为 $-\infty$：$E_{ij}\leftarrow -\infty$（若禁止）。</li>
</ul>
<ol>
<li>多头注意力与矩阵化实现</li>
</ol>
<ul>
<li>要点：并行 H 个 head，每头维度 $D_H$，最后 concat+线性投影。  </li>
<li>核心公式：头级 Q&#x2F;K&#x2F;V：$Q_h&#x3D;XW_Q^{(h)},;K_h&#x3D;XW_K^{(h)},;V_h&#x3D;XW_V^{(h)}$；合并输出：$$O&#x3D;\mathrm{concat}(Y_1,\dots,Y_H)W_O.$$</li>
</ul>
<ol>
<li>计算&#x2F;内存复杂度与优化</li>
</ol>
<ul>
<li>要点：注意力随序列长度 $N$ 的时间与内存为 $O(N^2)$；如 FlashAttention 可在不存全矩阵下精确计算以降内存。  </li>
<li>核心标注：复杂度 $&#x3D;\mathcal{O}(H N^2 D_H)$，FlashAttention 使内存从 $O(N^2)$ 实际降低但算量仍类似。</li>
</ul>
<ol>
<li>Transformer Block 结构与训练细节</li>
</ol>
<ul>
<li>要点：每块包含（多头）自注意力、残差、LayerNorm、逐位 MLP（Feed‑Forward）。经典 FFN: $D\to 4D\to D$。  </li>
<li>核心公式：LayerNorm 计算（对向量维度）：$$\mu&#x3D;\frac{1}{D}\sum_i h_i,\ \sigma&#x3D;\sqrt{\frac{1}{D}\sum_i(h_i-\mu)^2},\ z&#x3D;\frac{h-\mu}{\sigma},\ y&#x3D;\gamma\odot z+\beta.$$  </li>
<li>FFN：$ \mathrm{FFN}(x)&#x3D;W_2,\sigma(W_1 x)$（经典），或 SwiGLU 形式 $Y&#x3D;(XW_1)\odot\mathrm{GELU}(XW_2)W_3$。</li>
</ul>
<ol>
<li>可扩展性技巧（Pre‑Norm、RMSNorm、MoE）</li>
</ol>
<ul>
<li>要点：Pre‑Norm 将归一化移入残差内，RMSNorm 作为轻量替代；MoE 用专家网络扩大参数量但仅对部分 token 计算。  </li>
<li>核心公式（RMSNorm）：$$\mathrm{RMS}(x)&#x3D;\sqrt{\tfrac{1}{D}\sum_i x_i^2+\varepsilon},\quad y_i&#x3D;\frac{x_i}{\mathrm{RMS}(x)}\gamma_i.$$</li>
</ul>
<h3 id="Transformer架构"><a href="#Transformer架构" class="headerlink" title="Transformer架构"></a>Transformer架构</h3><div align="center"><img src="/img/image-65.png" alt="图示" width="80%" /></div>

<p>一、为什么需要 Attention  </p>
<ul>
<li>场景举例（直观）：<ul>
<li>任务：英文句子 → 意大利文句子（机器翻译）。常见 Seq2Seq 思路：用一个 RNN（或 LSTM&#x2F;GRU）把整句编码成一个固定向量 c，再由另一个 RNN 解码生成目标句子（译文）。</li>
</ul>
</li>
<li>问题（瓶颈）：<ul>
<li>如果输入很长（比如 T &#x3D; 1000），把所有信息压成单个固定向量 c 会造成信息丢失或难学（长期依赖被“挤出”）。</li>
<li>直观比喻：把一篇长篇文章浓缩成一句话摘要去翻译每个词——很多细节会丢失。</li>
</ul>
</li>
<li>解决思路（关键句）：<ul>
<li>在每个解码步 t，都“回头看”编码器产生的所有隐藏态 h_1..h_T，根据当前解码器状态自动决定“关注（attend）”哪个输入位置，从这些位置加权得到上下文向量 c_t。这样每一步都有一个动态的上下文，而不是单一的 c。</li>
</ul>
</li>
</ul>
<div align="center"><img src="/img/image-63.png" alt="图示" width="80%" /></div>

<p>二、Attention：</p>
<ul>
<li>核心步骤（针对 Seq2Seq 的 attention）：<ol>
<li>对于解码器在时刻 t，已有前一时刻的解码器状态 $s_{t-1}$（query），以及编码器在每个输入位置 i 的隐藏向量 $h_i$（data vectors）。</li>
<li>计算对每个 i 的“对齐分数（alignment score）”：<br>$$e_{t,i} &#x3D; f_\text{att}(s_{t-1}, h_i).$$<br>一个常用的实现（加性 attention）是：<br>$$e_{t,i} &#x3D; v_a^\top \tanh(W_a s_{t-1} + U_a h_i)$$<br>其中 $W_a,U_a,v_a$ 是可以学习的参数向量&#x2F;矩阵。</li>
<li>把分数做 softmax 归一化得到注意力权重：<br>$$a_{t,i} &#x3D; \frac{\exp(e_{t,i})}{\sum_j \exp(e_{t,j})},\quad 0\le a_{t,i}\le1,\ \sum_i a_{t,i}&#x3D;1.$$</li>
<li>计算上下文向量（加权和）：<br>$$c_t &#x3D; \sum_i a_{t,i} h_i.$$</li>
<li>把 $c_t$ 和解码器前状态 $s_{t-1}$（或前一步输出）一起输入解码网络得到新的解码状态 $s_t$ 并生成输出 $y_t$。</li>
</ol>
</li>
<li>直观比喻（书架与检索）：<ul>
<li>把编码的每个 $h_i$ 想成书架上一排条目（每个条目是某句话或短语）；$s_{t-1}$ 是你当前的问题（query）。你计算每本书的“相关度”（alignment score），把注意力概率当成图书借阅次数的分布，然后把最相关书的内容按权重合并成一个简短摘要 $c_t$ 给解码器看——这样解码器“知道”该关注输入里的哪些部分来生成下一个词。</li>
</ul>
</li>
<li>优点：<ul>
<li>每一步都能动态选取输入的不同部分（没有把全部信息压成单向量）。</li>
<li>注意力权重易于可视化（可解释性）。</li>
<li>全过程是可微的，可以 end-to-end 学（没有额外的监督来告诉模型哪几个位置重要）。<div align="center"><img src="/img/image-64.png" alt="图示" width="80%" /></div></li>
</ul>
</li>
</ul>
<p>三、把 Attention 抽象成 Query &#x2F; Key &#x2F; Value（QKV）算子</p>
<ul>
<li>更通用的描述：Attention 可以看成一个算子，输入是一组“数据向量”（X）和一组“查询向量”（Q），输出是一组向量（Y）。实现分三个角色：<ul>
<li>Queries（查询）：用于检索信息（Q）。</li>
<li>Keys（键）：每个数据向量对应一个 key（K），表示该条目的“标签&#x2F;索引”。</li>
<li>Values（值）：每个数据向量对应一个 value（V），是最终要混合的信息。</li>
</ul>
</li>
<li>算子形式（一个 query 对所有 keys 计算相似度，得到权重，再对 values 做加权和）：<ul>
<li>对单个 query q 与每个数据向量 x_i：<ol>
<li>相似度： $e_i &#x3D; \text{sim}(q, x_i)$</li>
<li>归一化： $a_i &#x3D; \text{sof</li>
<li>tmax}(e)$</li>
<li>输出： $y &#x3D; \sum_i a_i x_i$</li>
</ol>
</li>
</ul>
</li>
<li>把 key 与 value 分开是因为我们希望用某种方式匹配（key）但把不同的信息（value）混合进输出。</li>
</ul>
<hr>
<p>四、Scaled Dot-Product Attention（矩阵形式与维度）<br>这是 Transformer（Vaswani et al., 2017）里广泛使用的形式。</p>
<ul>
<li>设：<ul>
<li>Queries: $Q \in \mathbb{R}^{N_Q\times d_k}$</li>
<li>Keys: $K \in \mathbb{R}^{N_K\times d_k}$</li>
<li>Values: $V \in \mathbb{R}^{N_K\times d_v}$</li>
</ul>
</li>
<li>计算：<ol>
<li>相似度矩阵（点积）：<br>$$E &#x3D; \frac{Q K^\top}{\sqrt{d_k}}\quad\text{（形状 }N_Q\times N_K\text{）}.$$<br>注意：除以 $\sqrt{d_k}$ 是“scale”步骤，能防止点积数值过大导致 softmax 梯度消失&#x2F;饱和（下面会解释）。</li>
<li>注意力权重：<br>$$A &#x3D; \text{softmax}(E)\quad(\text{对每行做 softmax，行和为 1}).$$</li>
<li>输出：<br>$$Y &#x3D; A V\quad(\text{形状 }N_Q\times d_v).$$</li>
</ol>
</li>
<li>为什么要除以 $\sqrt{d_k}$？<ul>
<li>若 $q,k$ 的每个分量是均值为 0、方差为 $\sigma^2$ 的独立随机变量，则点积 $q\cdot k$ 的方差会随着维度 $d_k$ 增长而增长，规模约为 $d_k\sigma^4$；除以 $\sqrt{d_k}$ 可以把数量级恒定化，使 softmax 不容易进入极端分布，从而梯度更稳定。</li>
</ul>
</li>
</ul>
<div align="center"><img src="/img/image-110.png" alt="图示" width="80%" /></div>
---

<p>五、Self-Attention、Cross-Attention 与位置编码</p>
<ul>
<li>Self-Attention（自注意力）：<ul>
<li>在 Self-Attention 中，Queries &#x2F; Keys &#x2F; Values 都来自同一组输入向量 $X$（例如一段句子的词向量或图像的 patch 向量）：<br>$$Q &#x3D; X W_Q,\quad K &#x3D; X W_K,\quad V &#x3D; X W_V.$$<br>然后做 $Y &#x3D; \text{softmax}(QK^\top&#x2F;\sqrt{d_k})V$。输出 $Y$ 的第 i 行就是输入第 i 个向量“看”整个集合后得到的新表示。</li>
<li>性质：Self-Attention 是“置换等变（permutation equivariant）”的 —— 如果你对输入序列做同样的置换（打乱顺序），输出会被相同的置换影响。换句话说，单纯 self-attention 并不知道原始序列的顺序信息。<div align="center"><img src="/img/image-109.png" alt="图示" width="80%" /></div></li>
</ul>
</li>
<li>为什么需要位置编码（Positional Encoding）？<ul>
<li>因为 self-attention 本身没有顺序意识（它把输入当成一个集合），但序列问题（语言、时间、图像 patch 的 2D 位置）需要顺序信息。解决方法：在输入向量上叠加位置向量（positional encoding）。</li>
<li>两类常用位置编码：<ol>
<li>绝对位置学习（learned positional embeddings）：把每个位置学习一个向量。</li>
<li>正弦&#x2F;余弦编码（Vaswani 提出）：<br>$$\text{PE}<em>{pos,2i} &#x3D; \sin\left(\frac{pos}{10000^{2i&#x2F;d}}\right),\quad<br>  \text{PE}</em>{pos,2i+1} &#x3D; \cos\left(\frac{pos}{10000^{2i&#x2F;d}}\right).$$<br>优点：对位置之间的相对关系有一定的解析性质，并可推广到未见的更长序列位置（理论上）。</li>
</ol>
</li>
</ul>
</li>
<li>Masked Self-Attention（掩码自注意力）：<ul>
<li>在自回归语言模型（predict next token）中，为了不能“看见未来”需要屏蔽掉未来位置的注意力（把对应 E 的条目设置为 $-\infty$，使 softmax 后为 0）。这是实现 autoregressive 生成（例如 GPT）的关键。</li>
<li>直观：不要让模型“作弊”看未来单词。</li>
</ul>
</li>
</ul>
<hr>
<div align="center"><img src="/img/image-111.png" alt="图示" width="80%" /></div>
处理序列的三种方式：
1. RNN：按序列顺序逐步迭代，理论上对长距依赖有优势，但不可并行（序列依赖导致无法并行化）。
2. Convolution（CNN）：并行，可通过堆叠扩大感受野，但对很长序列通常需要很多层（对长距依赖较差）。
3. Self‑Attention：每个输出直接依赖所有输入，计算可高度并行（基本变成矩阵乘），利于捕获全局关系，但代价是 O(N^2)的开销。
---

<p>七、Transformer Block（完整结构）</p>
<ul>
<li>一个标准 Transformer block（以 Pre-LN 形式为例）包含两大子层：<ol>
<li>Multi-Head Self-Attention + Residual</li>
<li>Position-wise Feed-Forward Network (MLP) + Residual<br>并在每个子层之前或之后加 Layer Normalization（Pre-LN 更稳定，很多现代实现采用 Pre-LN）。</li>
</ol>
</li>
</ul>
<div align="center"><img src="/img/image-112.png" alt="图示" width="80%" /></div>

<hr>
<p>八、Transformer 在语言与视觉中的典型用法</p>
<ul>
<li><p>LLM语言模型（Decoder-only，自回归）：</p>
<ul>
<li>架构：词嵌入（Embedding） → 加位置编码 → 多层 Decoder-block（每层用 Masked Self-Attention）→ 最后用投影矩阵 $W_\text{out}\in\mathbb{R}^{D\times V}$ 投影到词表 logits，然后 softmax+交叉熵预测下一个 token。<div align="center"><img src="/img/image-113.png" alt="图示" width="80%" /></div></li>
</ul>
</li>
<li><p>Vision Transformer（ViT）：</p>
<ul>
<li>把图片分成固定大小 patch（例如 16×16），每块 flatten 后线性投影到维度 D，得到 N 个 patch token（N &#x3D; (H&#x2F;16)*(W&#x2F;16)）。</li>
<li>把这些 patch token 作为 Transformer 的输入（再加 positional encoding）；不使用 mask（patch 间可以自由互相看）。</li>
<li>最终把所有输出 token 平均池化或用一个专门的 [CLS] token 做分类。</li>
<li>等价实现：patch embedding 可以看作一个 conv 层：kernel &#x3D; patch size, stride &#x3D; patch size。</li>
</ul>
</li>
</ul>
<div align="center"><img src="/img/image-114.png" alt="图示" width="80%" /></div>
---

<h1 id="第九课-·-检测分割可视化与理解"><a href="#第九课-·-检测分割可视化与理解" class="headerlink" title="第九课 · 检测分割可视化与理解"></a>第九课 · 检测分割可视化与理解</h1><h2 id="本课核心内容-1"><a href="#本课核心内容-1" class="headerlink" title="本课核心内容"></a>本课核心内容</h2><ul>
<li>语义分割：从滑窗到 FCN，再到 U‑Net，核心是共享计算 + 合理上采样 + skip connections。</li>
<li>目标检测路线图：滑窗 → region proposal（Selective Search）→ R‑CNN → Fast R‑CNN → Faster R‑CNN（引入 RPN）→ Mask R‑CNN（再加 mask 支路）；另一条是单阶段（YOLO&#x2F;SSD&#x2F;RetinaNet），近年又出现 Transformer-based DETR。</li>
<li>可视化工具：filters、saliency（梯度）、CAM（GAP 限制）、Grad‑CAM（通用）和 guided backprop（细节增强）。</li>
</ul>
<h2 id="笔记内容-4"><a href="#笔记内容-4" class="headerlink" title="笔记内容"></a>笔记内容</h2><p>一、计算机视觉的主要任务</p>
<ul>
<li>图像分类：给整张图一个 label（不关心对象个数或位置）。</li>
<li>语义分割（Semantic Segmentation）：为每个像素分配一个语义类别（不区分不同实例），结果是和输入同分辨率的像素级预测图。</li>
<li>目标检测（Object Detection）：识别图中有多少对象、它们的类别以及每个对象的边界框（bounding box）。</li>
<li>实例分割（Instance Segmentation）：在检测的基础上，为每个检测到的实例给出像素级掩码（mask）。</li>
</ul>
<p>二、语义分割：从滑窗到 FCN，再到 U‑Net</p>
<ol>
<li>Fully Convolutional Network（FCN）的核心思想</li>
</ol>
<ul>
<li>用卷积网络把整张图做一次前向（共享计算），最后输出为 C × H × W 的分数图（每像素 C 类分数）。所以不再针对每个像素做独立的前向。</li>
<li>现实问题：很多分类网络通过 pooling&#x2F;stride 降采样以扩大 receptive field（感受野），但语义分割需要与输入尺寸相同的输出 → 需要在网络中做 upsampling（上采样）。</li>
</ul>
<div align="center"><img src="/img/image-115.png" alt="图示" width="80%" /></div>
2. 上采样（Upsampling）常见方法
- Unpooling（基于池化位置还原）：
  - Nearest neighbor / bed‑of‑nails：把小图像重复或在空格填零，简单但粗糙。
  - Max unpooling：利用 maxpool 时保存的“最大值位置”来定位还原，保留结构信息。
  - 比喻：像把压缩的图“用标签还原回原来的槽位”。
  <div align="center"><img src="/img/image-116.png" alt="图示" width="80%" /></div>
- Transposed convolution（反卷积 / 可学习上采样）：
  - 数学上可把卷积写成矩阵乘法，transposed convolution 是用该矩阵的转置去“放大”特征图，具有可学习的权重。
  - 示例（直观）：输入的每个像素会按卷积核生成一小块输出，然后在输出重叠处相加。
  <div align="center"><img src="/img/image-118.png" alt="图示" width="80%" /></div>
- Learnable upsampling（例如先上采样再卷积、或 transposed conv）
  - 现代实践常用上采样（bilinear）+ 卷积 或者 transposed conv，再结合 skip connections。<div align="center"><img src="/img/image-117.png" alt="图示" width="80%" /></div>
- U‑Net（Ronneberger et al., 2015）
  - 结构：编码（downsampling）- 解码（upsampling）对称结构，解码阶段与编码阶段的高解析度特征做拼接（skip connections）。
  - 直观比喻：像一条主干路（把信息压缩提取）和很多旁路（把高分辨率细节带回来），拼接让网络既有大视野（context）又有精细定位。
  - U‑Net 在医学影像与语义分割任务中非常成功。
  <div align="center"><img src="/img/image-119.png" alt="图示" width="80%" /></div>

<p>三、目标检测：从滑窗到 RPN 和单阶段方法</p>
<ol>
<li>单目标检测（回归框 + 分类）</li>
</ol>
<ul>
<li>如果图中只有一个对象，可以把分类与边框回归放在一起训练（多任务损失）。但现实图像有可变数量的对象，输出数量不固定。</li>
</ul>
<ol start="2">
<li>区域候选（Region Proposal）方法</li>
</ol>
<ul>
<li>先用快速的启发式算法（如 Selective Search）生成 ~2000 个候选框，再对每个候选框做精分类与回归。</li>
<li>R‑CNN（Girshick et al., 2014）：对每个候选框裁剪并 warp 成 224×224，分别送到 CNN（每个候选独立前向）→ 非常慢。</li>
</ul>
<div align="center"><img src="/img/image-120.png" alt="图示" width="80%" /></div>
- Fast R‑CNN（2015）：先对整张图做一次卷积，得到特征图；再对候选框在特征图上 crop+resize（RoI Pool）得到固定大小特征，最后分类与回归 → 大幅加速（共享卷积）。
  - RoI Pool: 将候选在特征图上“对齐到网格”并在每个子格做 max‑pooling，输出固定尺寸（但存在 “snap” 导致细微不对齐）。
- Faster R‑CNN（2015）：在 Fast R‑CNN 的基础上引入 RPN（Region Proposal Network），让网络本身学会生成候选框（anchors），从而摆脱外部候选算法，实现端到端训练。
  - RPN 概念：在特征图每个像素位置设 K 个预设 anchor（不同尺度与宽高比），网络为每个 anchor 预测是否是 object（得分）与 4 个回归值（dx,dy,dw,dh）。按 objectness 排序取 top‑N proposals（再 NMS）。
  - 比喻：在停车场每个停车位（anchor）放一个传感器，判断有没有车并微调车位边界。
<div align="center"><img src="/img/image-121.png" alt="图示" width="80%" /></div>


<p>四、实例分割（Mask R‑CNN）</p>
<ul>
<li>Mask R‑CNN 在 Faster R‑CNN 基础上，加了一个小的 mask 分支（在 RoI features 上预测 28×28 的二值 mask），同时使用 RoI Align 以保证像素级对齐。</li>
<li>同时可以扩展做姿态估计（keypoints）。</li>
<li>比喻：先检测出每辆车（检测器），然后在车的周围画上剪刀走出精确的轮廓（mask 分支）。</li>
</ul>
<div align="center"><img src="/img/image-122.png" alt="图示" width="80%" /></div>

<p>五、可视化与理解（为什么要可视化）<br>目的：帮助我们理解模型在学什么、发现错误模式、调试和解释模型决策。</p>
<ol>
<li>Class Activation Mapping (CAM)</li>
</ol>
<ul>
<li>要求：网络在最后有一个 global average pooling（GAP）然后接全连接做分类。</li>
<li>定义：最后一层卷积特征 f_{h,w,k}（大小 H×W×K），FC 层权重为 w_{k,c}，则类 c 的 activation map 为：<br>$$<br>M_{c,h,w}&#x3D;\sum_{k} w_{k,c}, f_{h,w,k}<br>$$</li>
<li>直观：把每个通道的热度按权重线性组合起来得到 class‑specific 的热图。缺点：只能直接用于特殊结构（GAP + FC）。</li>
</ul>
<div align="center"><img src="/img/image-123.png" alt="图示" width="80%" /></div>

<ol start="2">
<li>Grad‑CAM（更通用）</li>
</ol>
<ul>
<li>Grad‑CAM 可以应用于任意层（不要求 GAP 结构），核心步骤：<ol>
<li>选定某一层的激活 A ∈ R^{H×W×K}。</li>
<li>计算类分数 S_c 关于该层激活的梯度 $$\frac{\partial S_c}{\partial A_{h,w,k}}$$。</li>
<li>用梯度的空间平均得到权重 α_k：<br>$$<br>\alpha_k &#x3D; \frac{1}{H W}\sum_{h,w}\frac{\partial S_c}{\partial A_{h,w,k}}<br>$$</li>
<li>得到激活图并过 ReLU：<br>$$<br>M_{c,h,w} &#x3D; \text{ReLU}!\Big(\sum_k \alpha_k A_{h,w,k}\Big)<br>$$</li>
</ol>
</li>
<li>解释：α_k 衡量通道 k 对目标类别的重要性，然后把通道加权求和得到热图。ReLU 保证只保留对该类有正向贡献的区域。</li>
<li>Grad‑CAM 通常与 Guided Backprop 结合，得到更清晰的可视化（Grad‑CAM 给出定位，guided backprop 给出细节边缘，两者乘积可以得到高分辨率的解释图）。</li>
</ul>
<div align="center"><img src="/img/image-124.png" alt="图示" width="80%" /></div>

<h1 id="第十课-·-视频理解"><a href="#第十课-·-视频理解" class="headerlink" title="第十课 · 视频理解"></a>第十课 · 视频理解</h1><h2 id="课程核心内容"><a href="#课程核心内容" class="headerlink" title="课程核心内容"></a>课程核心内容</h2><p>视频理解的本质是“将空间（2D 图像）与时间（motion &#x2F; temporal structure）联合建模”——实现方式有多条主线（per‑frame + pooling、optical‑flow 两流、3D 卷积、RNN&#x2F;recurrence、自注意力&#x2F;transformer），每条主线有自己的计算—性能权衡：短期内用 3D conv &#x2F; two‑stream 捕获局部 motion，长期依赖 RNN &#x2F; attention 捕获长程依赖；现代趋势是用 Transformer &#x2F; masked‑autoencoder（VideoMAE）和大规模预训练来获得更强的全局时空表示，并与音频等模态联合以提升性能和实际应用能力</p>
<h2 id="笔记内容-5"><a href="#笔记内容-5" class="headerlink" title="笔记内容"></a>笔记内容</h2><p>一、Video 问题概览与挑战</p>
<ul>
<li>Video &#x3D; 图片 + 时间：输入张量通常表示为 T × 3 × H × W（或 3 × T × H × W）。</li>
<li>主要任务：短片动作分类（clip classification）、长视频的时序动作定位（temporal localization）、时空检测（spatio‑temporal detection，如 AVA）、实例级时空分割、音视频多模态任务等。</li>
<li>挑战：视频数据体积大、帧率高（通常 ~30fps）、计算与存储开销巨大；还需同时建模空间和时间信息（短期 motion、长期结构）。</li>
</ul>
<p>二、训练策略：为何用短 clips</p>
<ul>
<li>直接输入整段高 fps 视频不可行，常用策略：<ul>
<li>在训练时采样短 clips（例如 T&#x3D;8、16、32；H&#x3D;W&#x3D;112 或 224），通常降采样时间轴（低 fps）。</li>
<li>测试时可对视频多片段采样并平均或投票得到最终预测。</li>
</ul>
</li>
<li>原则：在时间上和空间上做折中，保证合理的时空上下文且能并行训练。</li>
</ul>
<p>三、视频分类的基本方案对比（直觉与优劣）</p>
<ol>
<li>Single‑frame baseline</li>
</ol>
<ul>
<li>把每一帧独立用 2D CNN 做分类，测试时对帧概率取平均。</li>
<li>优点：实现简单、计算便宜；往往是强基线。</li>
<li>缺点：忽略运动信息（但对很多场景外观信息已足够）。</li>
</ul>
<ol start="2">
<li>Late Fusion</li>
</ol>
<ul>
<li>对每帧用 2D CNN 提取特征（或中高层特征），在时间维度上拼接&#x2F;池化，再用 MLP&#x2F;Linear 做分类。</li>
<li>优点：能结合多帧的高层表示；实现相对简单。</li>
<li>缺点：难以建模低层 motion（像素级&#x2F;边缘移动）。</li>
</ul>
<ol start="3">
<li>Early Fusion</li>
</ol>
<ul>
<li>把 T 帧在 channel 维度拼接（相当于把 3×T 当作“超通道”输入），用 2D Conv 在第一层融合时间信息。</li>
<li>优点：比 single‑frame 多做一点时序融合。</li>
<li>缺点：时间融合仅在最开始层完成，可能不足以建模复杂时序结构。</li>
</ul>
<ol start="4">
<li>3D CNN（直接在空间+时间上卷积）</li>
</ol>
<ul>
<li>用 3D 卷积核（Kt × Kh × Kw）对时空体积做逐层建模（例如 C3D、I3D、SlowFast、X3D）。</li>
<li>优点：具有时间平移不变性、能逐层累积时域感受野，能学习运动模式（可将一段短时视频视作“3D 图像”）。</li>
<li>缺点：计算&#x2F;内存开销大（3×3×3 卷积昂贵），训练需要更多数据&#x2F;计算。</li>
</ul>
<p>对比直观：</p>
<ul>
<li>Single‑frame：只看外观（fast, simple）</li>
<li>Two‑stream &#x2F; optical flow：显式把 motion 与 appearance 分离（更鲁棒地捕获动作）</li>
<li>3D CNN：从底层学时空滤波器（更通用但更费算力）</li>
<li>Transformer &#x2F; attention：直接建模长距离时空依赖（可扩展，但 token 数量与复杂度需处理）</li>
</ul>
<div align="center"><img src="/img/image-125.png" alt="图示" width="80%" /></div>


<p>四、长时序建模：RNN &#x2F; Recurrent CNN &#x2F; 注意力</p>
<ul>
<li>RNN &#x2F; LSTM：先用 CNN（2D&#x2F;3D）提取每片段特征，再用 LSTM 处理序列，能建模较长时间依赖（many‑to‑one 或 many‑to‑many）。</li>
<li>Recurrent Convolutional Network：把 RNN 的矩阵乘替换成卷积（按时间共享参数），保留空间结构同时获得长时序记忆（Ballas et al.）。</li>
<li>缺点：RNN 不易并行，长序列训练慢；因此近年来更倾向用注意力&#x2F;Transformer 进行并行化建模。</li>
</ul>
<div align="center"><img src="/img/image-126.png" alt="图示" width="80%" /></div><div align="center"><img src="/img/image-127.png" alt="图示" width="80%" /></div>

<p>七、空间-时间自注意力：Nonlocal Block 与 Video Transformer</p>
<ul>
<li><p>Nonlocal Block（Wang et al., 2018）：在 3D feature 上做全局（或稀疏）时空 attention，用 1×1×1 卷积生成 Query&#x2F;Key&#x2F;Value，把 attention 权重作用回特征图并加残差。效果类似 Transformer 的 self‑attention 在时空上的应用。<div align="center"><img src="/img/image-128.png" alt="图示" width="80%" /></div></p>
</li>
<li><p>Video Transformer（如 ViViT、MViT 等）：把帧的 patches&#x2F;token 在时空上做 attention（可做因式分解的时空注意力、分层池化以控制 token 数），在大规模预训练下性能很好。</p>
</li>
</ul>
<hr>
<h1 id="第十一课-·-大规模分布式训练"><a href="#第十一课-·-大规模分布式训练" class="headerlink" title="第十一课 · 大规模分布式训练"></a>第十一课 · 大规模分布式训练</h1><h2 id="本课核心内容-2"><a href="#本课核心内容-2" class="headerlink" title="本课核心内容"></a>本课核心内容</h2><h2 id="笔记内容-6"><a href="#笔记内容-6" class="headerlink" title="笔记内容"></a>笔记内容</h2><p>本课动机（为什么要学这些）</p>
<ul>
<li>当模型参数超过单张 GPU 能容纳的内存、或者训练速度需要成百上千张 GPU 协同工作时，我们必须把“模型”和“数据”按不同维度拆分到多张 GPU 上训练。  </li>
<li>不同的拆分策略（Batch &#x2F; Sequence &#x2F; Layer &#x2F; Hidden-dim）有不同的通信、内存、计算特性与优缺点。本课教你这些策略是什么、什么时候用、各自的代价与互补组合。</li>
</ul>
<p>分布式训练的四个切分维度：</p>
<ol>
<li>Batch (DP)</li>
<li>Layer (PP)</li>
<li>Sequence (CP)</li>
<li>Dimension (TP)</li>
</ol>
<p>并行策略详解</p>
<p>A. 数据并行（Data Parallelism, DP）</p>
<ul>
<li>比喻：想象你和 M 个同学各自做同一个菜的不同份量（不同数据批次），每个人都有一份菜谱（模型副本）。每个人分别计算损失并算梯度，最后把各自的梯度平均合并，然后每人都用合并后的梯度更新自己的菜谱（模型）。  </li>
<li>数学表达（假设每个小批量 N，M 台 GPU，共 MN 样本）：<br>LaTeX:<br>$$<br>L &#x3D; \frac{1}{MN}\sum_{i&#x3D;1}^{M}\sum_{j&#x3D;1}^{N} \ell(x_{i,j}, W)<br>$$<br>$$<br>\frac{\partial L}{\partial W}<br>&#x3D; \frac{1}{M}\sum_{i&#x3D;1}^{M} \left(\frac{1}{N}\sum_{j&#x3D;1}^{N} \frac{\partial \ell(x_{i,j},W)}{\partial W}\right)<br>$$<div align="center"><img src="/img/image-129.png" alt="图示" width="80%" /></div></li>
<li>特点&#x2F;代价：<ul>
<li>简单、易实现（成熟框架支持）。</li>
<li>通信模式：每步需要跨设备同步梯度（All-Reduce），通信量与模型大小成正比。</li>
<li>缺点：模型副本占用内存，限制能训练的模型大小（单卡内存瓶颈）。</li>
</ul>
</li>
</ul>
<p>B. Fully Sharded Data Parallelism（FSDP &#x2F; ZeRO）</p>
<ul>
<li>比喻：把整个“菜谱”（模型参数）按章节分给不同同学保管（每个权重只由一个机器“拥有”），但在做某一步菜时，需要把那部分章节借给所有人使用（广播），用完后归还。归还的人同时也保管该章节对应的优化器状态（例如 Adam 的动量）与梯度。  </li>
<li>核心流程（针对一个层 i 的权重 W_i）：<ol>
<li>在前向计算层 i 之前，权重的主人把 W_i 广播到所有 GPU。  </li>
<li>所有 GPU 使用 W_i 进行前向计算后删掉本地拷贝（以节省内存），继续预取下一个权重 W_{i+1}。  </li>
<li>在反向时，权重的主人再次广播 W_i（因为反向需要它），各卡计算对 W_i 的局部梯度 dL&#x2F;dW_i 并删除本地副本。  </li>
<li>所有卡把局部梯度发送回拥有 W_i 的卡，拥有卡聚合并执行参数更新（它保有优化器状态）。</li>
</ol>
</li>
</ul>
<div align="center"><img src="/img/image-130.png" alt="图示" width="80%" /></div>
- 优化细节：前向和反向期间交叉通信/计算以隐藏延迟，不删除最后一个权重以避免立即重传等。  
- 优点：显著节省每卡必须存储的参数份额和优化器状态，使训练超大模型（数十亿到万亿参数）成为可能。  
- 代价：增加了更多层间通信（频繁地广播/收集权重与梯度），需要精细的调度与工程实现（DeepSpeed FSDP、PyTorch FSDP 等）。

<p>C. Hybrid Sharded Data Parallel（HSDP）</p>
<ul>
<li>思路：把 N 张 GPU 分成 M 个组，每组 K 张（N&#x3D;M×K）。组内做 FSDP（权重在组内拆分），组间做 DP（数据并行）。  </li>
<li>类比：每个小组像一个班级内部把菜谱分开保存；不同班级之间仍旧用数据并行（同样的工作不同数据）。  </li>
<li>优点：把频繁、低延迟的通信限制在组内（通常是同一节点或 pod），组间只需较少通信（如同步梯度），适合大规模集群部署并降低跨慢链路的通信。</li>
</ul>
<div align="center"><img src="/img/image-131.png" alt="图示" width="80%" /></div>

<p>D. 激活检查点（Activation Checkpointing）</p>
<ul>
<li>背景：Transformer 等深层模型在前向要保存大量中间激活（activations），这些激活在反向时要用，导致内存消耗巨大。  </li>
<li>核心思想：不要保存每层的所有激活，而是在反向时“重算”一部分前向的激活以节省内存。  </li>
<li>类比：你读一本很长的书，如果没法把整本书放在桌上看（内存不足），你只在若干关键页做书签（checkpoint）。做书签后再需要某页时，你回到书的前面重新翻到那一页（重算）。  </li>
<li>复杂度与权衡：<ul>
<li>全部不保存，反向时每次都从头重算：计算量会变成 O(N^2)（层数 N），但内存几乎 O(1)。</li>
<li>保存每 C 层的检查点：计算量大约 O(N^2&#x2F;C)，内存 O(C)。特别地，当 C ≈ sqrt(N) 时可以在计算和内存之间取得折中。  </li>
<li>关键公式表达（复杂度用大 O）：<br>LaTeX:<br>$$<br>\text{全重算：计算量 } O(N^2),\ \text{内存 } O(1)<br>$$<br>$$<br>\text{每 }C\text{ 层做 checkpoint：计算量 } O!\left(\frac{N^2}{C}\right),\ \text{内存 } O(C)<br>$$</li>
</ul>
</li>
</ul>
<div align="center"><img src="/img/image-132.png" alt="图示" width="80%" /></div>


<h1 id="第十二课-·-自监督学习"><a href="#第十二课-·-自监督学习" class="headerlink" title="第十二课 · 自监督学习"></a>第十二课 · 自监督学习</h1><h2 id="本课核心内容-3"><a href="#本课核心内容-3" class="headerlink" title="本课核心内容"></a>本课核心内容</h2><p>自监督学习：利用数据本身构造监督信号，训练模型学到通用表示，减少对人工标注的依赖。常见方法包括基于图像变换的预任务（rotation、jigsaw、inpainting、colorization）、Masked Autoencoders（MAE）、对比学习（SimCLR、MoCo）、自蒸馏（DINO）等。评估方式主要是通过下游任务的迁移性能（linear probing、fine-tuning）来衡量表示质量。近年来，自监督学习在视觉和语言领域均取得显著进展，成为预训练的重要手段。<br>对比学习：通过对比不同样本之间的相似性，学习到更具判别性的特征表示。常用的方法包括对比损失（contrastive loss）和信息最大化（mutual information maximization）等。</p>
<h2 id="笔记内容-7"><a href="#笔记内容-7" class="headerlink" title="笔记内容"></a>笔记内容</h2><ol>
<li>为什么要自监督学习</li>
</ol>
<ul>
<li>问题：深度模型（尤其视觉&#x2F;语言模型）往往需要大量标注数据才能学到好表示，而手工标注昂贵。  </li>
<li>思路：利用“数据自身”构造监督信号（不需要人工标签），让模型通过解决自动生成的‘预任务’来学到通用的特征，这些特征可以迁移到下游任务（分类、检测、分割等）。</li>
</ul>
<ol start="2">
<li>预训练 → 下游迁移：如何评估自监督方法？</li>
</ol>
<ul>
<li>常见评估方式：<ul>
<li>预任务性能</li>
<li>表征质量<ul>
<li>Linear probing（冻结编码器，只训练一个线性分类器）用来检验编码器是否把不同类别分开；</li>
<li>Fine‑tuning（全模型或部分模型微调）以看看最优迁移效果；</li>
<li>可视化&#x2F;聚类&#x2F;鲁棒性测试等。</li>
</ul>
</li>
</ul>
</li>
</ul>
<div align="center"><img src="/img/image-133.png" alt="图示" width="80%" /></div>

<ol start="3">
<li><p>实现自监督学习的两种方法：</p>
<ol>
<li>基于重建&#x2F;遮挡的重构方法<ol>
<li>MAE方法类似于原始的 ViT，将输入划分为不重叠的图像块。• 均匀地采样大量这些图像块并进行遮盖 • 遮盖高比例（75%）的图像块使得预测任务具有挑战性且有意义。• 此外，不使用遮盖标记并选择高采样比例（遮盖大部分图像，例如 75%，并采样少量可见部分，例如 25%，输入编码器）可以让编码器非常庞大<div align="center"><img src="/img/image-138.png" alt="图示" width="80%" /></div></li>
<li>MAE编码器仅对未掩码的图像块（25%）进行操作 ， 通过线性投影嵌入图像块并添加位置嵌入 ，使用变压器模块 由于输入的图像块只是输入的一小部分，因此编码器被设计得非常大（编码器的令牌数量是解码器的9倍以上）</li>
<li>MAE将编码器输出与先前被遮蔽位置的共享遮蔽标记合并，并向其添加位置编码。使用变压器块，然后通过线性投影完成像素重建。 完全负责重建，这意味着它在训练阶段使用，但在训练后不使用。因此，它独立于编码器设计，使其具有灵活性（不同于传统的自动编码器或 UNet）。这是一种非对称自动编码器设计。</li>
</ol>
</li>
<li>对比学习（Contrastive Learning）：核心直觉与 InfoNCE<ul>
<li><p>直觉：吸引正样本，排斥负样本。把“属于同一对象&#x2F;实例的不同视角”吸引到一起（positive），把“不同实例&#x2F;对象”推远（negative）。学到的表示把同一物体的不同变形映射到相近向量。  </p>
</li>
<li><p>一般构造：对于一个样本 x，通过数据增强或时间上下文生成一个正样本 x+（两个视图），同时采样一批负样本 {x-}。训练编码器 f 使得 f(x) 与 f(x+) 相似，而与其它样本不相似。  </p>
</li>
<li><p>InfoNCE 损失（给定一个正样本与 N-1 个 negatives）常写为：<br>$$<br>\mathcal{L}<em>{\text{InfoNCE}} &#x3D; -\log\frac{\exp\big(\mathrm{sim}(z, z^+)&#x2F;\tau\big)}<br>{\exp\big(\mathrm{sim}(z, z^+)&#x2F;\tau\big) + \sum</em>{k&#x3D;1}^{N-1}\exp\big(\mathrm{sim}(z, z_k^-)&#x2F;\tau\big)}<br>$$<br>其中 $$z&#x3D;f(x)$$、$$z^+&#x3D;f(x^+)$$、$$z_k^-&#x3D;f(x_k^-)$$、$$\mathrm{sim}(\cdot,\cdot)$$ 例如余弦相似度，$$\tau$$ 为温度超参。  </p>
</li>
<li><p>这也是N分类Softmax分类器的交叉熵损失，学习从N个样本中找到正样本</p>
</li>
<li><p>f(x) 与 f(x+) 之间互信息的下界满足如下关系：<div align="center"><img src="/img/image-135.png" alt="图示" width="80%" /></div>负样本数量 (N) 越大，下界越紧密。</p>
</li>
</ul>
</li>
</ol>
<p>对比学习的两种方法：</p>
<ol>
<li>SimCLR</li>
</ol>
  <div align="center"><img src="/img/image-136.png" alt="图示" width="80%" /></div>
  - 核心：用强数据增强生成两视图作为正样本；在batch内把其它样本当 negatives（batch‑based negatives）。  
  - 结构：encoder f + 非线性 projection head g（即先把表示映射到对比空间再做 InfoNCE）。  
  - 关键发现：projection head 有助于提高 encoder 的迁移性能（projection head 空间学习 invariance，而 encoder 的输出保留更多信息供下游使用）；大 batch size 对性能非常重要（因为更多 negatives）。  

<ol>
<li>MoCo</li>
</ol>
  <div align="center"><img src="/img/image-137.png" alt="图示" width="80%" /></div>
  - 目标：减轻 SimCLR 对巨大 batch 的依赖。  
  momentum update 写成：

<p>  $$<br>  \theta_k \leftarrow m \theta_k + (1-m)\theta_q<br>  $$<br>  其中 $$\theta_k$$ 是 key encoder 参数，$$\theta_q$$ 是 query encoder 参数，$$m$$ 接近 1（如 0.99）。</p>
</li>
</ol>
<h1 id="第十三课-·-生成式模型（上）"><a href="#第十三课-·-生成式模型（上）" class="headerlink" title="第十三课 · 生成式模型（上）"></a>第十三课 · 生成式模型（上）</h1><h2 id="本课核心内容-4"><a href="#本课核心内容-4" class="headerlink" title="本课核心内容"></a>本课核心内容</h2><p>生成模型<br>自回归模型<br>变分自编码器VAE</p>
<h2 id="笔记内容-8"><a href="#笔记内容-8" class="headerlink" title="笔记内容"></a>笔记内容</h2><ol>
<li>什么是生成模型？为什么要学？</li>
</ol>
<ul>
<li>定义（直观）：生成模型学习数据分布 p(x)，能“生成”与训练数据同类的新样本；条件生成模型学习 p(x|y)，能做“有约束地生成”（如文本到图像）。  </li>
<li>应用举例：图像生成（DALL·E、Stable Diffusion）、语言生成（GPT 系列）、语音合成、图像修复&#x2F;超分辨率、模拟物理系统、异常检测（低概率样本）等。</li>
</ul>
<ol start="2">
<li>判别模型 vs 生成模型（概率角度）</li>
</ol>
<ul>
<li>判别模型学习 p(y|x)：把输入映射到标签（例如分类）。  </li>
<li>生成模型学习 p(x) 或 p(x|y)：学习如何产出样本本身。  </li>
<li>概率直觉：生成模型给每个可能的 x 一个“概率质量&#x2F;密度”，高概率表示更常见、更合理的样本；因此它可以用来检测异常或拒绝不合理输入。 <div align="center"><img src="/img/image-139.png" alt="图示" width="80%" /></div></li>
<li>Bayes 关系：<br>$$<br>p(y \mid x) &#x3D; \frac{p(x \mid y),p(y)}{p(x)}<br>$$</li>
</ul>
<ol start="3">
<li>生成模型的分类</li>
</ol>
<div align="center"><img src="/img/image-140.png" alt="图示" width="80%" /></div>
- 显式密度（explicit density）：模型能直接给出或近似给出 p(x) 的值。进一步分为：
  - 可解析/可计算密度（tractable）：自回归模型（PixelRNN、语言模型）可以精确写出 p(x) 并用 MLE 训练。  
  - 近似密度（approximate）：变分自编码器（VAE），通过下界（ELBO）近似最大化似然。  
- 隐式密度（implicit）：无法写出 p(x) 的显式表达，但能从模型快速采样（生成）；典型代表是 GAN。  
- 采样方式维度：
  - 直接采样（一次性）：GAN（给噪声直接输出图像）。  
  - 迭代采样（多步过程）：扩散模型（从噪声逐步去噪）。  
- 讲义把这些方法按“能否计算 p(x)”与“采样方式”画成谱系图，方便记忆。

<ol start="4">
<li>自回归模型（Autoregressive models）</li>
</ol>
<ul>
<li>核心思想（链式法则）：把复杂分布 p(x) 分解为条件分布的连乘形式。若 x 是序列 x&#x3D;(x1,…,xT)，则<br>LaTeX:<br>$$<br>p(x)&#x3D;p(x_1,x_2,\dots,x_T)&#x3D;\prod_{t&#x3D;1}^T p(x_t\mid x_{&lt;t})<br>$$<br>（其中 $$x_{&lt;t}$$ 表示前 t−1 个元素）  </li>
<li>优点：每一项都是一个标准条件概率，可以直接用最大似然（MLE）训练（把训练集的对数似然相加）。<br>LaTeX:<br>$$<br>\mathcal{L}(\theta)&#x3D;\sum_{i&#x3D;1}^N \log p_\theta(x^{(i)}) &#x3D; \sum_{i&#x3D;1}^N \sum_{t&#x3D;1}^{T} \log p_\theta(x^{(i)}<em>t \mid x^{(i)}</em>{&lt;t})<br>$$</li>
<li>示例：<ul>
<li>语言模型（RNN&#x2F;Transformer）就是自回归：每步预测下一个 token 的分布（LLMs）。  </li>
<li>图像自回归（PixelRNN &#x2F; PixelCNN）：按 scanline 把图像转为像素序列，逐像素预测（每像素通常分 256 类）。</li>
</ul>
</li>
</ul>
<div align="center"><img src="/img/image-141.png" alt="图示" width="80%" /></div>

<ol start="5">
<li>变分自编码器（VAE）：从自编码器到概率生成模型</li>
</ol>
<ul>
<li><p>先回顾普通自编码器（AE）：encoder 把 x 映射为 latent z，decoder 从 z 重建 x，用 L2 或其它重构损失训练。问题是：如何用 AE「生成」全新的样本？如果 z 没有一个已知分布，从它采样没有意义。<div align="center"><img src="/img/image-142.png" alt="图示" width="80%" /></div></p>
</li>
<li><p>VAE 的核心想法：把 z 的先验 p(z) 设定为已知分布（常取标准正态 N(0,I)），并学习一个 encoder q_\phi(z|x) 来近似真实后验 p_\theta(z|x)。通过变分推断，联合训练 encoder（近似后验）和 decoder（条件生成 p_\theta(x|z)），目标是最大化观测数据的对数似然的下界（ELBO）。  </p>
</li>
<li><p>重要的公式（ELBO 推导要点）——最终得到的训练目标为最大化下界（等价于最小化负下界）：<br>LaTeX:<br>$$<br>\log p_\theta(x) ;\ge; \mathbb{E}<em>{z\sim q</em>\phi(z|x)}\big[\log p_\theta(x\mid z)\big] ;-; D_{\mathrm{KL}}\big(q_\phi(z\mid x),|, p(z)\big).<br>$$<br>其中第一项是重构项（期望似然），第二项是后验与先验的 KL 散度（正则项，鼓励 q 與先验一致）。  </p>
</li>
<li><p>直观意义：</p>
<ul>
<li>重构项希望 decoder 能从 encoder 给出的 z 恢复 x（鼓励信息保留）。  </li>
<li>KL 项希望 encoder 的输出分布接近先验（鼓励可采样性）。<br>这两项会发生博弈：重构要求 z 带足够信息；KL 要求 z 看起来像来自 N(0,I)。</li>
</ul>
</li>
<li><p>实现步骤：</p>
<ol>
<li>encoder 网络输出 $$\mu_\phi(x),\ \sigma_\phi(x)$$（或对角协方差）；  </li>
<li>采样 $$\epsilon\sim N(0,I)$$，构造 z；  </li>
<li>decoder 给出 p_\theta(x|z)（例如高斯分布，均值由网络输出）；  </li>
<li>计算重构损失（通常是负对数似然或 L2）与 KL(q||p)；  </li>
<li>最小化负 ELBO（或最大化 ELBO）。</li>
</ol>
</li>
<li><p>生成&#x2F;采样：训练后从先验 p(z)&#x3D;N(0,I) 采样 z，然后把 z 放 decoder，得到样本 x。  <div align="center"><img src="/img/image-143.png" alt="图示" width="80%" /></div></p>
</li>
</ul>
<h1 id="第十四课-·-生成式模型（下）"><a href="#第十四课-·-生成式模型（下）" class="headerlink" title="第十四课 · 生成式模型（下）"></a>第十四课 · 生成式模型（下）</h1><h2 id="本课核心内容-5"><a href="#本课核心内容-5" class="headerlink" title="本课核心内容"></a>本课核心内容</h2><p>生成对抗网络（GAN）<br>扩散模型（Diffusion Models）<br>潜空间扩散模型（Latent Diffusion Models, LDMs）</p>
<p>现代高质量图像&#x2F;视频生成的主流路线是把噪声-还原思想做成可训练的“扩散&#x2F;流”模型（Rectified Flow &#x2F; Generalized Diffusion），在潜空间上做（Latent Diffusion）以节省计算；CFG 是实践中控制条件的重要技巧；Distillation 用来把慢采样变快；VAE&#x2F;GAN 仍在 pipeline（例如 LDM 的 encoder&#x2F;decoder）中扮演角色。</p>
<h2 id="笔记内容-9"><a href="#笔记内容-9" class="headerlink" title="笔记内容"></a>笔记内容</h2><p>本讲重点介绍三大现代一线方法：GAN（显式采样、隐式密度）、Diffusion（迭代去噪、能做到极高质量但采样慢）和把扩散放到潜空间里的 Latent Diffusion（工程上最常用的组合）。</p>
<ol>
<li><p>GAN（Generative Adversarial Networks）<br>  直觉</p>
<ul>
<li>想象两个选手：Generator（G）负责“伪造”样本，Discriminator（D）负责区分“真／假”。G 的目标是骗过 D，D 的目标是正确识别。两者打“零和博弈”。</li>
</ul>
  <div align="center"><img src="/img/image-144.png" alt="图示" width="80%" /></div>
  基本对抗目标
  - 原始 min-max 目标为：

<pre><code>$$
\min_G \max_D \; \mathbb{E}_{x\sim p_{\text{data}}}\big[\log D(x)\big] \;+\; \mathbb{E}_{z\sim p(z)}\big[\log(1 - D(G(z)))\big].
$$
</code></pre>
<ul>
<li>在实践中，为了给 G 更强的初期梯度，常替换为“非饱和”生成器损失：<br>$$<br>\mathcal{L}<em>G &#x3D; -\mathbb{E}</em>{z\sim p(z)}\big[\log D(G(z))\big].<br>$$</li>
</ul>
  <div align="center"><img src="/img/image-145.png" alt="图示" width="80%" /></div>
  重要结论（性质）
  - 对任意固定 G，最优的判别器为
    $$
    D^*(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_G(x)}.
    $$
  - 如果 D 能达到最优并且两者有无限容量，minimax 最优解在于 $$p_G = p_{\text{data}}$$


</li>
<li><p>Diffusion Models（扩散模型）——直觉与核心实现</p>
<ul>
<li><p>核心直觉（简化）<br>把生成问题反过来想：把真实数据逐步加入噪声（前向过程），变成纯噪声；训练网络学习如何逐步去噪（逆过程）。采样时从噪声开始，逐步去噪回到数据分布。</p>
</li>
<li><p>Rectified Flow（修正流）<br> 采样真实图像 x ~ p_data，采样噪声 z ~ p_noise（通常标准高斯），采样噪声比例 t ~ Uniform(0,1)。 <div align="center"><img src="/img/image-146.png" alt="图示" width="80%" /></div></p>
</li>
</ul>
</li>
<li><p>Latent Diffusion Models（LDMs）</p>
</li>
</ol>
<ul>
<li>扩散直接在高分辨率像素上训练成本高（内存、计算、相关性强）。LDM 的核心是把扩散放到较低维的潜空间来做，从而显著降低计算量，同时保持视觉质量。<div align="center"><img src="/img/image-147.png" alt="图示" width="80%" /></div></li>
</ul>
<h1 id="第十六课-·-多模态大模型"><a href="#第十六课-·-多模态大模型" class="headerlink" title="第十六课 · 多模态大模型"></a>第十六课 · 多模态大模型</h1><h2 id="本课核心内容-6"><a href="#本课核心内容-6" class="headerlink" title="本课核心内容"></a>本课核心内容</h2><p>多模态大模型（Multimodal Large Models, MLLMs）是指能够处理和生成多种模态数据（如图像、文本、音频等）的深度学习模型。近年来，随着基础模型（Foundation Models）的兴起，MLLMs 在视觉-语言任务中表现出色，能够实现图像描述生成、视觉问答、图像分类等功能。常见的多模态模型架构包括 CLIP、Flamingo、LLaVA 等，这些模型通常结合了对比学习、自注意力机制和大规模预训练技术。MLLMs 的发展推动了跨模态理解和生成的研究，促进了人工智能在实际应用中的广泛应用，如图像搜索、内容生成和辅助决策等。</p>
<ul>
<li>理解“基础模型（Foundation Model）”的概念以及为啥对视觉任务重要。  </li>
<li>掌握 CLIP 类型的对比式视觉-文本模型的核心思想、训练目标与如何做 zero-shot 分类（含 prompt engineering）。  </li>
<li>了解 CoCa、LLaVA、Flamingo、SAM 等近期视觉+语言&#x2F;分割基础模型的设计要点与差异。  </li>
<li>理解“链式（chaining）”方法（用 LLM 生成 prompt&#x2F;逻辑，再调用视觉模型）与 Visual Programming 的思想。</li>
</ul>
<h2 id="笔记内容-10"><a href="#笔记内容-10" class="headerlink" title="笔记内容"></a>笔记内容</h2><ol>
<li>为什么需要“基础模型”？</li>
</ol>
<ul>
<li>传统做法：为每个任务&#x2F;数据域训练一个专门模型（费时、难以复用）。  </li>
<li>基础模型思路：先做大规模自监督或弱监督的预训练，得到一个通用模型（encoder 或 encoder+decoder）。下游任务可以零样本 &#x2F; 少样本 &#x2F; 微调使用。  </li>
<li>直观比喻：把基础模型看作“通用工具箱”，而不是每个任务都制造一把新工具。</li>
</ul>
<ol start="2">
<li>CLIP：用对比学习把图像与文本拉到同一向量空间</li>
</ol>
<ul>
<li><p>把图像和对应的文本描述编码到同一个 embedding 空间，使得配对 (image, caption) 在向量空间里靠得近，不配对的远离。这样就能用文本向量直接去匹配图片（或反过来），实现 open‑vocabulary 的零样本分类与检索。<div align="center"><img src="/img/image-148.png" alt="图示" width="80%" /></div></p>
</li>
<li><p>用 CLIP 做 zero-shot 分类（技巧）<br>思路：为每类别（category）写几条自然语言描述（prompt template），编码成文本向量，得到每个类别的 prototype；把图片编码后与这些 prototype 做相似度比对，选最大者。<div align="center"><img src="/img/image-149.png" alt="图示" width="80%" /></div></p>
</li>
</ul>
<ol start="3">
<li><p>CoCa：对比 + 生成的联合目标<br>  CoCa 在 CLIP 的基础上加入了生成（captioning）decoder，使模型同时学习对比式嵌入与生成式的描述能力。这样 embedding 既能用于检索&#x2F;分类，又能支持更好的图文生成或评分任务。直观上把“理解”与“表达”两个能力一并训练。<div align="center"><img src="/img/image-150.png" alt="图示" width="80%" /></div></p>
</li>
<li><p>把视觉接入大语言模型（LLM）：LLaVA、Flamingo 等</p>
<ul>
<li><p>让强大的自回归 LLM 能直接以图像 + 文本为输入，输出文本（问答、说明、推理、指令执行等）。优势是利用 LLM 的通用推理与语言技巧，扩展视觉任务的能力。</p>
</li>
<li><p>LLaVA<br> 用预训练的视觉 encoder（典型是 CLIP encoder）提取图像表示。把这些 visual features 线性映射到 LLM 的 embedding 空间，然后把它们作为 LLM 的一部分输入（例如作为 prefix tokens）。<br> 训练流程：初始化来自 pretrained LLM（如 LLaMA）和 visual encoder，训练一个线性桥接层，然后微调 LLM+桥接层（通常冻结 visual encoder 或按需微调）。<div align="center"><img src="/img/image-151.png" alt="图示" width="80%" /></div></p>
</li>
<li><p>Flamingo（重要设计）<br> Flamingo 引入了 gated cross‑attention（门控的跨模态注意力）和 Perceiver Sampler（将可变大小的视觉 token 投影为定长 token）来融合视觉信息。<br> 它通过特殊的 attention 布局和训练数据格式（在文本序列中插入 <image> 标记）支持 few‑shot 的 in‑context 学习：给模型若干图文示例 + 测试图像，就能在零微调下完成新任务。<div align="center"><img src="/img/image-152.png" alt="图示" width="80%" /></div></p>
</li>
<li><p>Segmentation 基础模型：SAM（Segment Anything Model）</p>
<p> 传统 Mask R‑CNN 按一套固定类别输出掩码（例如 COCO 的 80 类）。SAM 的目标是“任何用户关心的”物体或区域都能被 mask 出来：prompt-driven segmentation（用点、框、文本等提示定位要分割的目标）。<br> 架构要点：<br> 大型 image encoder（例如 ViT）提取特征；prompt encoder（处理点&#x2F;框&#x2F;文本）将 prompt 映射成 conditioning；mask decoder 把特征+prompt 解码为 mask，并伴随置信度分数。<br> 训练数据覆盖大量类型的对象与类别，使其具备强泛化的 zero‑shot 分割能力。<br> 因为 prompt 有歧义（用户输入“点在图上”可能指不同对象），SAM 给出多个 mask 候选并返回置信度，用户可通过进一步提示或选择来精化结果。<div align="center"><img src="/img/image-153.png" alt="图示" width="80%" /></div></p>
</li>
</ul>
</li>
<li><p>链式（Chaining）与 Visual Programming（将多模型组合成更强大的系统）</p>
</li>
</ol>
<ul>
<li>CuPL（Customized Prompts via Language models）<br>问题：某些类别模型从未见过（例如非常专业的实体），直接用基本 prompt 可能效果差。解决：用 LLM 为每个类别生成高质量、细粒度的描述（多个 prompt），然后把这些作为 CLIP 的文本 prototype。效果往往比人工写单一句子更好。  <div align="center"><img src="/img/image-154.png" alt="图示" width="80%" /></div></li>
<li>VisProg（Visual Programming）<br>思路：不训练新模型，而是让 LLM 编写 Python 脚本&#x2F;逻辑，通过调用现有的各个视觉模型模块来完成复杂的组合推理任务。</li>
</ul>
<h1 id="深度学习笔记总结"><a href="#深度学习笔记总结" class="headerlink" title="深度学习笔记总结"></a>深度学习笔记总结</h1><p>本文档是我学习MIT CS231n课程整理出的笔记，全文的图片来自于课程ppt，该课程PPT地址见<a target="_blank" rel="noopener" href="https://cs231n.stanford.edu/schedule.html">https://cs231n.stanford.edu/schedule.html</a> ，因为内容过多，我使用了chat-gpt5.2辅助我写笔记，文章有很多不好理解或者错误的地方，如有勘误，请通过我的邮件 <a href="mailto:&#x33;&#50;&#x33;&#x39;&#x39;&#52;&#x37;&#x30;&#x39;&#x33;&#x40;&#113;&#113;&#x2e;&#99;&#x6f;&#x6d;">3239947093@qq.com</a> 告诉我修正，谢谢。<br>笔记内容涵盖深度学习的核心内容，涉及</p>
<ol>
<li>深度学习基础（线性分类、优化与正则化、神经网络与反向传播、卷积神经网络（CNN）、循环神经网络（RNN）、注意力机制与Transformers）</li>
<li>视觉感知与理解（检测分割可视化与理解、视频理解）</li>
<li>生成式与交互式视觉智能（大规模分布式训练、自监督学习、生成式模型以及多模态大模型）</li>
</ol>
<p>欢迎大家交流学习，共同进步！</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://SmallSmallQi.github.io">ChangQibo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://smallsmallqi.github.io/posts/14569/">https://smallsmallqi.github.io/posts/14569/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://SmallSmallQi.github.io" target="_blank">MyBLog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/myavatar.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/myavatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">ChangQibo</div><div class="author-info-description">欢迎来到常淇博的个人博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">1</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/SmallSmallQi" target="_blank" title="GitHub"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:your-email@3239947093@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E8%AF%BE-%C2%B7-%E5%AF%BC%E8%AE%BA"><span class="toc-number">1.</span> <span class="toc-text">第一课 · 导论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BE%E7%A8%8B%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">课程简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-number">1.2.</span> <span class="toc-text">1. 深度学习基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%84%9F%E7%9F%A5%E4%B8%8E%E7%90%86%E8%A7%A3%E8%A7%86%E8%A7%89%E4%B8%96%E7%95%8C"><span class="toc-number">1.3.</span> <span class="toc-text">2. 感知与理解视觉世界</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%9B%BE%E5%83%8F%E7%90%86%E8%A7%A3%E7%9A%84%E4%B8%BB%E8%A6%81%E4%BB%BB%E5%8A%A1"><span class="toc-number">1.3.1.</span> <span class="toc-text">2.1 图像理解的主要任务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E8%B6%85%E8%B6%8A%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E4%BB%A3%E8%A1%A8%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.2.</span> <span class="toc-text">2.2 超越多层感知机的代表模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%94%9F%E6%88%90%E5%BC%8F%E4%B8%8E%E4%BA%A4%E4%BA%92%E5%BC%8F%E8%A7%86%E8%A7%89%E6%99%BA%E8%83%BD"><span class="toc-number">1.4.</span> <span class="toc-text">3. 生成式与交互式视觉智能</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E8%AF%BE-%C2%B7-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB"><span class="toc-number">2.</span> <span class="toc-text">第二课 · 线性分类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Image-Classification-with-Linear-Classifiers"><span class="toc-number">2.1.</span> <span class="toc-text">Image Classification with Linear Classifiers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E8%8A%82%E8%AF%BE%E7%A8%8B%E6%A0%B8%E5%BF%83%E6%80%BB%E7%BB%93"><span class="toc-number">2.1.1.</span> <span class="toc-text">本节课程核心总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%94%E8%AE%B0%E5%86%85%E5%AE%B9"><span class="toc-number">2.1.2.</span> <span class="toc-text">笔记内容</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E8%AF%BE-%C2%B7-%E6%AD%A3%E5%88%99%E5%8C%96%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="toc-number">3.</span> <span class="toc-text">第三课 · 正则化与优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E8%8A%82%E8%AF%BE%E7%A8%8B%E6%A0%B8%E5%BF%83%E6%80%BB%E7%BB%93-1"><span class="toc-number">3.1.</span> <span class="toc-text">本节课程核心总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%94%E8%AE%B0%E5%86%85%E5%AE%B9-1"><span class="toc-number">3.2.</span> <span class="toc-text">笔记内容</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Regularization%EF%BC%89"><span class="toc-number">3.2.1.</span> <span class="toc-text">一、正则化（Regularization）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E4%BC%98%E5%8C%96%EF%BC%88Optimization%EF%BC%89"><span class="toc-number">3.2.2.</span> <span class="toc-text">二、优化（Optimization）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E6%AC%A1%E8%AF%BE%E7%A8%8B%E9%87%8D%E7%82%B9%E5%85%AC%E5%BC%8F"><span class="toc-number">3.3.</span> <span class="toc-text">本次课程重点公式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AE%8C%E6%95%B4%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-Total-Loss-Function"><span class="toc-number">3.3.1.</span> <span class="toc-text">1. 完整的损失函数 (Total Loss Function)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%99%AE%E9%80%9A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%9B%B4%E6%96%B0-Vanilla-SGD-Update"><span class="toc-number">3.3.2.</span> <span class="toc-text">2. 普通梯度下降更新 (Vanilla SGD Update)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%8A%A8%E9%87%8F%E6%9B%B4%E6%96%B0-SGD-Momentum"><span class="toc-number">3.3.3.</span> <span class="toc-text">3. 动量更新 (SGD + Momentum)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-RMSProp-%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">3.3.4.</span> <span class="toc-text">4. RMSProp (自适应学习率)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-Adam-%E4%B8%87%E8%83%BD%E5%85%AC%E5%BC%8F"><span class="toc-number">3.3.5.</span> <span class="toc-text">5. Adam (万能公式)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E8%AF%BE-%C2%B7-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">4.</span> <span class="toc-text">第四课 · 神经网络与反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E8%8A%82%E8%AF%BE%E7%A8%8B%E6%A0%B8%E5%BF%83%E6%80%BB%E7%BB%93-2"><span class="toc-number">4.1.</span> <span class="toc-text">本节课程核心总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%94%E8%AE%B0%E5%86%85%E5%AE%B9-2"><span class="toc-number">4.2.</span> <span class="toc-text">笔记内容</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E8%AF%BE-%C2%B7-CNN%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB"><span class="toc-number">5.</span> <span class="toc-text">第五课 · CNN图像分类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E8%8A%82%E8%AF%BE%E7%A8%8B%E6%A0%B8%E5%BF%83%E6%80%BB%E7%BB%93-3"><span class="toc-number">5.1.</span> <span class="toc-text">本节课程核心总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%94%E8%AE%B0%E5%86%85%E5%AE%B9-3"><span class="toc-number">5.2.</span> <span class="toc-text">笔记内容</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%B1%A0%E5%8C%96%E5%B1%82%EF%BC%88Pooling%EF%BC%89"><span class="toc-number">5.2.1.</span> <span class="toc-text">3) 池化层（Pooling）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%8D%B7%E7%A7%AF%E7%9A%84%E4%B8%A4%E5%A4%A7%E4%BC%98%E7%82%B9%EF%BC%88%E4%B8%8E-FC-%E6%AF%94%E8%BE%83%EF%BC%89"><span class="toc-number">5.2.2.</span> <span class="toc-text">5) 卷积的两大优点（与 FC 比较）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9%E5%9B%9E%E9%A1%BE"><span class="toc-number">5.3.</span> <span class="toc-text">深度学习内容回顾</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E8%AF%BE-%C2%B7-CNN%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%9E%B6%E6%9E%84"><span class="toc-number">6.</span> <span class="toc-text">第六课 · CNN网络训练与架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%9E%B6%E6%9E%84"><span class="toc-number">6.1.</span> <span class="toc-text">CNN网络训练与架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.2.</span> <span class="toc-text">1. 如何构建卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81CNN%E7%9A%84%E9%87%8D%E8%A6%81%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86"><span class="toc-number">6.2.1.</span> <span class="toc-text">一、CNN的重要组成部分</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%8D%B7%E7%A7%AF%E5%B1%82%EF%BC%9A"><span class="toc-number">6.2.1.1.</span> <span class="toc-text">1. 卷积层：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%B1%A0%E5%8C%96%E5%B1%82%EF%BC%9A"><span class="toc-number">6.2.1.2.</span> <span class="toc-text">2. 池化层：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%9A"><span class="toc-number">6.2.1.3.</span> <span class="toc-text">3. 全连接层：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82%EF%BC%9A"><span class="toc-number">6.2.1.4.</span> <span class="toc-text">4. 归一化层：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-Dropout%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%9A"><span class="toc-number">6.2.1.5.</span> <span class="toc-text">5. Dropout正则化：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9A"><span class="toc-number">6.2.2.</span> <span class="toc-text">二、激活函数：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81CNN%E6%9E%B6%E6%9E%84"><span class="toc-number">6.2.3.</span> <span class="toc-text">三、CNN架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%82%E7%9A%84%E6%9D%83%E9%87%8D"><span class="toc-number">6.2.4.</span> <span class="toc-text">四、初始化神经网络层的权重</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.3.</span> <span class="toc-text">2. 如何训练卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">6.3.1.</span> <span class="toc-text">1. 数据预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">6.3.2.</span> <span class="toc-text">2. 数据增强</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.3.3.</span> <span class="toc-text">3. 迁移学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E9%80%89%E6%8B%A9%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-number">6.3.4.</span> <span class="toc-text">4. 选择超参数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%83%E8%AF%BE-%C2%B7-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88RNN%EF%BC%89"><span class="toc-number">7.</span> <span class="toc-text">第七课 · 循环神经网络（RNN）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%BB%8E%E6%99%AE%E9%80%9A%EF%BC%88%E5%89%8D%E9%A6%88%EF%BC%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%B0-RNN%EF%BC%9A%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%85%AC%E5%BC%8F"><span class="toc-number">7.1.</span> <span class="toc-text">一、从普通（前馈）神经网络到 RNN：基本概念与公式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%B1%95%E5%BC%80%EF%BC%88Unroll%EF%BC%89%E4%B8%8E%E8%AE%A1%E7%AE%97%E5%9B%BE%EF%BC%9B%E4%BB%BB%E5%8A%A1%E7%B1%BB%E5%9E%8B"><span class="toc-number">7.2.</span> <span class="toc-text">二、展开（Unroll）与计算图；任务类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E8%AE%AD%E7%BB%83%EF%BC%9A%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E9%80%9A%E8%BF%87%E6%97%B6%E9%97%B4%EF%BC%88BPTT%EF%BC%89%E4%B8%8E%E6%88%AA%E6%96%AD-BPTT"><span class="toc-number">7.3.</span> <span class="toc-text">三、训练：反向传播通过时间（BPTT）与截断 BPTT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E7%A4%BA%E4%BE%8B%EF%BC%9A%E5%AD%97%E7%AC%A6%E7%BA%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88Char-RNN%EF%BC%89"><span class="toc-number">7.4.</span> <span class="toc-text">六、示例：字符级语言模型（Char-RNN）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81RNN-%E7%9A%84%E4%BC%98%E7%82%B9%E4%B8%8E%E7%BC%BA%E7%82%B9%EF%BC%88%E5%B9%BB%E7%81%AF%E7%89%87%E4%B8%AD%E4%B9%9F%E6%9C%89%E6%80%BB%E7%BB%93%EF%BC%89"><span class="toc-number">7.5.</span> <span class="toc-text">七、RNN 的优点与缺点（幻灯片中也有总结）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB%E3%80%81LSTM%EF%BC%88Long-Short-Term-Memory%EF%BC%89%EF%BC%9A%E7%BB%93%E6%9E%84%E3%80%81%E5%85%AC%E5%BC%8F%E4%B8%8E%E7%9B%B4%E8%A7%82%E8%A7%A3%E9%87%8A"><span class="toc-number">7.6.</span> <span class="toc-text">八、LSTM（Long Short-Term Memory）：结构、公式与直观解释</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%85%AB%E8%AF%BE-%C2%B7-Attention%E6%9C%BA%E5%88%B6%E4%B8%8ETransformers"><span class="toc-number">8.</span> <span class="toc-text">第八课 · Attention机制与Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E8%AF%BE%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9"><span class="toc-number">8.1.</span> <span class="toc-text">本课核心内容</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer%E6%9E%B6%E6%9E%84"><span class="toc-number">8.1.1.</span> <span class="toc-text">Transformer架构</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B9%9D%E8%AF%BE-%C2%B7-%E6%A3%80%E6%B5%8B%E5%88%86%E5%89%B2%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B8%8E%E7%90%86%E8%A7%A3"><span class="toc-number">9.</span> <span class="toc-text">第九课 · 检测分割可视化与理解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E8%AF%BE%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9-1"><span class="toc-number">9.1.</span> <span class="toc-text">本课核心内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%94%E8%AE%B0%E5%86%85%E5%AE%B9-4"><span class="toc-number">9.2.</span> <span class="toc-text">笔记内容</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%8D%81%E8%AF%BE-%C2%B7-%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3"><span class="toc-number">10.</span> <span class="toc-text">第十课 · 视频理解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BE%E7%A8%8B%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9"><span class="toc-number">10.1.</span> <span class="toc-text">课程核心内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%94%E8%AE%B0%E5%86%85%E5%AE%B9-5"><span class="toc-number">10.2.</span> <span class="toc-text">笔记内容</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%8D%81%E4%B8%80%E8%AF%BE-%C2%B7-%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83"><span class="toc-number">11.</span> <span class="toc-text">第十一课 · 大规模分布式训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E8%AF%BE%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9-2"><span class="toc-number">11.1.</span> <span class="toc-text">本课核心内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%94%E8%AE%B0%E5%86%85%E5%AE%B9-6"><span class="toc-number">11.2.</span> <span class="toc-text">笔记内容</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%8D%81%E4%BA%8C%E8%AF%BE-%C2%B7-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">12.</span> <span class="toc-text">第十二课 · 自监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E8%AF%BE%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9-3"><span class="toc-number">12.1.</span> <span class="toc-text">本课核心内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%94%E8%AE%B0%E5%86%85%E5%AE%B9-7"><span class="toc-number">12.2.</span> <span class="toc-text">笔记内容</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%8D%81%E4%B8%89%E8%AF%BE-%C2%B7-%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%B8%8A%EF%BC%89"><span class="toc-number">13.</span> <span class="toc-text">第十三课 · 生成式模型（上）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E8%AF%BE%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9-4"><span class="toc-number">13.1.</span> <span class="toc-text">本课核心内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%94%E8%AE%B0%E5%86%85%E5%AE%B9-8"><span class="toc-number">13.2.</span> <span class="toc-text">笔记内容</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%8D%81%E5%9B%9B%E8%AF%BE-%C2%B7-%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%B8%8B%EF%BC%89"><span class="toc-number">14.</span> <span class="toc-text">第十四课 · 生成式模型（下）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E8%AF%BE%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9-5"><span class="toc-number">14.1.</span> <span class="toc-text">本课核心内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%94%E8%AE%B0%E5%86%85%E5%AE%B9-9"><span class="toc-number">14.2.</span> <span class="toc-text">笔记内容</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%8D%81%E5%85%AD%E8%AF%BE-%C2%B7-%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">15.</span> <span class="toc-text">第十六课 · 多模态大模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E8%AF%BE%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9-6"><span class="toc-number">15.1.</span> <span class="toc-text">本课核心内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%94%E8%AE%B0%E5%86%85%E5%AE%B9-10"><span class="toc-number">15.2.</span> <span class="toc-text">笔记内容</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93"><span class="toc-number">16.</span> <span class="toc-text">深度学习笔记总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/14569/" title="深度学习个人笔记(MIT-CS231n)"><img src="/img/myavatar.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="深度学习个人笔记(MIT-CS231n)"/></a><div class="content"><a class="title" href="/posts/14569/" title="深度学习个人笔记(MIT-CS231n)">深度学习个人笔记(MIT-CS231n)</a><time datetime="2026-01-29T08:00:00.000Z" title="发表于 2026-01-29 16:00:00">2026-01-29</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/myavatar.jpg);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By ChangQibo</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>